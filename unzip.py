file_dict = {'./LPO-dailydialog\\config_and_utils.py': 'import os\nimport re\n\noffline_mode = True\n# this mode requires datasets and base models are pre-downloaded and put into this project\n# our experiments are all carried in this mode since our machines are not connected to Internet\n#   offline_mode = False\n# this mode has not been tested yet\n\nif offline_mode:\n    BASE_MODEL_PATH = "./gpt2"\n    BASE_RWD_MODEL_PATH = "./roberta-intent-classification"\n    BERT_SCORE_MODEL_PATH = "microsoft/deberta-v2-xlarge-mnli"\nelse:\n    BASE_MODEL_PATH = "openai-community/gpt2"\n    BASE_RWD_MODEL_PATH = "rajkumarrrk/roberta-daily-dialog-intent-classifier"\n    BERT_SCORE_MODEL_PATH = "microsoft/deberta-v2-xlarge-mnli"\n# it should be noted that, we use this BASE_RWD_MODEL_PATH her since it is used in RL4LMs.\n# however, we find it is underfitting to the intent labels in DailyDialog, so we will\n# further fine-tune it, i.e., train_classifier.py, before using it as the reward model\n\nintent_label2id = {\n    "Inform": 0,\n    "Questions": 1,\n    "Directives": 2,\n    "Commissive": 3\n}\n\ninput_max_length = 78\noutput_max_length = 50\n\ndef get_tokenizer():\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n    return tokenizer\n\ndef truncate_generation_after_first_EOU(output):\n    from dailydialog_dataset import DailyDialogDataset\n    EOU = DailyDialogDataset.EOU_TOKEN\n    if EOU not in output:\n        return output\n    else:\n        return output[:output.index(EOU)] + EOU\n\ndef get_sft_dataset(usage="train"):\n    from datasets import load_dataset, load_from_disk\n    if offline_mode:\n        dataset = load_from_disk(f"./daily_dialog/{usage}")\n    else:\n        dataset = load_dataset("daily_dialog")[usage]\n    return dataset\n\ndef context_utterance_to_rwd_inputs(context, utterance, tokenizer):\n    import torch\n    context_ids = tokenizer(context, return_tensors="pt").input_ids\n    utterance_ids = tokenizer(utterance, return_tensors="pt").input_ids\n    input_ids = torch.cat([context_ids, utterance_ids], dim=1)\n    attention_mask = torch.ones_like(input_ids)\n    token_type_ids = torch.cat([torch.zeros_like(context_ids),\n                                torch.ones_like(utterance_ids)], dim=1)\n    return input_ids, attention_mask, token_type_ids\n\ndef get_reward_fn(device):\n    import torch\n    import numpy as np\n    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n    rwd_model_path = auto_find_checkpoint_dir("classifier_fine_tuned/checkpoint")\n    tokenizer = AutoTokenizer.from_pretrained(rwd_model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(rwd_model_path, device_map=device)\n    def reward_fn(prompts, labels, intents=None, return_probs=False, return_scores=False, preferred_intent="consistency"):\n        batch_inputs = []\n        for context, utterance in zip(prompts, labels):\n            inputs = context_utterance_to_rwd_inputs(context, utterance, tokenizer)\n            batch_inputs.append(inputs)\n        input_ids = pad_and_concat([batch_input[0] for batch_input in batch_inputs],\n                                   pad_token_id=tokenizer.pad_token_id)\n        attention_mask = pad_and_concat([batch_input[1] for batch_input in batch_inputs],\n                                        pad_token_id=0)\n        token_type_ids = pad_and_concat([batch_input[2] for batch_input in batch_inputs],\n                                        pad_token_id=0)\n        inputs = {\n            "input_ids": input_ids.to(device),\n            "attention_mask": attention_mask.to(device),\n            "token_type_ids": token_type_ids.to(device),\n        }\n        with torch.no_grad():\n            outputs = model(**inputs)\n            intents_probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n        if return_probs:\n            return intents_probs\n        if preferred_intent == "consistency":\n            assert intents is not None, "consistency preference on intent needs prompt intents specification"\n            if return_scores:\n                return [intents_probs[i, intents[i] - 1].item() for i in range(len(intents))]\n            else:\n                return (torch.argmax(intents_probs, dim=1).cpu().numpy() == np.array(intents) - 1).astype(np.int32).tolist()\n        else:\n            if type(preferred_intent) is str:\n                preferred_intent = intent_label2id[preferred_intent]\n            assert type(preferred_intent) is int, f"unrecognized preferred_intent: {preferred_intent}"\n            if return_scores:\n                return [intents_probs[i, preferred_intent].item() for i in range(intents_probs.shape[0])]\n            else:\n                return (torch.argmax(intents_probs, dim=1).cpu().numpy() == preferred_intent).astype(np.int32).tolist()\n    return reward_fn\n\ndef get_trainer(args, model, tokenizer, train_dataset, valid_dataset, collate_fn, **kwargs):\n    from transformers import TrainingArguments, Trainer\n    import inspect\n    gradient_accumulation_steps = args.global_batch_size // (args.n_devices * args.mini_batch_size)\n    total_training_steps = args.epochs * len(train_dataset) // args.global_batch_size\n    warmup_steps = int(total_training_steps * 0.1)\n    eval_steps = int(total_training_steps * 0.1)\n    save_steps = eval_steps\n\n    deepspeed = None if args.no_deepspeed else {\n        "train_micro_batch_size_per_gpu": args.mini_batch_size,\n        "gradient_accumulation_steps": gradient_accumulation_steps,\n        "fp16": {\n            "enabled": True,\n            "min_loss_scale": 1,\n            "opt_level": "O2"\n        },\n        "zero_optimization": {\n            "stage": 2,\n            "offload_optimizer": {\n                "device": "cpu",\n                "pin_memory": True\n            },\n        },\n        "optimizer": {\n            "type": "AdamW",\n            "params": {\n                "lr": args.learning_rate,\n            }\n        },\n        "scheduler": {\n            "type": "WarmupLR",\n            "params": {\n                "warmup_min_lr": "auto",\n                "warmup_max_lr": "auto",\n                "warmup_num_steps": "auto",\n            }\n        }\n    }\n\n    training_args = TrainingArguments(\n        output_dir=args.output_dir,\n        evaluation_strategy="steps",\n        eval_accumulation_steps=1,\n        learning_rate=args.learning_rate,\n        per_device_train_batch_size=args.mini_batch_size,\n        per_device_eval_batch_size=args.mini_batch_size,\n        fp16=True,\n        adam_beta1=0.9,\n        adam_beta2=0.95,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        num_train_epochs=args.epochs,\n        warmup_steps=warmup_steps,\n        eval_steps=eval_steps,\n        save_steps=save_steps,\n        save_total_limit=1,\n        logging_steps=min(10, max(1, eval_steps // 10)),\n        deepspeed=deepspeed,\n        load_best_model_at_end=True,\n        **{k:v for k,v in kwargs.items() if k in inspect.signature(TrainingArguments.__init__).parameters},\n    )\n\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        data_collator=collate_fn,\n        **{k:v for k,v in kwargs.items() if k in inspect.signature(Trainer.__init__).parameters}\n    )\n    return trainer\n\ndef auto_find_checkpoint_dir(path):\n    if path is None or path == "None":\n        return path\n    def find_checkpoint_dir(path):\n        probable_checkpoint_dirs = [dir for dir in os.listdir(path) if re.search(r"checkpoint-(\\d+)", dir)]\n        if len(probable_checkpoint_dirs) == 1:\n            return probable_checkpoint_dirs[0]\n        elif len(probable_checkpoint_dirs) > 1:\n            print(f"multiple checkpoint dir in {path}, choose the last one")\n            probable_checkpoint_dirs = sorted(probable_checkpoint_dirs, key=lambda dir:int(re.search(r"checkpoint-(\\d+)", dir).group(1)))\n            return probable_checkpoint_dirs[-1]\n        else:\n            raise Exception(f"no checkpoint under {path} is found")\n    path_parts = os.path.normpath(path).split("/")\n    for i, part in enumerate(path_parts):\n        if part == "checkpoint":\n            parent_path = "/".join(path_parts[:i])\n            path_parts[i] = find_checkpoint_dir(parent_path)\n    return "/".join(path_parts)\n\ndef load_checkpoint(path, device):\n    import torch\n    device = torch.device(device) if type(device) is str else device\n    if os.path.exists(os.path.join(path, "pytorch_model.bin")):\n        state_dict = torch.load(os.path.join(path, "pytorch_model.bin"), map_location=device)\n    elif os.path.exists(os.path.join(path, "model.safetensors")):\n        from safetensors.torch import load_file\n        state_dict = load_file(os.path.join(path, "model.safetensors"), device="cpu")\n        state_dict = {k:v.to(device) for k,v in state_dict.items()}\n    else:\n        raise ValueError(f"no checkpoint file found in {path}")\n    return state_dict\n\ndef pad_and_concat(list_tensors, pad_token_id, padding_side="right", dim=-1):\n    import torch\n    max_length = max([tensor.shape[dim] for tensor in list_tensors])\n    list_padded_tensors = []\n    for tensor in list_tensors:\n        tensor_shape = list(tensor.shape)\n        padding_shape = tensor_shape\n        padding_shape[dim] = max_length - tensor_shape[dim]\n        padding = torch.empty(size=padding_shape, dtype=tensor.dtype).fill_(pad_token_id)\n        if padding_side == "right":\n            list_padded_tensors.append(torch.cat([tensor, padding], dim=dim))\n        elif padding_side == "left":\n            list_padded_tensors.append(torch.cat([padding, tensor], dim=dim))\n        else:\n            raise NotImplementedError(padding_side)\n    concated_tensors = torch.cat(list_padded_tensors, dim=0)\n    return concated_tensors', './LPO-dailydialog\\dailydialog_comparison_dataset.py': 'import os\nimport json\nimport argparse\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom config_and_utils import (\n    get_tokenizer, pad_and_concat,\n    get_reward_fn, intent_label2id\n)\nfrom dailydialog_dataset import DailyDialogDataset\nfrom gpu_map_reduce import TaskConsumer, TaskProducer, BatchTaskConsumer\n\ndef pad_or_truncate(ids, target_length, pad_id):\n    if ids.shape[1] > target_length:\n        ids = ids[:,:target_length]\n    elif ids.shape[1] < target_length:\n        padding_ids = torch.LongTensor(size=(ids.shape[0], target_length-ids.shape[1]))\n        padding_ids.fill_(pad_id)\n        padding_ids = padding_ids.to(ids.device)\n        ids = torch.cat([ids, padding_ids], dim=1)\n    return ids\n\n\nclass DailyDialogComparisonDataset:\n    def __init__(self, usage="train", preference="consistency"):\n        cache_file = f"one2many_inference_with_intent_scores.json"\n        with open(cache_file, "r", encoding="utf-8") as f:\n            inference = json.loads(f.read())\n        assert not any([data is None for data in inference]), f"the data in cache file contains \'None\': {cache_file}"\n        self.data = [data for data in inference if data["usage"]==usage]\n        self.usage = usage\n        self.preference = preference\n        self.tokenizer = get_tokenizer()\n\n        self.many = 32\n        self.num_pairs_per_group = 2\n\n    def __len__(self):\n        return len(self.data) * self.num_pairs_per_group\n\n    def intent_analysis(self):\n        preference2scores = {preference:[] for preference in (list(intent_label2id.keys())+["consistency"])}\n        for data in self.data:\n            for preference in preference2scores:\n                preference2scores[preference] += data[f"{preference}_score"]\n        preference2scores = pd.DataFrame({k:np.mean(v) for k,v in preference2scores.items()}, index=["mean"])\n        print(preference2scores)\n\n    def __getitem__(self, item):\n        item, bias = item // self.num_pairs_per_group, item % self.num_pairs_per_group\n\n        data = self.data[item]\n        prompt_ids = data["input_ids"]#[0]\n        prompt_len = len(prompt_ids)\n        prompt_ids = torch.LongTensor(prompt_ids)\n        prompt = self.tokenizer.batch_decode(prompt_ids, skip_special_tokens=True)[0]\n        reward = torch.Tensor(data[f"{self.preference}_score"])\n\n        sorted_reward, indices = torch.sort(reward, descending=True)\n        indices = indices.tolist()\n        chosen = data["generation"][indices[bias]]\n        rejected = data["generation"][indices[bias-self.num_pairs_per_group]]\n        chosen_ids = self.tokenizer(chosen+self.tokenizer.eos_token, return_tensors="pt").input_ids\n        rejected_ids = self.tokenizer(rejected+self.tokenizer.eos_token, return_tensors="pt").input_ids\n        return prompt_ids, chosen_ids, rejected_ids\n\n    def lpo_collate_fn(self, batch_items):\n        batch_prompt_ids, batch_chosen_ids, batch_rejected_ids = [], [], []\n        for prompt_ids, chosen_ids, rejected_ids in batch_items:\n            batch_prompt_ids.append(prompt_ids)\n            batch_chosen_ids.append(chosen_ids)\n            batch_rejected_ids.append(rejected_ids)\n\n        pad_token_id = self.tokenizer.pad_token_id\n        return {\n            "prompt_ids": pad_and_concat(batch_prompt_ids, pad_token_id),\n            "chosen_ids": pad_and_concat(batch_chosen_ids, pad_token_id),\n            "rejected_ids": pad_and_concat(batch_rejected_ids, pad_token_id),\n        }\n\n    def dpo_collate_fn(self, batch_items):\n        input_ids, labels = [], []\n        for prompt_ids, chosen_ids, rejected_ids in batch_items:\n            chosen_input_ids = torch.cat([prompt_ids, chosen_ids], dim=1)\n            rejected_input_ids = torch.cat([prompt_ids, rejected_ids], dim=1)\n            chosen_labels = torch.cat([prompt_ids.clone().fill_(-100), chosen_ids], dim=1)\n            rejected_labels = torch.cat([prompt_ids.clone().fill_(-100), rejected_ids], dim=1)\n            input_ids.append(chosen_input_ids)\n            input_ids.append(rejected_input_ids)\n            labels.append(chosen_labels)\n            labels.append(rejected_labels)\n        inputs = {\n            "input_ids": pad_and_concat(input_ids, self.tokenizer.pad_token_id).long(),\n            "labels": pad_and_concat(labels, -100).long()\n        }\n        return inputs\n\n\nclass RewardProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        input_file = f"one2many_inference.json"\n        with open(input_file, "r", encoding="utf-8") as f:\n            tasks = json.loads(f.read())\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None and \\\n               all([task[key]==result[key]\n                    for key in ["input_ids", "usage", "index", "prompt", "generation"]]) and \\\n               all([f"{preference}_score" in result for preference in\n                    (list(intent_label2id.keys())+["consistency"])])\n\n\nclass RewardConsumer(TaskConsumer):\n    def init_model(self) -> None:\n        self.reward_fn = get_reward_fn(device=self.device)\n        self.data = []\n        for usage in ["train", "validation", "test"]:\n            self.data += DailyDialogDataset(usage=usage).data\n\n    def process_task(self, task: dict) -> dict:\n        item = self.data[task["index"]]\n        if not item["prompt"].replace(" ","").endswith(task["prompt"].replace(" ","")):\n            print(f"prompt not completely matched:\\n"\n                  f"from dataset:\\t{item[\'prompt\'][-50:]}\\n"\n                  f"from task:\\t{task[\'prompt\'][-50:]}")\n        prompts = [task["prompt"]] * len(task["generation"])\n        labels = task["generation"]\n        intent_probs = self.reward_fn(prompts, labels, return_probs=True)\n\n        for intent, id in intent_label2id.items():\n            task[f"{intent}_score"] = intent_probs[:,id].tolist()\n        task["consistency_score"] = intent_probs[:,item["intent"]].tolist()\n\n        return task\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce processing on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="one2many_inference_with_intent_scores")\n    parser.add_argument("--output_file", type=str, default=f"one2many_inference_with_intent_scores.json")\n    parser.add_argument("--erase", type=int, default=0)\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    if args.local_rank == -1:\n        producer = RewardProducer(args)\n        producer.run()\n        for usage in ["train", "validation", "test"]:\n            dataset = DailyDialogComparisonDataset(usage)\n            dataset.intent_analysis()\n    else:\n        consumer = RewardConsumer(args)\n        consumer.run()\n', './LPO-dailydialog\\dailydialog_dataset.py': 'import json\n\nimport torch\nfrom tqdm import tqdm\n\nfrom config_and_utils import (\n    get_tokenizer, pad_and_concat,\n    input_max_length, output_max_length,\n    BASE_RWD_MODEL_PATH, get_sft_dataset,\n)\nfrom transformers import AutoTokenizer\n\n\ndef pad_or_truncate(ids, target_length, pad_id):\n    if ids.shape[1] > target_length:\n        ids = ids[:,:target_length]\n    elif ids.shape[1] < target_length:\n        padding_ids = torch.LongTensor(size=(ids.shape[0], target_length-ids.shape[1]))\n        padding_ids.fill_(pad_id)\n        padding_ids = padding_ids.to(ids.device)\n        ids = torch.cat([ids, padding_ids], dim=1)\n    return ids\n\n\nclass DailyDialogDataset:\n    EOU_TOKEN = " <EOU> "\n    def __init__(self, usage="train", context_size=5):\n        self.dataset = get_sft_dataset(usage=usage)\n        self.usage = usage\n        self.tokenizer = get_tokenizer()\n        self.rwd_tokenizer = AutoTokenizer.from_pretrained(BASE_RWD_MODEL_PATH)\n        self.data = []\n        for item in tqdm(self.dataset):\n            contexts = []\n            for utterance, emotion, intent in zip(\n                item["dialog"], item["emotion"], item["act"]\n            ):\n                utterance = utterance.strip()\n                if len(contexts) >= context_size:\n                    context = self.EOU_TOKEN.join(contexts[-context_size:]) + self.EOU_TOKEN\n                    target = utterance + self.EOU_TOKEN\n                    self.data.append({\n                        "prompt": context,\n                        "label": target,\n                        "emotion": emotion,\n                        "intent": intent - 1\n                    })\n                contexts.append(utterance)\n\n    def intent_analysis(self, context_size=5):\n        all_intents = []\n        for item in tqdm(self.dataset):\n            contexts = []\n            for utterance, emotion, intent in zip(\n                    item["dialog"], item["emotion"], item["act"]\n            ):\n                if len(contexts) >= context_size:\n                    all_intents.append(intent-1)\n                contexts.append(utterance)\n        print(f"intent analysis with context size {context_size}:")\n        for i in range(max(all_intents)+1):\n            print(f"number / ratio of intent-{i}: {all_intents.count(i)} / {all_intents.count(i)/len(all_intents):.5f}")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        prompt, label, intent = self.data[item]["prompt"], self.data[item]["label"], self.data[item]["intent"]\n        self.tokenizer.truncation_side = "left"\n        prompt_ids = self.tokenizer([prompt], return_tensors="pt", truncation=True,\n                                   max_length=input_max_length).input_ids\n        self.tokenizer.truncation_side = "right"\n        label_ids = self.tokenizer([label+self.tokenizer.eos_token], return_tensors="pt", truncation=True,\n                               max_length=output_max_length).input_ids\n        return {"prompt_ids": prompt_ids, "label_ids": label_ids, "intent": torch.LongTensor([intent])}\n\n    def sft_collate_fn(self, batch_items):\n        input_ids = pad_and_concat(\n            [torch.cat([item["prompt_ids"], item["label_ids"]], dim=1)\n             for item in batch_items],\n            pad_token_id=self.tokenizer.pad_token_id,\n            padding_side="left", dim=1\n        )\n        labels = pad_and_concat(\n            [torch.cat([item["prompt_ids"].clone().fill_(-100), item["label_ids"]], dim=1)\n             for item in batch_items],\n            pad_token_id=-100,\n            padding_side="left", dim=1\n        )\n        return {\n            "input_ids": input_ids,\n            "labels": labels,\n        }\n\n    def context_utterance_to_rwd_inputs(self, context, utterance):\n        from config_and_utils import context_utterance_to_rwd_inputs\n        return context_utterance_to_rwd_inputs(\n            context=context,\n            utterance=utterance,\n            tokenizer=self.rwd_tokenizer\n        )\n\n    def rwd_collate_fn(self, batch_items):\n        batch_inputs = []\n        for batch_item in batch_items:\n            context = self.tokenizer.decode(batch_item["prompt_ids"][0], skip_special_tokens=True)\n            utterance = self.tokenizer.decode(batch_item["label_ids"][0], skip_special_tokens=True)\n            inputs = self.context_utterance_to_rwd_inputs(context, utterance)\n            batch_inputs.append(inputs)\n\n        input_ids = pad_and_concat([batch_input[0] for batch_input in batch_inputs],\n                                   pad_token_id=self.rwd_tokenizer.pad_token_id)\n        attention_mask = pad_and_concat([batch_input[1] for batch_input in batch_inputs],\n                                        pad_token_id=0)\n        token_type_ids = pad_and_concat([batch_input[2] for batch_input in batch_inputs],\n                                        pad_token_id=2)\n        labels = torch.cat([item["intent"] for item in batch_items], dim=0)\n        return {\n            "input_ids": input_ids,\n            "attention_mask": attention_mask,\n            "token_type_ids": token_type_ids,\n            "labels": labels,\n        }\n\n    def save_to_json(self):\n        with open(f"{self.usage}_data.json", "w") as f:\n            f.write(json.dumps(self.data, ensure_ascii=False, indent=4))\n\n\nif __name__ == "__main__":\n    for usage in ["train", "validation", "test"]:\n        dataset = DailyDialogDataset(usage)\n        print(f"{usage}: {len(dataset)}")\n        dataset.intent_analysis(context_size=5)\n        dataset.intent_analysis(context_size=0)\n        dataset.save_to_json()', './LPO-dailydialog\\dailydialog_one2many_dataset.py': 'import os\nimport json\nimport argparse\n\nimport torch\nimport numpy as np\n\nfrom config_and_utils import (\n    get_tokenizer, input_max_length, output_max_length,\n    auto_find_checkpoint_dir, pad_and_concat, truncate_generation_after_first_EOU\n)\nfrom dailydialog_dataset import DailyDialogDataset\n\nfrom gpu_map_reduce import BatchTaskConsumer, TaskConsumer, TaskProducer\n\n\ndef pad_or_truncate(ids, target_length, pad_id):\n    if ids.shape[1] > target_length:\n        ids = ids[:,:target_length]\n    elif ids.shape[1] < target_length:\n        padding_ids = torch.LongTensor(size=(ids.shape[0], target_length-ids.shape[1]))\n        padding_ids.fill_(pad_id)\n        padding_ids = padding_ids.to(ids.device)\n        ids = torch.cat([ids, padding_ids], dim=1)\n    return ids\n\n\nclass DailyDialogOne2ManyDataset:\n    def __init__(self, usage="train", total_many=32):\n        cache_file = f"one2many_inference.json"\n        with open(cache_file, "r", encoding="utf-8") as f:\n            inference = json.loads(f.read())\n        assert not any([data is None for data in inference]), f"the data in cache file contains \'None\': {cache_file}"\n        self.data = [data for data in inference if data["usage"]==usage and len(data["generation"])>=total_many]\n        for data in self.data:\n            data["generation"] = data["generation"][:total_many]\n        self.usage = usage\n        self.tokenizer = get_tokenizer()\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        data = self.data[item]\n        prompt_ids = torch.LongTensor(data["input_ids"])\n        prompt_len = prompt_ids.shape[1]\n        generation = data["generation"]\n        input_ids = prompt_ids\n        # [1, prompt_len]\n        #labels = torch.where(pred_ids == self.tokenizer.pad_token_id, -100, pred_ids)\n        labels = []\n        for gen in generation:\n            label_ids = self.tokenizer([gen+self.tokenizer.eos_token], return_tensors="pt",\n                                       truncation=True, max_length=output_max_length).input_ids\n            label_ids = torch.cat([label_ids, torch.empty(size=(1, output_max_length-label_ids.shape[1]),\n                                                          device=label_ids.device,\n                                                          dtype=label_ids.dtype).fill_(-100)], dim=1)\n            labels.append(label_ids)\n        labels = torch.concat(labels, dim=0)\n        # [N, output_max_length]\n        prior_ids = pad_or_truncate(prompt_ids, input_max_length, self.tokenizer.pad_token_id)\n        # [1, input_max_length]\n        post_ids = self.tokenizer(generation, return_tensors="pt", padding="max_length",\n                                  truncation=True, max_length=output_max_length).input_ids\n        # [N, output_max_length]\n\n        N = len(data["generation"])\n        assert input_ids.shape == (1, prompt_len), input_ids.shape\n        assert labels.shape == (N, output_max_length), labels.shape\n        assert prior_ids.shape == (1, input_max_length), prior_ids.shape\n        assert post_ids.shape == (N, output_max_length), post_ids.shape\n        assert not torch.any(input_ids==self.tokenizer.pad_token_id), input_ids\n\n        return {\n            "input_ids": input_ids,\n            "labels": labels,\n            "prior_ids": prior_ids,\n            "post_ids": post_ids\n        }\n\n    def collate_fn(self, batch_items):\n        assert len(batch_items) == 1, f"batch_size {len(batch_items)} > 1 " \\\n                                      f"is not supported in one2many training"\n        return batch_items[0]\n\n    def vae_collate_fn(self, batch_items):\n        assert len(batch_items) == 1, f"batch_size {len(batch_items)} > 1 " \\\n                                      f"is not supported in one2many training"\n        post_ids = batch_items[0]["post_ids"]\n        prompt_ids = batch_items[0]["input_ids"]\n        pred_labels = batch_items[0]["labels"]\n        prompt_ids = prompt_ids.repeat(pred_labels.shape[0], 1)\n        input_ids = torch.cat([prompt_ids, torch.where(pred_labels==-100, self.tokenizer.pad_token_id, pred_labels)], dim=1)\n        labels = torch.cat([torch.zeros_like(prompt_ids).fill_(-100), pred_labels], dim=1)\n        return {\n            "post_ids": post_ids,\n            "input_ids": input_ids,\n            "labels": labels,\n        }\n\n    def sft_collate_fn(self, batch_items):\n        assert len(batch_items) == 1, f"batch_size {len(batch_items)} > 1 " \\\n                                      f"is not supported in one2many training"\n        prompt_ids = batch_items[0]["input_ids"]\n        pred_labels = batch_items[0]["labels"]\n        prompt_ids = prompt_ids.repeat(pred_labels.shape[0], 1)\n        input_ids = torch.cat([prompt_ids, torch.where(pred_labels==-100, self.tokenizer.pad_token_id, pred_labels)], dim=1)\n        labels = torch.cat([torch.zeros_like(prompt_ids).fill_(-100), pred_labels], dim=1)\n        return {\n            "input_ids": input_ids,\n            "labels": labels,\n        }\n\n\ndef extend_one2many_dataset_with_mini_many(one2many_dataset_class):\n    class one2many_dataset_class_with_across_batch(one2many_dataset_class):\n        def __init__(self, total_many, mini_many, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.total_many = total_many\n            self.mini_many = mini_many\n            self.index_queue = []\n            self.present_index = None\n            self.present_order = None\n            self.present_bias = 0\n\n        def __getitem__(self, index):\n            self.index_queue.append(index)\n            if self.present_bias == 0:\n                self.present_index = self.index_queue.pop(0)\n                self.present_order = torch.from_numpy(np.random.permutation(self.total_many))\n            else:\n                self.present_order = torch.roll(self.present_order, self.mini_many, dims=0)\n            self.present_bias = (self.present_bias + self.mini_many) % self.total_many\n\n            item = super().__getitem__(self.present_index)\n            item_new = {}\n            for k,v in item.items():\n                if v.shape[0] == self.total_many:\n                    v = v[self.present_order]\n                    v = v.view(self.total_many // self.mini_many, self.mini_many, *v.shape[1:])\n                    item_new[k] = v\n                else:\n                    item_new[k] = v\n            return item_new\n\n    return one2many_dataset_class_with_across_batch\n\n\nclass One2ManyInferenceProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        tasks = []\n        for usage in ["train", "validation", "test"]:\n            dataset = DailyDialogDataset(usage)\n            for item in dataset:\n                task = {"input_ids": item["prompt_ids"].tolist(),\n                        "usage": usage, "from_model": self.args.sft_model_path}\n                tasks.append(task)\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None and \\\n               all([task[key]==result[key]\n                    for key in ["input_ids", "usage", "from_model"]])\n\n\nclass One2ManyInferenceConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        from transformers import GPT2LMHeadModel\n        self.model = GPT2LMHeadModel.from_pretrained(\n            self.args.sft_model_path,\n            device_map=f"cuda:{self.args.local_rank}",\n            torch_dtype=torch.float16,\n        ).eval()\n        self.tokenizer = get_tokenizer()\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        many = 32\n        inputs_len = max([len(task["input_ids"][0]) for task in tasks])\n        input_ids = pad_and_concat(\n            [torch.LongTensor(task["input_ids"]) for task in tasks],\n            pad_token_id=self.tokenizer.pad_token_id,\n            padding_side="left"\n        ).to(self.device)\n        assert input_ids.shape[1] == inputs_len\n\n        with torch.no_grad():\n            output_ids = self.model.generate(\n                input_ids=input_ids,\n                do_sample=True,\n                top_k=20,\n                min_length=2,\n                max_new_tokens=output_max_length,\n                num_return_sequences=many,\n                eos_token_id=self.tokenizer.eos_token_id,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        outputs = self.tokenizer.batch_decode(output_ids[:, inputs_len:], skip_special_tokens=True)\n        prompts = self.tokenizer.batch_decode(output_ids[:, :inputs_len], skip_special_tokens=True)\n        for i, task in enumerate(tasks):\n            task[\'prompt\'] = prompts[i*many]\n            assert all([prompts[i*many+j]==task[\'prompt\'] for j in range(many)])\n            task["generation"] = [truncate_generation_after_first_EOU(outputs[i*many+j]) for j in range(many)]\n\n        return tasks\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce processing on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="one2many_inference")\n    parser.add_argument("--output_file", type=str, default=f"one2many_inference.json")\n    parser.add_argument("--erase", type=int, default=0)\n    parser.add_argument("--sft_model_path", type=auto_find_checkpoint_dir,\n                        default=auto_find_checkpoint_dir(f"sft/checkpoint"))\n    parser.add_argument("--batch_size", type=int, default=32)\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    if args.local_rank == -1:\n        producer = One2ManyInferenceProducer(args)\n        producer.run()\n    else:\n        consumer = One2ManyInferenceConsumer(args)\n        consumer.run()\n', './LPO-dailydialog\\gpu_map_reduce.py': '"""\nThis Python file implements a Map-Reduce framework with a TaskProducer abstract class and a\nTaskConsumer abstract class. Each TaskConsumer instance independently utilizes a GPU to load\na Torch model for data processing, i.e., in data-parallel.\n\nClasses:\n- TaskProducer:\n    load tasks from file\n    load and check cached results from file\n    clear the present tasks and results in redis queue (may be left by killed programmes)\n    launch consumers and send tasks to them\n    recieve results from consumers and save them to file\n    stop consumers at the end\n    - abstractmethod:\n        load_tasks\n        cached_result_is_valid\n- TaskConsumer:\n    init model on corresponding gpu (specified by local_rank)\n    recieve tasks from producer\n    process tasks with model\n    send results to producer\n    - abstractmethod:\n        init_model\n        process_task\nFunctions:\n- parse_args (demo):\n    parse args for TaskProducer and TaskConsumer\n- main (demo):\n    the programme entrance\n\nNote:\nThis script requires redis server running in the background for Inter-Process Communication.\nYou can install redis server and run it through the following commands:\n# apt-get install redis-server\n# redis-server --port PORT\n"""\nimport os\nimport re\nimport sys\nimport json\nimport time\nimport argparse\nimport threading\nimport concurrent\n\nimport redis\nimport torch\nfrom tqdm import tqdm\nfrom abc import ABC, abstractmethod\n\nclass TaskProducer(ABC):\n    def __init__(self, args):\n        self.args = args\n        self.producer = redis.Redis(host=args.host, port=args.port, db=0)\n        self.consumer = redis.Redis(host=args.host, port=args.port, db=1)\n        self.task_queue_name = f\'{self.args.task_name}_task\'\n        self.result_queue_name = f\'{self.args.task_name}_result\'\n        self.has_slept_since_last_save = False\n\n    def clear_queue(self):\n        num_tasks_cleared = 0\n        task = self.producer.lpop(self.task_queue_name)\n        while task is not None:\n            num_tasks_cleared += 1\n            task = self.producer.lpop(self.task_queue_name)\n\n        num_results_cleared = 0\n        result = self.consumer.lpop(self.result_queue_name)\n        while result is not None:\n            num_results_cleared += 1\n            result = self.consumer.lpop(self.result_queue_name)\n\n        print(f"clear {num_tasks_cleared} tasks and {num_results_cleared} results in queue")\n        return num_tasks_cleared, num_results_cleared\n\n    def auto_clear_queue(self):\n        stop_clearing = False\n        while not stop_clearing:\n            num_tasks_cleared, num_results_cleared = self.clear_queue()\n            def input_choice():\n                time.sleep(10)\n                return num_tasks_cleared == 0 and num_results_cleared == 0\n            executor = concurrent.futures.ThreadPoolExecutor()\n            future = executor.submit(input_choice)\n            try:\n                stop_clearing = future.result(timeout=30)\n            except concurrent.futures.TimeoutError:\n                stop_clearing = num_tasks_cleared == 0 and num_results_cleared == 0\n            finally:\n                executor.shutdown(wait=False)\n\n    def wait_for_next_result(self):\n        while True:\n            result = self.consumer.lpop(self.result_queue_name)\n            if result is not None:\n                result = json.loads(result.decode(\'utf-8\'))\n                return result\n            time.sleep(1)\n            self.has_slept_since_last_save = True\n\n    @abstractmethod\n    def load_tasks(self) -> list[dict]:\n        tasks = []\n        return tasks\n\n    def save_results(self):\n        num_valid_results = len([_ for _ in self.results if _ is not None])\n        print(f"save results ({num_valid_results}/{len(self.tasks)}) into {self.args.output_file}")\n        with open(self.args.output_file, "w", encoding="utf-8") as f:\n            f.write(json.dumps(self.results, ensure_ascii=False, indent=4))\n\n    def load_cached_results(self):\n        if os.path.exists(self.args.output_file) and not self.args.erase:\n            with open(self.args.output_file, "r", encoding="utf-8") as f:\n                results = json.loads(f.read())\n            if len(results) == len(self.tasks):\n                return results\n        results = [None for index in range(len(self.tasks))]\n        return results\n\n    @abstractmethod\n    def cached_result_is_valid(self, task, result) -> bool:\n        return True\n\n    def push_tasks(self):\n        num_cached_results, num_pushed_tasks = 0, 0\n        for index, task in enumerate(tqdm(self.tasks, desc=f"pushing tasks into {self.task_queue_name}...")):\n            if "index" not in task:\n                task["index"] = index\n            if self.results[index] is not None:\n                if self.cached_result_is_valid(task, self.results[index]):\n                    num_cached_results += 1\n                    continue\n                else:\n                    self.results[index] = None\n            self.producer.rpush(self.task_queue_name, json.dumps(task, ensure_ascii=False))\n            num_pushed_tasks += 1\n        print(f"read {num_cached_results} results from cache_file and push {num_pushed_tasks} tasks into {self.task_queue_name}")\n        return num_cached_results, num_pushed_tasks\n\n    def recieve_results(self):\n        first_result = self.wait_for_next_result()\n        for num_received_results in tqdm(range(self.num_pushed_tasks), desc=f"receiving results from {self.result_queue_name}..."):\n            if num_received_results == 0:\n                result = first_result\n            else:\n                result = self.wait_for_next_result()\n            index = result["index"]\n            while not self.cached_result_is_valid(self.tasks[index], result):\n                print(f"invalid result is received and ignored:\\n{json.dumps(result, ensure_ascii=False, indent=4)}")\n                result = self.wait_for_next_result()\n                index = result["index"]\n            self.results[index] = result\n            num_valid_results = self.num_cached_results + num_received_results + 1\n            if num_valid_results % max(len(self.tasks)//100, 100) == 0 and self.has_slept_since_last_save:\n                self.save_results()\n                self.has_slept_since_last_save = False\n\n    def run(self):\n        self.tasks = self.load_tasks()\n        self.results = self.load_cached_results()\n        self.auto_clear_queue()\n        self.num_cached_results, self.num_pushed_tasks = self.push_tasks()\n\n        if self.num_pushed_tasks > 0:\n            self.launch_consumers()\n            self.recieve_results()\n            self.save_results()\n            self.stop_consumers()\n\n    def launch_consumers(self):\n        def launch_consumers(args):\n            args_input = vars(args)\n            args_input = {k:v for k,v in args_input.items() if v is not None}\n            for local_rank in range(torch.cuda.device_count()):\n                args_input["local_rank"] = local_rank\n                argv = \' \'.join([f\'--{k} "{v}"\' for k, v in args_input.items()])\n                cmd = f\'nohup python {args.py_file_name} {argv}\' \\\n                      f\' > nohup.{args.py_file_name}.rank_{local_rank}.txt 2>&1 &\'\n                print(f"{cmd}")\n                os.system(cmd)\n        launch_consumers(self.args)\n\n    def stop_consumers(self):\n        for local_rank in range(torch.cuda.device_count()):\n            task = {"signal": "exit"}\n            self.producer.rpush(self.task_queue_name, json.dumps(task, ensure_ascii=False))\n\nclass TaskConsumer(ABC):\n    def __init__(self, args):\n        self.args = args\n        self.consumer = redis.Redis(host=args.host, port=args.port, db=0)\n        self.producer = redis.Redis(host=args.host, port=args.port, db=1)\n        self.task_queue_name = f\'{self.args.task_name}_task\'\n        self.result_queue_name = f\'{self.args.task_name}_result\'\n        self.device = torch.device(f"cuda:{args.local_rank}")\n        self.local_task_queue, self.task_queue_lock = [], threading.Lock()\n        self.local_result_queue, self.result_queue_lock = [], threading.Lock()\n        self.exit_signal = threading.Event()\n\n    @abstractmethod\n    def init_model(self) -> None:\n        pass\n\n    def listening(self):\n        num_tasks_per_sleep = 0\n        num_tasks_before_sleep = 0\n        while not self.exit_signal.is_set():\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            num_tasks_this_sleep = num_tasks_before_sleep - num_local_tasks\n            num_tasks_per_sleep = max(int(num_tasks_per_sleep * 0.8), num_tasks_this_sleep)\n            num_tasks_expected_in_queue = max(10, num_tasks_per_sleep * 2)\n            if num_local_tasks >= num_tasks_expected_in_queue:\n                num_tasks_before_sleep = num_local_tasks\n                time.sleep(1)\n                continue\n            for _ in range(num_tasks_expected_in_queue-num_local_tasks):\n                task = self.consumer.lpop(self.task_queue_name)\n                if task is None:\n                    break\n                task = json.loads(task.decode(\'utf-8\'))\n                if "signal" in task and task["signal"] == "exit":\n                    self.exit_signal.set()\n                    break\n                with self.task_queue_lock:\n                    self.local_task_queue.append(task)\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            num_tasks_before_sleep = num_local_tasks\n            time.sleep(1)\n\n    def sending(self):\n        while not self.exit_signal.is_set():\n            with self.result_queue_lock:\n                num_local_results = len(self.local_result_queue)\n            if num_local_results == 0:\n                time.sleep(1)\n                continue\n            with self.result_queue_lock:\n                result = self.local_result_queue.pop(0)\n            self.producer.rpush(self.result_queue_name, result)\n\n    def start_communication_threads(self):\n        self.threads = [\n            threading.Thread(target=self.listening),\n            threading.Thread(target=self.sending)\n        ]\n        for thread in self.threads:\n            thread.start()\n\n    def join_communication_threads(self):\n        for thread in self.threads:\n            thread.join()\n\n    @abstractmethod\n    def process_task(self, task: dict) -> dict:\n        result = task\n        return result\n\n    def processing_task_loop(self):\n        while not self.exit_signal.is_set():\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            if num_local_tasks == 0:\n                time.sleep(1)\n                continue\n            with self.task_queue_lock:\n                task = self.local_task_queue.pop(0)\n            result = self.process_task(task)\n            with self.result_queue_lock:\n                self.local_result_queue.append(json.dumps(result, ensure_ascii=False))\n\n    def run(self):\n        self.init_model()\n        self.start_communication_threads()\n        self.processing_task_loop()\n        self.join_communication_threads()\n\nclass BatchTaskConsumer(TaskConsumer):\n    def __init__(self, args):\n        super().__init__(args)\n        self.batch_size = getattr(args, "batch_size", 2)\n\n    def process_task(self, task: dict) -> dict:\n        pass\n\n    @abstractmethod\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        results = tasks\n        return results\n\n    def processing_task_loop(self):\n        while not self.exit_signal.is_set():\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            if num_local_tasks == 0:\n                time.sleep(1)\n                continue\n            with self.task_queue_lock:\n                tasks = self.local_task_queue[:self.batch_size]\n                self.local_task_queue = self.local_task_queue[self.batch_size:]\n            results = self.process_tasks(tasks)\n            with self.result_queue_lock:\n                for result in results:\n                    self.local_result_queue.append(json.dumps(result, ensure_ascii=False))\n\n\n# implementation examples\nclass MyProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        tasks = []\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return True\n\n\nclass MyConsumer(TaskConsumer):\n    def init_model(self) -> None:\n        pass\n\n    def process_task(self, task: dict) -> dict:\n        result = task\n        return result\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce processing on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="inference")\n    parser.add_argument("--output_file", type=str, default=None)\n    parser.add_argument("--erase", type=int, default=0)\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    return args\n\n\ndef main():\n    args = parse_args()\n    if args.local_rank == -1:\n        producer = TaskProducer(args)\n        producer.run()\n    else:\n        consumer = TaskConsumer(args)\n        consumer.run()\n\n\nif __name__ == "__main__":\n    main()\n', './LPO-dailydialog\\modeling_gpt2_cvae.py': 'import copy\n\nimport torch.nn.functional as F\nfrom transformers.models.gpt2.modeling_gpt2 import *\n\nfrom config_and_utils import BASE_MODEL_PATH, load_checkpoint\n\n\ndef sampling(mean, logvar, n_samples=1):\n    mu = torch.stack([mean] * n_samples, dim=-1)\n    sigma = torch.stack([torch.exp(logvar * 0.5)] * n_samples, dim=-1)\n    eps = torch.zeros_like(sigma).normal_()\n    zs = eps * sigma + mu\n    return zs\n\n\ndef log_pdf(mean, logvar, zs) -> torch.Tensor:\n    import numpy as np\n    if len(zs.shape) == len(mean.shape) + 1:\n        mean = mean.unsqueeze(-1)\n        logvar = logvar.unsqueeze(-1)\n    return -0.5 * np.log(2 * np.pi) - 0.5 * logvar - \\\n           (zs - mean).pow(2) / (2 * torch.exp(logvar) + 1e-4)\n\n\ndef dgkld(config, prior_mean, prior_logvar, post_mean, post_logvar):\n    batch_size, dim_z = prior_mean.shape\n\n    n_samples = config.kl_sampling_times\n    zs = sampling(post_mean, post_logvar, n_samples)\n\n    priors_mean = prior_mean.unsqueeze(-1).repeat(1, 1, n_samples)\n    priors_logvar = prior_logvar.unsqueeze(-1).repeat(1, 1, n_samples)\n    # [batch_size, dim_z, n_samples]\n    logp_priors_zs = log_pdf(priors_mean, priors_logvar, zs)\n    # e.g.\n    #         z1 z2 z3\n    # prior   .. .. ..\n    # [batch_size, dim_z, n_samples]\n\n    zs = zs.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\n    posts_mean = post_mean.unsqueeze(1).unsqueeze(-1).repeat(1, batch_size, 1, n_samples)\n    posts_logvar = post_logvar.unsqueeze(1).unsqueeze(-1).repeat(1, batch_size, 1, n_samples)\n    # [batch_size, batch_size, dim_z, n_samples]\n    # the first "batch_size" is of prior/post, and the second "batch_size" is of zs\n    # in another perspective, the first / second is of aggreagation / stratified sampling\n    logp_posts_zs = log_pdf(posts_mean, posts_logvar, zs)\n    # e.g.\n    #        z1 z2 z3\n    # post1  .. .. ..\n    # post2  .. .. ..\n    # post3  .. .. ..\n\n    if config.marginal_kl:\n        # regularization on each dimension respectively\n        logp_posts_zs = logp_posts_zs.view(batch_size, batch_size, dim_z, n_samples)\n        # [batch_size, batch_size, dim_z, n_samples]\n        logp_priors_zs = logp_priors_zs.view(batch_size, dim_z, n_samples)\n        # [batch_size, dim_z, n_samples]\n    else:\n        # regularization in the high-dimensional joint latent space\n        logp_posts_zs = logp_posts_zs.sum(dim=-2, keepdims=True)\n        # [batch_size, batch_size, 1, n_samples]\n        logp_priors_zs = logp_priors_zs.sum(dim=-2, keepdims=True)\n        # [batch_size, 1, n_samples]\n\n    # aggregation: post1(z), post2(z), post3(z) -> post_agg(z)\n\n    logp_posts_max = logp_posts_zs.max(dim=0).values\n    logp_agg_post_zs = (logp_posts_zs - logp_posts_max).exp().mean(dim=0).log() + logp_posts_max\n    # [batch_size, 1 or dim_z, n_samples]\n    # e.g. (the dim of post-agg is removed by mean with keepdims=False)\n    #           z1 z2 z3\n    # post-agg  .. .. ..\n\n    # mote carlo with stratified sampling: post_agg(z1), post_agg(z2), post_agg(z3) -> post_agg(z_agg)\n\n    # mote carlo with stratified sampling: prior(z1), prior(z2), prior(z3) -> prior(z_agg)\n    density_gaps = logp_agg_post_zs - logp_priors_zs\n    # [batch_size, 1 or dim_z, n_samples]\n    kl = density_gaps.mean(dim=0)\n    # [1 or dim_z, n_samples]\n\n    kl = kl.sum(dim=0).mean(dim=-1)\n\n    # []\n    return kl\n\n\nclass GPT2CVAEConfig(GPT2Config):\n    def __init__(\n        self,\n        base_model_path=BASE_MODEL_PATH,\n        num_q=4,\n        dim_z=32,\n        num_p=4,\n        latent_aggregator_layers=2,\n        post_with_x=0,\n        frozen_pretrained=True,\n        use_standard_prior=True,\n        marginal_kl=True,\n        lm_sampling_times=1,\n        kl_sampling_times=16,\n        lpo_sampling_times=64,\n        latent_weight_init_norm=0.1,\n        without_contra=0,\n        without_dg_kld=0,\n        with_bn=0,\n        contra_loss_weight=1.0,\n        two_stage_contra=0,\n        add_skip_connection=0,\n        add_skip_residue=0,\n        add_contra_loss=0,\n        add_skip_residue_contra=0,\n        lpo_post_sampling=0,\n        lpo_evidence_ratio=0.5,\n        marginal_lpo=0,\n        expectation_lpo=0,\n        beta=0.5,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        # for model structure\n        self.base_model_path = base_model_path\n        self.num_q = num_q\n        self.dim_z = dim_z\n        self.num_p = num_p\n        self.latent_aggregator_layers = latent_aggregator_layers\n        self.post_with_x = post_with_x\n        # for training and inference\n        self.frozen_pretrained = frozen_pretrained\n        self.use_standard_prior = use_standard_prior\n        self.marginal_kl = marginal_kl\n        self.lm_sampling_times = lm_sampling_times\n        self.kl_sampling_times = kl_sampling_times\n        self.lpo_sampling_times = lpo_sampling_times\n        self.latent_weight_init_norm = latent_weight_init_norm\n        self.without_contra = without_contra\n        self.without_dg_kld = without_dg_kld\n        self.with_bn = with_bn\n        self.contra_loss_weight = contra_loss_weight\n        self.two_stage_contra = two_stage_contra\n        self.add_skip_connection = add_skip_connection\n        self.add_skip_residue = add_skip_residue\n        self.add_contra_loss = add_contra_loss\n        self.add_skip_residue_contra = add_skip_residue_contra\n        self.lpo_post_sampling = lpo_post_sampling\n        self.lpo_evidence_ratio = lpo_evidence_ratio\n        self.marginal_lpo = marginal_lpo\n        self.expectation_lpo = expectation_lpo\n        self.beta = beta\n        # others\n        self.pad_token_id = self.eos_token_id\n\n\nclass LatentAggregator(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.queries = nn.Parameter(torch.Tensor(1, config.num_q, config.n_embd))\n        self.queries.data.normal_(mean=0.0, std=config.initializer_range)\n        self.ln_input = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.blocks = nn.ModuleList([\n            GPT2Block(config)\n            for _ in range(config.latent_aggregator_layers)\n        ])\n        self.ln_output = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.h2z = nn.Linear(config.n_embd, config.dim_z * 2 // config.num_q)\n\n    def standardize(self):\n        # regularize the output latent distribution to standard gaussian\n        self.h2z.weight.data.zero_()\n        self.h2z.bias.data.zero_()\n        return\n\n    def forward(\n        self,\n        input_embeds: torch.FloatTensor,\n        input_embeds_mask: torch.FloatTensor = None,\n        fix_entropy: bool = False,\n    ):\n        dtype, device = self.queries.dtype, self.queries.device\n        batch_size, seq_len, hidden_size = input_embeds.shape\n        if input_embeds_mask is not None:\n            input_embeds_mask = input_embeds_mask.to(dtype=dtype, device=device)  # fp16 compatibility\n        else:\n            input_embeds_mask = torch.ones([batch_size, seq_len], dtype=dtype, device=device)\n\n        self_attn_mask = torch.ones([batch_size, self.config.num_q], dtype=dtype, device=device)\n\n        attention_mask = torch.cat([input_embeds_mask, self_attn_mask], dim=1)  # [batch_size, to_seq_length]\n        attention_mask = attention_mask[:, None, None, :]  # [batch_size, num_heads, from_seq_length, to_seq_length]\n        attention_mask = (1.0 - attention_mask) * torch.finfo(dtype).min\n\n        hidden_states = torch.cat([input_embeds, self.queries.repeat(batch_size, 1, 1)], dim=1)\n\n        hidden_states = self.ln_input(hidden_states)\n\n        for block in self.blocks:\n            hidden_states = block(\n                hidden_states,\n                attention_mask=attention_mask,\n            )[0]\n        latent = self.h2z(self.ln_output(hidden_states[:, -self.config.num_q:, :]))\n        mean = latent[:, :, :latent.shape[-1]//2].reshape(batch_size, self.config.dim_z)\n        logvar = latent[:, :, latent.shape[-1]//2:].reshape(batch_size, self.config.dim_z)\n\n        if fix_entropy:\n            logvar = logvar - logvar.sum(dim=-1, keepdim=True) / self.config.dim_z\n            # enforce the entropy of prior distribution equal\n            # to that of standard gaussian distribution\n\n        return mean, logvar\n\n\nclass LatentEncoder(nn.Module):\n    def __init__(self, config, fix_entropy=False):\n        super().__init__()\n        self.config = config\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.aggregator = LatentAggregator(config)\n        self.fix_entropy = fix_entropy\n        self.aggregator.standardize()\n\n    def load_pretrained(self, pretrained_model):\n        self.wte.load_state_dict(pretrained_model.transformer.wte.state_dict())\n        for i in range(self.config.latent_aggregator_layers):\n            self.aggregator.blocks[i].load_state_dict(pretrained_model.transformer.h[i].state_dict())\n        self.aggregator.standardize()\n\n    def standardize(self):\n        self.aggregator.standardize()\n\n    def init_wte(self, wte):\n        self.wte.load_state_dict(wte.state_dict())\n\n    def init_wte_from_lm_head(self, lm_head):\n        self.wte.load_state_dict(lm_head.state_dict(), strict=False)\n\n    def roll_right_padding_to_left(self, right_padded_ids):\n        batch_size, seq_len = right_padded_ids.shape\n        last_valid_idx = (right_padded_ids != self.config.pad_token_id).sum(dim=1)\n        left_padded_ids = right_padded_ids.clone()\n        for i in range(batch_size):\n            left_padded_ids[i] = torch.roll(left_padded_ids[i], seq_len - last_valid_idx[i].item(), dims=0)\n        return left_padded_ids\n\n    def forward(self, input_ids):\n        input_ids = self.roll_right_padding_to_left(input_ids).long()\n        input_embeds = self.wte(input_ids)\n        input_embeds_mask = (input_ids != self.config.pad_token_id).float()\n        mean, logvar = self.aggregator(\n            input_embeds=input_embeds,\n            input_embeds_mask=input_embeds_mask,\n            fix_entropy=self.fix_entropy,\n        )\n        return mean, logvar\n\n\nclass LatentDecoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.z2hs = nn.ModuleList([nn.Sequential(\n            nn.Linear(config.dim_z, config.n_embd),\n            nn.GELU(),\n            nn.Linear(config.n_embd, config.n_embd * config.num_p)\n        ) for i in range(config.n_layer)])\n        """\n        self.z2hs = nn.ModuleList([nn.Linear(config.dim_z, config.n_embd * config.num_p)\n                                  for i in range(config.n_layer)])\n        """\n        self.lns = nn.ModuleList([nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n                                  for i in range(config.n_layer)])\n        self.attns = nn.ModuleList([GPT2Attention(config, layer_idx=i)\n                                    for i in range(config.n_layer)])\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n\n    def load_pretrained(self, pretrained_model):\n        for i in range(self.config.n_layer):\n            self.attns[i].load_state_dict(pretrained_model.transformer.h[i].attn.state_dict())\n\n    def decode(self, zs):\n        # input_ps mimic past_key_values in gptj\n        # past_key_values: list (in length of n_layers) of past_key and past_value\n        # past_key and past_value: (batch_size, num_heads, seq_length, head_features)\n        batch_size = zs.shape[0]\n\n        past_key_values = []\n        for i in range(self.config.n_layer):\n            z2h, ln, attn = self.z2hs[i], self.lns[i], self.attns[i]\n            hidden_states = z2h(zs)\n            hidden_states = hidden_states.view(batch_size, self.config.num_p, self.config.n_embd)\n            hidden_states = ln(hidden_states)\n            present = attn(\n                hidden_states=hidden_states,\n                use_cache=True,\n            )[1]\n            past_key_values.append(present)\n        \n        input_ps = past_key_values\n        return input_ps\n\n    def forward(self, mean, logvar):\n        # mean, logvar -> zs -> input_ps\n        if self.config.lm_sampling_times == 0:\n            zs = mean\n        else:\n            zs = sampling(mean, logvar).squeeze(-1)\n        input_ps = self.decode(zs)\n        return input_ps\n\n    def get_latent_nlls(self, zs, mean, logvar):\n        return -log_pdf(mean, logvar, zs).sum(dim=-1)\n\n\nclass GPT2ForVAE(GPT2PreTrainedModel):\n    config_class = GPT2CVAEConfig\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config: GPT2CVAEConfig):\n        super().__init__(config)\n        self.config = config\n        self.latent_encoder = LatentEncoder(config)\n        self.latent_decoder = LatentDecoder(config)\n        self.base = GPT2LMHeadModel(config)\n        self.skip_connection = nn.Sequential(\n            nn.Linear(self.config.dim_z, self.config.n_embd),\n            nn.GELU(),\n            nn.Linear(self.config.n_embd, self.config.n_embd)\n        )\n\n        if self.config.with_bn:\n            self.bn = nn.BatchNorm1d(self.config.dim_z)\n            self.bn.weight.requires_grad = False\n            self.bn.weight.data.fill_(0.5)\n\n        self.update_requires_grad_()\n\n    def update_requires_grad_(self):\n        self.base.requires_grad_(not self.config.frozen_pretrained)\n        self.latent_decoder.attns.requires_grad_(not self.config.frozen_pretrained)\n\n    def load_pretrained(self, pretrained_model=None):\n        if pretrained_model is None:\n            pretrained_model = GPT2LMHeadModel.from_pretrained(self.config.base_model_path)\n        self.base.load_state_dict(pretrained_model.state_dict(), strict=False)\n        self.latent_encoder.load_pretrained(pretrained_model)\n        self.latent_decoder.load_pretrained(pretrained_model)\n\n    def skip_grad_lm_loss(self, post_mean, post_logvar, input_ps, input_ids, labels, grad_weight=1.0, reduction="mean"):\n        last_hidden_state = self.base.transformer(\n            input_ids=input_ids,\n            past_key_values=input_ps,\n            return_dict=True,\n        ).last_hidden_state\n        if self.config.lm_sampling_times == 0:\n            zs = post_mean.unsqueeze(-1).repeat(1, 1, last_hidden_state.shape[1])\n        else:\n            zs = sampling(post_mean, post_logvar, n_samples=last_hidden_state.shape[1])\n        # [batch_size, dim_z, seq_len]\n        skip_hidden_state = F.layer_norm(self.skip_connection(zs.transpose(1, 2)),\n                                         normalized_shape=[self.config.n_embd],\n                                         eps=self.config.layer_norm_epsilon)\n        skip_residue = (skip_hidden_state - skip_hidden_state.detach()) \\\n            if not self.config.add_skip_residue else skip_hidden_state\n        last_hidden_state = last_hidden_state + skip_residue * grad_weight\n        lm_logits = self.base.lm_head(last_hidden_state)\n        nlls_skip = F.cross_entropy(\n            input=lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n            target=labels[:, 1:].contiguous(),\n            reduction=\'none\'\n        ).sum(dim=-1)\n\n        if reduction == "mean" and self.config.add_skip_residue_contra:\n            loss_skip_residue_contra = self.skip_residue_contra_loss(last_hidden_state, skip_hidden_state, labels)\n            return nlls_skip.mean(), loss_skip_residue_contra\n\n        if reduction == "mean":\n            loss_lm_skip = nlls_skip.mean()\n            return loss_lm_skip\n        elif reduction == "none" or reduction is None:\n            return nlls_skip\n        else:\n            raise NotImplementedError(f"reduction: {reduction}")\n\n    def skip_residue_contra_loss(self, last_hidden_state, skip_hidden_state, labels):\n        self.base.requires_grad_(False)\n        in_batch_skip_nlls = []\n        batch_size = last_hidden_state.shape[0]\n        for bias in range(batch_size):\n            biased_skip_hidden_state = torch.roll(skip_hidden_state, shifts=bias, dims=0)\n            biased_skip_last_hidden_state = last_hidden_state.detach() + biased_skip_hidden_state\n            biased_skip_lm_logits = self.base.lm_head(biased_skip_last_hidden_state)\n            biased_skip_nlls = F.cross_entropy(\n                input=biased_skip_lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n                target=labels[:, 1:].contiguous(),\n                reduction=\'none\'\n            ).sum(dim=-1)\n            in_batch_skip_nlls.append(biased_skip_nlls)\n        in_batch_skip_nlls = torch.stack(in_batch_skip_nlls, dim=0)\n        loss_skip_residue_contra = F.cross_entropy(\n            input=-in_batch_skip_nlls.T,\n            target=torch.zeros(batch_size, dtype=torch.int64, device=in_batch_skip_nlls.device),\n            reduction=\'mean\'\n        )\n        self.base.requires_grad_(not self.config.frozen_pretrained)\n        return loss_skip_residue_contra\n\n    def nlls(self, input_ps, input_ids, labels, skip_hidden_state=None):\n        last_hidden_state = self.base.transformer(\n            input_ids=input_ids,\n            past_key_values=input_ps,\n            return_dict=True,\n        ).last_hidden_state\n        if skip_hidden_state is not None:\n            last_hidden_state = last_hidden_state + skip_hidden_state\n        lm_logits = self.base.lm_head(last_hidden_state)\n        nlls = F.cross_entropy(\n            input=lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n            target=labels[:, 1:].contiguous(),\n            reduction=\'none\'\n        ).sum(dim=-1)\n        return nlls\n\n    def lm_loss(self, input_ps, input_ids, labels):\n        return self.nlls(input_ps, input_ids, labels).mean()\n\n    def contra_loss(self, input_ps, input_ids, labels, positive_skip_zs=None):\n        batch_size = labels.shape[0]\n        prefix_length = torch.stack([torch.nonzero(labels[i, :] != -100)[0][0] for i in range(batch_size)], dim=0)\n        assert torch.all(prefix_length == prefix_length[0]), f"unexpected data with variable prefix length: " \\\n                                                             f"{prefix_length}\\n{labels.tolist()}"\n        prefix_length = prefix_length[0].item()\n        assert prefix_length > 0, f"unexpected data with no prefix:\\n{labels.tolist()}"\n        assert torch.all(input_ids[:, :prefix_length] == input_ids[0, :prefix_length]), \\\n            f"unexpected data with non-shared prefix:\\n{input_ids[:, :prefix_length].tolist()}"\n\n        prefix_length -= 1  # the labels will be shifted\n        prefix_ids = input_ids[:, :prefix_length]\n        input_ids = input_ids[:, prefix_length:]\n        labels = labels[:, prefix_length:]\n\n        prompted_prefix_outputs = self.base.transformer(\n            input_ids=prefix_ids,\n            past_key_values=input_ps,\n            use_cache=True,\n            return_dict=True\n        )\n        past_key_values = prompted_prefix_outputs.past_key_values\n\n        in_batch_nlls = []\n        for bias in range(batch_size):\n            biased_past_key_values = [\n                (torch.roll(past_keys, shifts=bias, dims=0),\n                 torch.roll(past_values, shifts=bias, dims=0))\n                for past_keys, past_values in past_key_values\n            ]\n            if positive_skip_zs is not None and bias == 0:\n                skip_hidden_state = F.layer_norm(\n                    self.skip_connection(positive_skip_zs.transpose(1, 2)),\n                    normalized_shape=[self.config.n_embd],\n                    eps=self.config.layer_norm_epsilon\n                )[:, prefix_length:, :]\n                skip_hidden_state = skip_hidden_state - skip_hidden_state.detach()\n            else:\n                skip_hidden_state = None\n            in_batch_nlls.append(self.nlls(biased_past_key_values, input_ids, labels,\n                                           skip_hidden_state=skip_hidden_state))\n        in_batch_nlls = torch.stack(in_batch_nlls, dim=0)\n        loss_contra = F.cross_entropy(\n            input=-in_batch_nlls.T,\n            target=torch.zeros(batch_size, dtype=torch.int64, device=in_batch_nlls.device),\n            reduction=\'mean\'\n        )\n        return loss_contra\n\n    def forward(\n        self,\n        post_ids: torch.LongTensor,  # [batch, ans_seq_len]\n        input_ids: torch.LongTensor,  # [batch, seq_len]\n        labels: torch.LongTensor,  # [batch, seq_len]\n        return_loss=True,\n        **kwargs,\n    ):\n        if self.config.post_with_x:\n            post_mean, post_logvar = self.latent_encoder(input_ids)\n        else:\n            post_mean, post_logvar = self.latent_encoder(post_ids)\n        if self.config.with_bn:\n            post_mean = self.bn(post_mean)\n\n        prior_mean, prior_logvar = torch.zeros_like(post_mean), torch.zeros_like(post_logvar)\n        loss_kld = dgkld(self.config, prior_mean, prior_logvar, post_mean, post_logvar)\n\n        # only compute lm loss for the (currently) concerned output\n        input_ps = self.latent_decoder(post_mean, post_logvar)\n        # n_layer * [mini_many, num_p, n_embd] <- [mini_many, dim_z]\n\n        if self.config.add_contra_loss and self.config.add_skip_connection:\n            positive_skip_zs = sampling(post_mean, post_logvar, n_samples=labels.shape[1])\n            loss_contra = self.contra_loss(input_ps, input_ids, labels, positive_skip_zs=positive_skip_zs)\n            loss_lm = self.lm_loss(input_ps, input_ids, labels)\n        elif self.config.add_contra_loss:\n            loss_contra = self.contra_loss(input_ps, input_ids, labels, positive_skip_zs=None)\n            loss_lm = self.lm_loss(input_ps, input_ids, labels)\n        elif self.config.add_skip_connection:\n            loss_contra = torch.zeros_like(input_ps[0][0].sum())\n            loss_lm = self.skip_grad_lm_loss(post_mean, post_logvar, input_ps, input_ids, labels)\n        else:\n            loss_contra = torch.zeros_like(input_ps[0][0].sum())\n            loss_lm = self.lm_loss(input_ps, input_ids, labels)\n\n        if self.config.without_dg_kld:\n            standard_kld = 0.5 * (post_mean.pow(2) + post_logvar.exp() - post_logvar - 1).sum(dim=1).mean(dim=0)\n            loss_vae = loss_lm + standard_kld + loss_contra\n        else:\n            loss_vae = loss_lm + loss_kld + loss_contra\n\n        return loss_vae, loss_lm, loss_kld, loss_contra\n\n    def load_lpo_policy_latent_encoder(self, lpo_model_path):\n        lpo_model = LPOModel(self.config)\n        lpo_model.load_state_dict(load_checkpoint(lpo_model_path, device=self.device))\n        self.latent_encoder.load_state_dict(lpo_model.policy_latent_encoder.state_dict())\n        self.config.use_standard_prior = False\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=None,\n        streamer=None,\n        post_labels: torch.LongTensor = None,\n        post_latent: Tuple[torch.FloatTensor] = None,\n        standard_prior: bool = False,\n        latent_sampling: bool = None,\n        **kwargs,\n    ):\n        input_ids = kwargs["input_ids"]\n        if latent_sampling is None:\n            latent_sampling = self.config.use_standard_prior or standard_prior\n\n        prior_mean, prior_logvar = self.latent_encoder(input_ids)\n        if post_latent is not None:\n            mean, logvar = post_latent\n            mean = mean.to(device=prior_mean.device, dtype=prior_mean.dtype)\n            logvar = logvar.to(device=prior_logvar.device, dtype=prior_logvar.dtype)\n        elif post_labels is not None:\n            mean, logvar = self.latent_encoder(post_labels)\n        elif self.config.use_standard_prior or standard_prior:\n            mean, logvar = torch.zeros_like(prior_mean), torch.zeros_like(prior_logvar)\n        else:\n            mean, logvar = prior_mean, prior_logvar\n\n        if not latent_sampling:\n            logvar.fill_(-100)\n\n        input_ps = self.latent_decoder(mean, logvar)\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids[:, :-1] != self.config.pad_token_id).long()\n        ], dim=1)\n\n        outputs = self.base.transformer(\n            input_ids=input_ids[:, :-1],\n            past_key_values=input_ps,\n            attention_mask=attention_mask,\n            use_cache=True,\n            return_dict=True\n        )\n        kwargs["past_key_values"] = outputs.past_key_values\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids != self.config.pad_token_id).long()\n        ], dim=1)\n        kwargs["attention_mask"] = attention_mask\n        kwargs["use_cache"] = True\n\n        if self.config.add_skip_connection and self.config.add_skip_residue:\n            if self.config.lm_sampling_times == 0:\n                zs = mean.unsqueeze(-1)\n            else:\n                zs = sampling(mean, logvar, n_samples=1)\n            skip_hidden_state = F.layer_norm(self.skip_connection(zs.transpose(1, 2)),\n                                             normalized_shape=[self.config.n_embd],\n                                             eps=self.config.layer_norm_epsilon)\n            kwargs["skip_residue"] = skip_hidden_state\n\n        return self.base.generate(\n            inputs=inputs,\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            stopping_criteria=stopping_criteria,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            synced_gpus=synced_gpus,\n            streamer=streamer,\n            **kwargs,\n        )\n\n\nclass LPOModel(nn.Module):\n    main_input_name = "prompt_ids"\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.prior_latent_encoder = LatentEncoder(config)\n        self.prior_latent_encoder.requires_grad_(False)\n        self.post_latent_encoder = LatentEncoder(config)\n        self.post_latent_encoder.requires_grad_(False)\n        self.policy_latent_encoder = LatentEncoder(config)\n\n    @classmethod\n    def from_vae(cls, base_model_path, **kwargs):\n        gpt2_vae = GPT2ForVAE.from_pretrained(base_model_path)\n        config = gpt2_vae.config\n        config.update(kwargs)\n        config.use_standard_prior = True\n        model = cls(config)\n        model.policy_latent_encoder.load_state_dict(gpt2_vae.latent_encoder.state_dict())\n        model.policy_latent_encoder.standardize()\n        model.post_latent_encoder.load_state_dict(gpt2_vae.latent_encoder.state_dict())\n        return model\n\n    def forward(\n        self,\n        prompt_ids: torch.LongTensor,  # x\n        chosen_ids: torch.LongTensor,  # y_w\n        rejected_ids: torch.LongTensor,  # y_l\n    ):\n        prior_ids, chosen_post_ids, rejected_post_ids = prompt_ids, chosen_ids, rejected_ids\n        # the latent distribution of pi_theta(z|x), pi_ref(z|x)\n        # and approximation of pi_ref(z|x,y_w), pi_ref(z|x,y_l) (by their posterior distributions)\n        policy_mean, policy_logvar = self.policy_latent_encoder(prior_ids)\n        with torch.no_grad():\n            if self.config.use_standard_prior:\n                prior_mean, prior_logvar = torch.zeros_like(policy_mean), torch.zeros_like(policy_logvar)\n            else:\n                prior_mean, prior_logvar = self.prior_latent_encoder(prior_ids)\n            if self.config.post_with_x:\n                chosen_post_mean, chosen_post_logvar = self.post_latent_encoder(\n                    torch.cat([prior_ids, chosen_post_ids], dim=1)\n                )\n                rejected_post_mean, rejected_post_logvar = self.post_latent_encoder(\n                    torch.cat([prior_ids, rejected_post_ids], dim=1)\n                )\n            else:\n                chosen_post_mean, chosen_post_logvar = self.post_latent_encoder(chosen_post_ids)\n                rejected_post_mean, rejected_post_logvar = self.post_latent_encoder(rejected_post_ids)\n        # batch_size, dim_z\n\n        n_samples = self.config.lpo_sampling_times\n        if self.config.expectation_lpo:\n            zs = sampling(policy_mean, policy_logvar, n_samples=n_samples)\n        else:\n            prior_zs = sampling(prior_mean, prior_logvar, n_samples=n_samples)\n            if self.config.lpo_post_sampling:\n                chosen_zs = sampling(chosen_post_mean, chosen_post_logvar, n_samples=n_samples)\n                rejected_zs = sampling(rejected_post_mean, rejected_post_logvar, n_samples=n_samples)\n                zs = torch.cat([prior_zs, chosen_zs, rejected_zs], dim=-1)\n            else:\n                zs = prior_zs\n        # batch_size, dim_z, n_samples\n\n        # -log pi_ref(z_d|x,y_w)\n        zs_marginal_nlls_given_chosen = -log_pdf(chosen_post_mean, chosen_post_logvar, zs)\n        # -log pi_ref(z_d|x,y_l)\n        zs_marginal_nlls_given_rejected = -log_pdf(rejected_post_mean, rejected_post_logvar, zs)\n        # batch_size, dim_z, n_samples\n\n        if self.config.marginal_lpo:\n            batch_size, dim_z, n_samples = zs_marginal_nlls_given_chosen.shape\n            zs_nlls_given_chosen = zs_marginal_nlls_given_chosen.view(batch_size * dim_z, n_samples)\n            zs_nlls_given_rejected = zs_marginal_nlls_given_rejected.view(batch_size * dim_z, n_samples)\n        else:\n            # -log pi_ref(z|x,y_w)\n            zs_nlls_given_chosen = zs_marginal_nlls_given_chosen.sum(dim=1)\n            # -log pi_ref(z|x,y_l)\n            zs_nlls_given_rejected = zs_marginal_nlls_given_rejected.sum(dim=1)\n\n        # r(x,z) = (w_win*r_win + w_loss*r_loss) / (w_win + w_loss)\n        # w \\propto exp(-zs_nlls_given_chosen/rejected)\n        importance_weights = F.softmax(\n            torch.stack([-zs_nlls_given_chosen, -zs_nlls_given_rejected], dim=-1),\n            dim=-1\n        )\n        win_lose_rewards = torch.Tensor([1, 0]).to(\n            device=importance_weights.device, dtype=importance_weights.dtype\n        )[None, None, :]\n        latent_rewards = (importance_weights * win_lose_rewards).sum(dim=-1)\n        score_zs = latent_rewards\n        # batch_size, n_samples\n\n        zs_marginal_refer_nlls = -log_pdf(prior_mean, prior_logvar, zs)\n        zs_marginal_policy_nlls = -log_pdf(policy_mean, policy_logvar, zs)\n        if self.config.marginal_lpo:\n            zs_refer_nlls = zs_marginal_refer_nlls.view(batch_size * dim_z, n_samples)\n            zs_policy_nlls = zs_marginal_policy_nlls.view(batch_size * dim_z, n_samples)\n        else:\n            zs_refer_nlls = zs_marginal_refer_nlls.sum(dim=1)\n            zs_policy_nlls = zs_marginal_policy_nlls.sum(dim=1)\n        # batch_size, n_samples\n\n        if self.config.expectation_lpo:\n            # maximize E_{z \\sim \\pi_{\\theta}(z|x)} r(x,z) - \\beta [log \\pi_{\\theta}(z|x) - log \\pi_{ref}(z|x)]\n            loss = self.config.beta * (zs_refer_nlls - zs_policy_nlls) - latent_rewards\n            loss = loss.mean()\n            acc = torch.zeros_like(loss)\n            return loss, acc\n\n        comparison_matrix = score_zs[:, :, None] - score_zs[:, None, :]\n        # import pdb;pdb.set_trace()\n        comparison_matrix = comparison_matrix.detach()\n        # batch_size, n_samples, n_samples\n\n        zs_policy_reward = (zs_refer_nlls.detach() - zs_policy_nlls)\n        # batch_size, n_samples\n\n        batch_size, n_samples = zs_policy_reward.shape\n\n        zs_policy_reward_left = zs_policy_reward[:, :, None].repeat(1, 1, n_samples)\n        zs_policy_reward_right = zs_policy_reward[:, None, :].repeat(1, n_samples, 1)\n        # batch_size, n_samples, n_samples\n\n        evidence_thresh = torch.quantile(comparison_matrix.view(batch_size, -1).float(),\n                                         1 - self.config.lpo_evidence_ratio / 2, dim=1)\n        left_win = comparison_matrix >= evidence_thresh[:, None, None]\n        # batch_size, n_samples, n_samples\n        comparison_mask = 1 - torch.eye(n_samples).float()\n        comparison_mask = comparison_mask.unsqueeze(0).repeat(batch_size, 1, 1)\n        comparison_mask = comparison_mask.to(comparison_matrix.device)\n        # batch_size, n_samples, n_samples\n        comparison_mask = comparison_mask * left_win\n\n        disadvantage = (zs_policy_reward_right - zs_policy_reward_left) * self.config.beta  # to minimize\n        loss = -F.logsigmoid(-disadvantage)\n        acc = (loss < 0.6931).float()\n        loss = (loss * comparison_mask).sum() / comparison_mask.sum()\n        acc = (acc * comparison_mask).sum() / comparison_mask.sum()\n\n        return loss, acc\n\n\nclass DPOModel(GPT2PreTrainedModel):\n    config_class = GPT2Config\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.base = GPT2LMHeadModel(config)\n        self.beta = config.beta\n\n    def load_pretrained(self, pretrained_model=None):\n        if pretrained_model is None:\n            pretrained_model = GPT2LMHeadModel.from_pretrained(self.config.base_model_path)\n        self.base.load_state_dict(pretrained_model.state_dict())\n\n    def prepare_refer_model(self):\n        self.refer_model = copy.deepcopy(self)\n        self.refer_model.requires_grad_(False)\n        self.refer_model.eval()\n\n    def compute_nlls(self, input_ids, labels):\n        hidden_states = self.base.transformer(input_ids=input_ids)[0]\n        lm_logits = self.base.lm_head(hidden_states).to(torch.float32)\n        labels = labels.to(lm_logits.device)\n        shift_logits = lm_logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        nlls = F.cross_entropy(\n            input=shift_logits.transpose(1, 2),\n            target=shift_labels,\n            reduction=\'none\'\n        ).sum(dim=-1)\n        return nlls\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        labels: torch.LongTensor,\n        refer_nlls: torch.FloatTensor = None,\n        reward: torch.FloatTensor = None,  # r(x,y)\n    ):\n        theta_nlls = self.compute_nlls(input_ids=input_ids, labels=labels)\n        if refer_nlls is None:\n            assert hasattr(self, "refer_model")\n            with torch.no_grad():\n                refer_nlls = self.refer_model.compute_nlls(input_ids=input_ids, labels=labels)\n        log_iw = refer_nlls.detach() - theta_nlls\n        chosen_log_iw = log_iw[::2]\n        rejected_log_iw = log_iw[1::2]\n        disadvantage = (rejected_log_iw - chosen_log_iw) * self.beta\n        loss = -F.logsigmoid(-disadvantage)\n        acc = (loss < 0.6931).float()\n        return loss.mean(), acc.mean()\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=None,\n        streamer=None,\n        **kwargs,\n    ):\n        return self.base.generate(\n            inputs=inputs,\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            stopping_criteria=stopping_criteria,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            synced_gpus=synced_gpus,\n            streamer=streamer,\n            **kwargs,\n        )\n\n\nclass PTModel(GPT2PreTrainedModel):\n    config_class = GPT2CVAEConfig\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config: GPT2CVAEConfig):\n        super().__init__(config)\n        self.base = GPT2LMHeadModel(config)\n        self.base.requires_grad_(False)\n        self.hs = torch.nn.ParameterList([\n            torch.nn.Parameter(torch.Tensor(1, config.num_q, config.n_embd))\n            for i in range(config.n_layer)\n        ])\n        for param in self.hs:\n            param.data.normal_(mean=0.0, std=config.initializer_range)\n        self.hs_dropout = nn.Dropout(p=0.1)\n        self.lns = nn.ModuleList([nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n                                  for i in range(config.n_layer)])\n        self.attns = nn.ModuleList([GPT2Attention(config, layer_idx=i)\n                                    for i in range(config.n_layer)])\n        self.attns.requires_grad_(False)\n        self.beta = config.beta\n        self.mode = "sft"\n\n    def load_pretrained(self, pretrained_model=None):\n        if pretrained_model is None:\n            pretrained_model = GPT2LMHeadModel.from_pretrained(self.config.base_model_path)\n        self.base.load_state_dict(pretrained_model.state_dict())\n        for i in range(self.config.n_layer):\n            self.attns[i].load_state_dict(pretrained_model.transformer.h[i].attn.state_dict())\n\n    def switch_into_dpo_mode(self):\n        self.mode = "dpo"\n        self.refer_model = copy.deepcopy(self)\n        self.refer_model.requires_grad_(False)\n        self.refer_model.eval()\n\n    def get_input_ps(self, input_ids):\n        batch_size = input_ids.shape[0]\n        past_key_values = []\n        for i in range(self.config.n_layer):\n            hidden_states, ln, attn = self.hs[i], self.lns[i], self.attns[i]\n            hidden_states = hidden_states.repeat(batch_size, 1, 1)\n            hidden_states = ln(self.hs_dropout(hidden_states))\n            present = attn(\n                hidden_states=hidden_states,\n                use_cache=True,\n            )[1]\n            past_key_values.append(present)\n        input_ps = past_key_values\n        return input_ps\n\n    def compute_nlls(self, input_ids, labels):\n        input_ps = self.get_input_ps(input_ids)\n        hidden_states = self.base.transformer.forward(\n            input_ids=input_ids,\n            past_key_values=input_ps\n        )[0]\n        lm_logits = self.base.lm_head(hidden_states).to(torch.float32)\n        labels = labels.to(lm_logits.device)\n        shift_logits = lm_logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        nlls = F.cross_entropy(\n            input=shift_logits.transpose(1, 2),\n            target=shift_labels,\n            reduction=\'none\'\n        ).sum(dim=-1)\n        return nlls\n\n    def forward(self, input_ids, labels, refer_nlls=None):\n        if self.mode == "sft":\n            # supervised fine-tuning\n            input_ps = self.get_input_ps(input_ids)\n            return self.base(\n                input_ids=input_ids,\n                past_key_values=input_ps,\n                labels=labels,\n                return_dict=False\n            )\n        elif self.mode == "dpo":\n            # dpo\n            theta_nlls = self.compute_nlls(input_ids=input_ids, labels=labels)\n            if refer_nlls is None:\n                assert hasattr(self, "refer_model")\n                self.refer_model.eval()\n                with torch.no_grad():\n                    refer_nlls = self.refer_model.compute_nlls(input_ids=input_ids, labels=labels)\n            log_iw = refer_nlls.detach() - theta_nlls\n            chosen_log_iw = log_iw[::2]\n            rejected_log_iw = log_iw[1::2]\n            disadvantage = (rejected_log_iw - chosen_log_iw) * self.beta\n            loss = -F.logsigmoid(-disadvantage)\n            acc = (loss < 0.6931).float()\n            return loss.mean(), acc.mean()\n        else:\n            raise NotImplementedError(f"mode: {self.mode}")\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=None,\n        streamer=None,\n        **kwargs,\n    ):\n        input_ids = kwargs["input_ids"]\n        input_ps = self.get_input_ps(input_ids)\n\n        outputs = self.base.transformer(\n            input_ids=input_ids[:, :-1],\n            past_key_values=input_ps,\n            use_cache=True,\n            return_dict=True\n        )\n        kwargs["past_key_values"] = outputs.past_key_values\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids != self.config.pad_token_id).long()\n        ], dim=1)\n        kwargs["attention_mask"] = attention_mask\n\n        return self.base.generate(\n            inputs=inputs,\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            stopping_criteria=stopping_criteria,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            synced_gpus=synced_gpus,\n            streamer=streamer,\n            **kwargs,\n        )', './LPO-dailydialog\\test_generate_and_reward_and_ppl.py': 'import os\nimport json\nimport torch\nimport random\nimport argparse\nimport numpy as np\nimport pandas as pd\n\nfrom dailydialog_dataset import DailyDialogDataset\nfrom dailydialog_one2many_dataset import DailyDialogOne2ManyDataset\nfrom gpu_map_reduce import BatchTaskConsumer, TaskConsumer, TaskProducer\nfrom config_and_utils import (\n    BASE_MODEL_PATH, output_max_length, get_tokenizer,\n    get_reward_fn, intent_label2id, BERT_SCORE_MODEL_PATH,\n    pad_and_concat, load_checkpoint, truncate_generation_after_first_EOU\n)\nfrom modeling_gpt2_cvae import GPT2Config, GPT2ForVAE, GPT2LMHeadModel, PTModel\n\n\nclass BERTScoreProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        with open(self.args.input_file, "r", encoding="utf-8") as f:\n            tasks = json.loads(f.read())\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None \\\n               and all([(key in result and task[key]==result[key])\n                        for key in ["prompt", "output", "reference"]]) \\\n               and all([(key in result and task[key]==result[key])\n                        for key in [f"{preference}_{metric}"\n                                    for metric in ["score", "acc"]\n                                    for preference in (list(intent_label2id.keys())+["consistency"])]]) \\\n               and all([key in result\n                        for key in ["bertscore"]])\n\nclass BERTScoreConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        from datasets import load_metric\n        self.bertscore = load_metric("./bertscore")\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        predictions = [task["prompt"] + task["output"] for task in tasks]\n        references = [task["prompt"] + task["reference"] for task in tasks]\n        results = self.bertscore.compute(\n            predictions=predictions,\n            references=references,\n            model_type=BERT_SCORE_MODEL_PATH,\n            device=self.device,\n            batch_size=self.batch_size\n        )\n        for i in range(len(tasks)):\n            tasks[i]["bertscore"] = results["f1"][i]\n        return tasks\n\nclass RewardProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        with open(self.args.input_file, "r", encoding="utf-8") as f:\n            tasks = json.loads(f.read())\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None \\\n               and all([(key in result and task[key]==result[key])\n                        for key in ["prompt", "output", "reference"]]) \\\n               and all([f"{preference}_{metric}" in result\n                        for metric in ["score", "acc"]\n                        for preference in (list(intent_label2id.keys())+["consistency"])])\n\nclass RewardConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        self.reward_fn = get_reward_fn(device=self.device)\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        prompts = [task["prompt"] for task in tasks]\n        labels = [task["output"] for task in tasks]\n        intent_probs = self.reward_fn(prompts, labels, return_probs=True)\n        for i, task in enumerate(tasks):\n            output_intent = torch.argmax(intent_probs[i, :]).item()\n            local_intent_label2id = {**intent_label2id, "consistency": task["intent"]}\n            for intent, id in local_intent_label2id.items():\n                task[f"{intent}_score"] = intent_probs[i, id].item()\n                task[f"{intent}_acc"] = int(output_intent == id)\n            task["intent_probs"] = intent_probs[i, :].tolist()\n        return tasks\n\nclass GenerateProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        from dailydialog_dataset import DailyDialogDataset\n        test_dataset = DailyDialogDataset(usage="test")\n        tasks = [\n            {"input_ids": test_dataset[i]["prompt_ids"].tolist(),\n             "label_ids": test_dataset[i]["label_ids"].tolist(),\n             "intent": test_dataset[i]["intent"].item()}\n            for i in range(len(test_dataset))\n        ]\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None \\\n               and all([(key in result and task[key]==result[key])\n                        for key in ["input_ids", "label_ids", "intent"]]) \\\n               and all([key in result\n                        for key in ["prompt", "output"]])\n\nclass GenerateConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        tokenizer = get_tokenizer()\n        if self.args.test_ground_truth:\n            model = None\n        elif self.args.test_one2many_results:\n            model = None\n            dataset = DailyDialogOne2ManyDataset(usage="test")\n            self.input_ids_to_generation = {}\n            for data in dataset.data:\n                self.input_ids_to_generation[str(data["input_ids"])] = data["generation"]\n        elif self.args.vae_model_path is not None:\n            model = GPT2ForVAE.from_pretrained(self.args.vae_model_path)\n            model.config.eos_token_id = tokenizer.eos_token_id\n            model.config.pad_token_id = tokenizer.eos_token_id\n            if self.args.lpo_model_path is not None:\n                model.load_lpo_policy_latent_encoder(self.args.lpo_model_path)\n            model = model.half().to(self.device).eval()\n        elif self.args.dpo_model_path is not None:\n            model = GPT2LMHeadModel.from_pretrained(\n                BASE_MODEL_PATH,\n                torch_dtype=torch.float16,\n                device_map=self.device,\n            ).eval()\n            sd = load_checkpoint(self.args.dpo_model_path, self.device)\n            sd = {k.replace("base.", ""): v for k, v in sd.items() if k.startswith("base.")}\n            model.load_state_dict(sd)\n        elif self.args.lora_model_path is not None:\n            from peft import AutoPeftModelForCausalLM\n            model = AutoPeftModelForCausalLM.from_pretrained(self.args.lora_model_path)\n            model = model.eval().half().to(self.device)\n        elif self.args.pt_model_path is not None:\n            model = PTModel.from_pretrained(self.args.pt_model_path, device_map=self.device)\n            model = model.half().to(self.device).eval()\n        elif self.args.ppo_model_path is not None:\n            model = GPT2LMHeadModel(GPT2Config.from_pretrained(BASE_MODEL_PATH))\n            sd = load_checkpoint(self.args.ppo_model_path, device=self.device)\n            sd = {k[len("base_model."):]: v for k, v in sd.items() if k.startswith("base_model.")}\n            model.load_state_dict(sd)\n            model = model.half().to(self.device).eval()\n        else:\n            model = GPT2LMHeadModel.from_pretrained(\n                self.args.sft_model_path,\n                device_map=self.device,\n                torch_dtype=torch.float16,\n            ).eval()\n        self.tokenizer = tokenizer\n        self.model = model\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        inputs_len = max([len(task["input_ids"][0]) for task in tasks])\n        input_ids = pad_and_concat(\n            [torch.LongTensor(task["input_ids"]) for task in tasks],\n            pad_token_id=self.tokenizer.pad_token_id,\n            padding_side="left"\n        ).to(self.device)\n        assert input_ids.shape[1] == inputs_len\n        reference_ids = pad_and_concat(\n            [torch.LongTensor(task["label_ids"]) for task in tasks],\n            pad_token_id=self.tokenizer.pad_token_id,\n            padding_side="left"\n        ).to(self.device)\n        prompts = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n        references = self.tokenizer.batch_decode(reference_ids, skip_special_tokens=True)\n        if self.args.test_ground_truth:\n            for task, prompt, reference in zip(tasks,prompts,references):\n                task[\'prompt\'] = prompt\n                task["reference"] = reference\n                task[\'output\'] = reference\n                assert truncate_generation_after_first_EOU(reference) == reference\n        elif self.args.test_one2many_results:\n            for task, prompt, reference in zip(tasks,prompts,references):\n                task[\'prompt\'] = prompt\n                task["reference"] = reference\n                task[\'output\'] = self.input_ids_to_generation[str(task["input_ids"])][random.randint(0,31)]\n                assert truncate_generation_after_first_EOU(reference) == reference\n                assert truncate_generation_after_first_EOU(task[\'output\']) == task[\'output\']\n        else:\n            with torch.no_grad():\n                output_ids = self.model.generate(\n                    input_ids=input_ids,\n                    do_sample=True,\n                    top_k=20,\n                    min_length=2,\n                    max_new_tokens=output_max_length,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                )\n            outputs = self.tokenizer.batch_decode(output_ids[:,inputs_len:], skip_special_tokens=True)\n            prompts = self.tokenizer.batch_decode(output_ids[:,:inputs_len], skip_special_tokens=True)\n            for task,output,prompt,reference in zip(tasks,outputs,prompts,references):\n                task[\'prompt\'] = prompt\n                task["reference"] = reference\n                task[\'output\'] = truncate_generation_after_first_EOU(output)\n        return tasks\n\ndef summarize_results(input_file):\n    with open(input_file, "r", encoding="utf-8") as f:\n        results = json.loads(f.read())\n    pd_dict = {}\n    pd_index = ["score", "acc"]\n    for preference in (list(intent_label2id.keys())+["consistency"]):\n        preference_score = np.mean([result[f"{preference}_score"] for result in results])\n        preference_acc = np.mean([result[f"{preference}_acc"] for result in results])\n        pd_dict[preference] = [preference_score, preference_acc]\n    df = pd.DataFrame(pd_dict, index=pd_index)\n    print(df)\n    bertscore = np.mean([result["bertscore"] for result in results])\n    print(f"bertscore: {bertscore:.3f}")\n    return df, bertscore\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce generation and reward on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="generate")\n    parser.add_argument("--output_file", type=str, default=None)\n    parser.add_argument("--erase", type=int, default=0)\n\n    # checkpoint path of generation model to test\n    from config_and_utils import auto_find_checkpoint_dir\n    parser.add_argument(\'--test_ground_truth\', type=int, default=0)\n    parser.add_argument(\'--test_one2many_results\', type=int, default=0)\n    parser.add_argument(\'--sft_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--pt_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--dpo_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--vae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--cvae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--lpo_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--ppo_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--lora_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument("--batch_size", type=int, default=16)\n    parser.add_argument(\'--num_run_times\', type=int, default=1)\n\n    # the input file of generation to reward\n    parser.add_argument("--input_file", type=str, default=None)\n\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    if args.local_rank == -1:\n        if args.test_ground_truth:\n            args.output_file = "test_ground_truth_inference.json"\n        elif args.test_one2many_results:\n            args.output_file = "test_one2many_results_inference.json"\n        elif args.pt_model_path is not None:\n            args.output_file = os.path.join(args.pt_model_path, "inference.json")\n        elif args.dpo_model_path is not None:\n            args.output_file = os.path.join(args.dpo_model_path, "inference.json")\n        elif args.vae_model_path is not None:\n            if args.lpo_model_path is not None:\n                args.output_file = os.path.join(args.lpo_model_path, "inference.json")\n            else:\n                args.output_file = os.path.join(args.vae_model_path, "inference.json")\n        elif args.cvae_model_path is not None:\n            if args.lpo_model_path is not None:\n                args.output_file = os.path.join(args.lpo_model_path, "inference.json")\n            else:\n                args.output_file = os.path.join(args.cvae_model_path, "inference.json")\n        elif args.sft_model_path is not None:\n            args.output_file = os.path.join(args.sft_model_path, "inference.json")\n        elif args.ppo_model_path is not None:\n            args.output_file = os.path.join(args.ppo_model_path, "inference.json")\n        elif args.lora_model_path is not None:\n            args.output_file = os.path.join(args.lora_model_path, "inference.json")\n        else:\n            raise NotImplementedError(f"at least one kind of model path should be specified")\n\n    return args\n\ndef main():\n    args = parse_args()\n    args.erase = int(args.erase or args.num_run_times > 1)\n\n    if args.local_rank == -1:\n        #intent_accs, bertscores = [], []\n        dfs, bertscores = [], []\n        input_file, output_file = args.input_file, args.output_file\n        for i in range(args.num_run_times):\n            print(f"run - {i+1}/{args.num_run_times}")\n            # step1. generate output\n            args.task_name = "generate"\n            args.input_file, args.output_file = input_file, output_file\n            producer = GenerateProducer(args)\n            producer.run()\n            # step2. reward output\n            args.task_name = "reward"\n            args.input_file = args.output_file\n            args.output_file = args.output_file.replace("inference.json", "inference_and_reward.json")\n            producer = RewardProducer(args)\n            producer.run()\n            # step3. ppl\n            args.task_name = "bertscore"\n            args.input_file = args.output_file\n            args.output_file = args.output_file.replace("inference_and_reward.json", "inference_and_reward_and_bertscore.json")\n            producer = BERTScoreProducer(args)\n            producer.run()\n            # step4. analyze the results\n            df, bertscore = summarize_results(input_file=args.output_file)\n            dfs.append(df)\n            bertscores.append(bertscore)\n        df = pd.concat(dfs)\n        mean, std = df.groupby(level=0).mean(), df.groupby(level=0).std()\n        print(f"{mean.round(3).astype(str) + \'\' + std.round(3).astype(str)}")\n        print(f"bertscore: {np.mean(bertscores):.3f}{np.std(bertscores):.3f}")\n    else:\n        if args.task_name == "generate":\n            consumer = GenerateConsumer(args)\n            consumer.run()\n        elif args.task_name == "reward":\n            consumer = RewardConsumer(args)\n            consumer.run()\n        elif args.task_name == "bertscore":\n            consumer = BERTScoreConsumer(args)\n            consumer.run()\n        else:\n            raise NotImplementedError(args.task_name)\n\nif __name__ == "__main__":\n    main()\n', './LPO-dailydialog\\train_classifier.py': 'import os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\n\nfrom config_and_utils import get_trainer, BASE_RWD_MODEL_PATH\nfrom dailydialog_dataset import DailyDialogDataset\n\nfrom transformers import AutoTokenizer, RobertaForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_RWD_MODEL_PATH)\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    acc = (logits.argmax(axis=-1) == labels)\n\n    result = {\n        "acc": acc.mean(),\n    }\n\n    return result\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n    parser.add_argument(\'--epochs\', type=int, default=3)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-5)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=8)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    if args.output_dir is None:\n        args.output_dir = "classifier_fine_tuned"\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.mkdir(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    set_seed(args.seed)\n\n    train_dataset = DailyDialogDataset(usage="train")\n    valid_dataset = DailyDialogDataset(usage="validation")\n\n    model = RobertaForSequenceClassification.from_pretrained(\n        BASE_RWD_MODEL_PATH,\n        num_labels=4,\n        type_vocab_size=3,\n        ignore_mismatched_sizes=True\n    )\n\n    from transformers import TrainerCallback\n    class EvalAtStartCallback(TrainerCallback):\n        def on_step_begin(self, args, state, control, **kwargs):\n            if state.global_step == 1:\n                control.should_evaluate = True\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.rwd_collate_fn,\n        compute_metrics=compute_metrics,\n        remove_unused_columns=False,\n        callbacks=[EvalAtStartCallback()]\n    )\n    trainer.train()\n    trainer.save_state()\n', './LPO-dailydialog\\train_dpo.py': 'import json\nimport os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\nfrom transformers import GPT2LMHeadModel\n\nfrom config_and_utils import get_tokenizer, get_trainer, BASE_MODEL_PATH, auto_find_checkpoint_dir, load_checkpoint\nfrom modeling_gpt2_cvae import DPOModel, PTModel\nfrom dailydialog_comparison_dataset import (\n    DailyDialogComparisonDataset,\n)\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n\ndef compute_metrics(eval_preds):\n    acc = eval_preds.predictions\n    return {"acc": acc.mean()}\n\n\ntokenizer = get_tokenizer()\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--sft_model_path\', type=auto_find_checkpoint_dir,\n                        default=auto_find_checkpoint_dir(f"sft/checkpoint"))\n    parser.add_argument(\'--pt_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument("--preference", type=str, default="consistency",\n                        choices=["consistency", "Inform", "Questions", "Directives", "Commissive"])\n    parser.add_argument(\'--beta\', type=float, default=0.1)\n    parser.add_argument(\'--lora_dim\', type=int, default=0)\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-6)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=2)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    if args.output_dir is None:\n        if args.pt_model_path is not None:\n            args.output_dir = os.path.join(args.pt_model_path, f"dpo_{args.preference}_preference_beta_{args.beta}")\n        else:\n            if args.lora_dim:\n                args.output_dir = os.path.join(args.sft_model_path,\n                                               f"lora-{args.lora_dim}_dpo_{args.preference}_preference_beta_{args.beta}")\n            else:\n                args.output_dir = os.path.join(args.sft_model_path,\n                                               f"dpo_{args.preference}_preference_beta_{args.beta}")\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.mkdir(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    set_seed(args.seed)\n\n    train_dataset = DailyDialogComparisonDataset(usage="train", preference=args.preference)\n    valid_dataset = DailyDialogComparisonDataset(usage="validation", preference=args.preference)\n\n    device = f"cuda:{args.local_rank}"\n    if args.pt_model_path is not None:\n        model = PTModel.from_pretrained(args.pt_model_path, device_map=device)\n        model.switch_into_dpo_mode()\n        model.config.beta = args.beta\n        model.beta = args.beta\n        if args.learning_rate == 1e-6:\n            args.learning_rate = 1e-4\n    else:\n        pretrained_model = GPT2LMHeadModel.from_pretrained(args.sft_model_path, device_map=device)\n        config = pretrained_model.config\n        config.update({\n            "beta": args.beta, "lora_dim": args.lora_dim\n        })\n        model = DPOModel(config).to(device)\n        model.load_pretrained(pretrained_model=pretrained_model)\n        model.prepare_refer_model()\n\n        if args.lora_dim:\n            from peft import get_peft_model, LoraConfig, TaskType\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                inference_mode=False,\n                r=config.lora_dim, lora_alpha=config.lora_dim, lora_dropout=0.0\n            )\n            model.base = get_peft_model(model.base, peft_config)\n            if args.local_rank == 0:\n                model.base.print_trainable_parameters()\n\n            if args.learning_rate == 1e-6:\n                args.learning_rate = 1e-5\n\n    if args.local_rank == 0:\n        print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n        with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n            f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.dpo_collate_fn,\n        compute_metrics=compute_metrics\n    )\n    trainer.train()\n    trainer.save_state()\n\n    if args.lora_dim and args.local_rank == 0:\n        model.base.save_pretrained(os.path.join(args.output_dir, "best_model"))', './LPO-dailydialog\\train_lpo.py': 'import os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\n\nfrom config_and_utils import get_tokenizer, get_trainer, auto_find_checkpoint_dir\nfrom modeling_gpt2_cvae import LPOModel\nfrom dailydialog_comparison_dataset import (\n    DailyDialogComparisonDataset,\n)\n\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n\ndef compute_metrics(eval_preds):\n    acc = eval_preds.predictions\n    return {"acc": acc.mean()}\n\n\ntokenizer = get_tokenizer()\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--cvae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--vae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument("--preference", type=str, default="consistency",\n                        choices=["consistency", "Inform", "Questions", "Directives", "Commissive"])\n    parser.add_argument(\'--beta\', type=float, default=0.1)\n    parser.add_argument(\'--lpo_sampling_times\', type=int, default=16)\n    parser.add_argument(\'--lpo_post_sampling\', type=int, default=1)\n    parser.add_argument(\'--lpo_evidence_ratio\', type=float, default=0.5)\n    parser.add_argument(\'--expectation_lpo\', type=int, default=0)\n    parser.add_argument(\'--marginal_lpo\', type=int, default=0)\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-4)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=8)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    lpo_kwargs = {\n        key:getattr(args, key)\n        for key in [\n            "beta", "lpo_sampling_times", "lpo_post_sampling", "lpo_evidence_ratio",\n            "marginal_lpo", "expectation_lpo"\n        ]\n    }\n\n    if args.cvae_model_path is not None:\n        model = LPOModel.from_cvae(args.cvae_model_path, **lpo_kwargs)\n        model_path = args.cvae_model_path\n    elif args.vae_model_path is not None:\n        model = LPOModel.from_vae(args.vae_model_path, **lpo_kwargs)\n        model_path = args.vae_model_path\n    else:\n        raise ValueError(f"at least one of [cvae_model_path, vae_model_path] needs to be specified")\n\n    if args.output_dir is None:\n        args.output_dir = os.path.join(model_path, "exp_lpo" if args.expectation_lpo else "lpo")\n        if not args.expectation_lpo:\n            if args.lpo_post_sampling:\n                args.output_dir += f"_post_sampling_{args.lpo_sampling_times}"\n            else:\n                args.output_dir += f"_sampling_{args.lpo_sampling_times}"\n            args.output_dir += f"_evidence_{str(args.lpo_evidence_ratio).replace(\'.\',\'\')}"\n        if args.marginal_lpo:\n            args.output_dir += f"_mrg"\n        if args.beta != 0.1:\n            args.output_dir += f"_beta_{args.beta}"\n        if args.learning_rate != 1e-4:\n            args.output_dir += f"_lr_{args.learning_rate:.0e}"\n        if args.epochs != 1:\n            args.output_dir += f"_epochs_{args.epochs}"\n\n    set_seed(args.seed)\n\n    train_dataset = DailyDialogComparisonDataset(usage="train", preference=args.preference)\n    valid_dataset = DailyDialogComparisonDataset(usage="validation", preference=args.preference)\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.lpo_collate_fn,\n        compute_metrics=compute_metrics,\n        label_names=["prompt_ids"],\n        metric_for_best_model="eval_loss",\n    )\n    trainer.train()\n    trainer.save_state()\n', './LPO-dailydialog\\train_sft.py': 'import json\nimport os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\n\nfrom config_and_utils import get_tokenizer, get_trainer, BASE_MODEL_PATH, auto_find_checkpoint_dir\nfrom modeling_gpt2_cvae import GPT2LMHeadModel, PTModel, GPT2CVAEConfig\nfrom dailydialog_dataset import DailyDialogDataset\nfrom dailydialog_one2many_dataset import DailyDialogOne2ManyDataset\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\ntokenizer = get_tokenizer()\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--base_model_path\', type=auto_find_checkpoint_dir, default=BASE_MODEL_PATH)\n    parser.add_argument(\'--p_tuning\', action="store_true")\n    parser.add_argument(\'--p_tuning_on_generation\', action="store_true")\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=10)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-5)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=8)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    if args.output_dir is None:\n        if args.p_tuning:\n            args.output_dir = f"pt-sft"\n        elif args.p_tuning_on_generation:\n            args.output_dir = f"pt-sft-on-generation"\n        else:\n            args.output_dir = f"sft"\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.makedirs(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    if args.local_rank == 0:\n        print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n        with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n            f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n\n    set_seed(args.seed)\n\n    if args.p_tuning:\n        train_dataset = DailyDialogDataset(usage="train")\n        valid_dataset = DailyDialogDataset(usage="validation")\n        sft_model = GPT2LMHeadModel.from_pretrained(args.base_model_path,\n                                                    device_map=f"cuda:{args.local_rank}")\n        config = GPT2CVAEConfig(\n            **vars(args),\n            **vars(sft_model.config)\n        )\n        config.update({\n            "attn_pdrop": 0.0, "embd_pdrop": 0.0, "resid_pdrop": 0.0,\n        })\n        model = PTModel(config)\n        model.load_pretrained(sft_model)\n    elif args.p_tuning_on_generation:\n        train_dataset = DailyDialogOne2ManyDataset(usage="train")\n        valid_dataset = DailyDialogOne2ManyDataset(usage="validation")\n        sft_model = GPT2LMHeadModel.from_pretrained(args.base_model_path,\n                                                    device_map=f"cuda:{args.local_rank}")\n        config = GPT2CVAEConfig(\n            **vars(args),\n            **vars(sft_model.config)\n        )\n        config.update({\n            "attn_pdrop": 0.0, "embd_pdrop": 0.0, "resid_pdrop": 0.0,\n        })\n        model = PTModel(config)\n        model.load_pretrained(sft_model)\n    else:\n        train_dataset = DailyDialogDataset(usage="train")\n        valid_dataset = DailyDialogDataset(usage="validation")\n        model = GPT2LMHeadModel.from_pretrained(args.base_model_path)\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        remove_unused_columns=False,\n        collate_fn=train_dataset.sft_collate_fn\n    )\n    trainer.train()\n    trainer.save_state()\n', './LPO-dailydialog\\train_vae.py': 'import os\nimport json\nimport torch\n\nimport random\nimport argparse\nimport numpy as np\n\nfrom modeling_gpt2_cvae import GPT2ForVAE, GPT2CVAEConfig\n\nfrom dailydialog_one2many_dataset import (\n    DailyDialogOne2ManyDataset,\n)\nfrom transformers import (\n    AutoConfig,\n)\n\nfrom config_and_utils import get_tokenizer, get_trainer, BASE_MODEL_PATH, auto_find_checkpoint_dir\n\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n\ndef compute_metrics(eval_preds):\n    loss_lm, loss_kld, loss_contra = eval_preds.predictions\n\n    result = {\n        "loss_lm": loss_lm.mean(),\n        "loss_kld": loss_kld.mean(),\n        "loss_contra": loss_contra.mean(),\n    }\n\n    return result\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n\n    parser.add_argument(\'--vae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--base_model_path\', type=auto_find_checkpoint_dir, default=BASE_MODEL_PATH)\n\n    parser.add_argument(\'--num_q\', type=int, default=4)\n    parser.add_argument(\'--dim_z\', type=int, default=32)\n    parser.add_argument(\'--num_p\', type=int, default=4)\n    parser.add_argument(\'--latent_aggregator_layers\', type=int, default=2)\n    parser.add_argument(\'--post_with_x\', type=int, default=1)\n    parser.add_argument(\'--many\', type=int, default=32)\n\n    parser.add_argument(\'--frozen_pretrained\', type=int, default=0)\n    parser.add_argument(\'--marginal_kl\', type=int, default=1)\n    parser.add_argument(\'--without_dg_kld\', type=int, default=0)\n    parser.add_argument(\'--lm_sampling_times\', type=int, default=1)\n    parser.add_argument(\'--kl_sampling_times\', type=int, default=16)\n    parser.add_argument(\'--add_skip_connection\', type=int, default=0)\n    parser.add_argument(\'--add_contra_loss\', type=int, default=0)\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-4)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=8)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=1)\n\n    parser.add_argument(\'--output_dir\', type=auto_find_checkpoint_dir, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    set_seed(args.seed)\n\n    train_dataset = DailyDialogOne2ManyDataset(usage="train", total_many=args.many)\n    valid_dataset = DailyDialogOne2ManyDataset(usage="validation", total_many=args.many)\n\n    if args.vae_model_path is None:\n        # construct with base model\n        gpt2_config = AutoConfig.from_pretrained(args.base_model_path)\n        config = GPT2CVAEConfig(\n            **vars(args),\n            **vars(gpt2_config)\n        )\n        config.update({\n            "attn_pdrop": 0.0, "embd_pdrop": 0.0, "resid_pdrop": 0.0,\n        })\n        model = GPT2ForVAE(config)\n        model.load_pretrained()\n    else:\n        model = GPT2ForVAE.from_pretrained(args.vae_model_path)\n        model.config.update({\n            key: getattr(args, key) for key in [\n                "frozen_pretrained", "marginal_kl", "without_dg_kld",\n                "add_skip_connection", "add_contra_loss"\n            ]\n        })\n        model.update_requires_grad_()\n\n    if args.output_dir is None:\n        args.output_dir = f"gpt2_vae" if args.lm_sampling_times > 0 else f"gpt2_ae"\n        args.output_dir += f"_{args.latent_aggregator_layers}-layers_"\n        if (args.num_q, args.dim_z, args.num_p) != (4, 32, 4):\n            args.output_dir += f"_{args.num_q}q_{args.dim_z}z_{args.num_p}p"\n        if args.frozen_pretrained:\n            args.output_dir += f"_frozen_base"\n        if args.many != 32:\n            args.output_dir += f"_1-to-{args.many}"\n        if args.with_bn:\n            args.output_dir += f"_with_bn"\n        if args.post_with_x:\n            args.output_dir += f"_post_with_x"\n        if args.add_skip_residue_contra:\n            args.output_dir += f"_skip_residue_based_contra"\n        else:\n            if args.add_skip_connection:\n                args.output_dir += f"_skip"\n            if args.add_contra_loss:\n                args.output_dir += f"_contra"\n        if args.without_dg_kld:\n            args.output_dir += "_w.o.dg_kld"\n        if not args.learning_rate == 1e-4:\n            args.output_dir += f"_lr{args.learning_rate}"\n        if not args.epochs == 1:\n            args.output_dir += f"_{args.epochs}epochs"\n    tokenizer = get_tokenizer()\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.makedirs(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    if args.local_rank == 0:\n        print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n        with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n            f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.vae_collate_fn,\n        compute_metrics=compute_metrics,\n        label_names=[],\n        metric_for_best_model="loss",\n        greater_is_better=False,\n    )\n    trainer.train()\n    trainer.save_state()', './LPO-dailydialog\\scripts\\exp_dpo.sh': 'beta=0.1\nfor preference in Inform Questions Directives Commissive\ndo\n  deepspeed train_dpo.py --preference ${preference}\n  python test_generate_and_reward_and_ppl.py --dpo_model_path sft/checkpoint/dpo_${preference}_preference_beta_${beta}/checkpoint\ndone', './LPO-dailydialog\\scripts\\exp_lora_dpo.sh': 'beta=0.1\nlora_dim=8\nfor preference in Inform Questions Directives Commissive\ndo\n  deepspeed train_dpo.py --preference ${preference} --lora_dim ${lora_dim} --learning_rate 1e-5\n  python test_generate_and_reward_and_ppl.py --lora_model_path sft/checkpoint/lora-${lora_dim}_dpo_${preference}_preference_beta_${beta}/best_model\ndone', './LPO-dailydialog\\scripts\\exp_lpo.sh': '#!/bin/bash\n\nworking_dir=lpo_exp\n\ndeepspeed train_vae.py --add_skip_connection 1 --frozen_pretrained 1 --output_dir ${working_dir}/vae-frozen-base\npython test_generate_and_reward_and_ppl.py --vae_model_path ${working_dir}/vae-frozen-base/checkpoint\n\ndeepspeed train_vae.py --vae_model_path ${working_dir}/vae-frozen-base/checkpoint --add_contra_loss 1 --frozen_pretrained 0 --output_dir ${working_dir}/vae-unfrozen-base\npython test_generate_and_reward_and_ppl.py --vae_model_path ${working_dir}/vae-unfrozen-base/checkpoint\n\nfor preference in Inform Questions Directives Commissive\ndo\n  deepspeed train_lpo.py --vae_model_path ${working_dir}/vae-frozen-base/checkpoint --preference ${preference} --output_dir ${working_dir}/lpo-${preference}-frozen-base\n  python test_generate_and_reward_and_ppl.py --vae_model_path ${working_dir}/vae-frozen-base/checkpoint --lpo_model_path ${working_dir}/lpo-${preference}-frozen-base/checkpoint\ndone\n\nfor preference in Inform Questions Directives Commissive\ndo\n  deepspeed train_lpo.py --vae_model_path ${working_dir}/vae-unfrozen-base/checkpoint --preference ${preference} --output_dir ${working_dir}/lpo-${preference}-unfrozen-base\n  python test_generate_and_reward_and_ppl.py --vae_model_path ${working_dir}/vae-unfrozen-base/checkpoint --lpo_model_path ${working_dir}/lpo-${preference}-unfrozen-base/checkpoint\ndone', './LPO-dailydialog\\scripts\\exp_pt_dpo.sh': '# train the P-Tuning SFT model\ndeepspeed train_sft.py --p_tuning --base_model_path sft/checkpoint --learning_rate 1e-3\npython test_generate_and_reward_and_ppl.py --pt_model_path pt-sft/checkpoint\n# ppl:  30.86\n\n# train the P-Tuning DPO model\nbeta=0.1\n# beta  ppl\nfor preference in Inform Questions Directives Commissive\ndo\n  deepspeed train_dpo.py --beta ${beta} --preference ${preference} --pt_model_path pt-sft/checkpoint\n  python test_generate_and_reward_and_ppl.py --pt_model_path pt-sft/checkpoint/dpo_${preference}_preference_beta_${beta}/checkpoint\ndone', './LPO-dailydialog\\scripts\\exp_pt_dpo_on_generation.sh': '# train the P-Tuning SFT model\ndeepspeed train_sft.py --p_tuning_on_generation --base_model_path sft/checkpoint --learning_rate 1e-3\npython test_generate_and_reward_and_ppl.py --pt_model_path pt-sft-on-generation/checkpoint\n# ppl:  30.86\n\n# train the P-Tuning DPO model\nbeta=0.1\n# beta  ppl\nfor preference in Inform Questions Directives Commissive\ndo\n  deepspeed train_dpo.py --beta ${beta} --preference ${preference} --pt_model_path pt-sft-on-generation/checkpoint\n  python test_generate_and_reward_and_ppl.py --pt_model_path pt-sft-on-generation/checkpoint/dpo_${preference}_preference_beta_${beta}/checkpoint\ndone', './LPO-dailydialog\\scripts\\prepare_sft_rwd_and_data.sh': '# train the SFT model\ndeepspeed train_sft.py\n\n# train the reward model\ndeepspeed train_classifier.py\n\n# test the ground truth with the reward model\npython test_generate_and_reward_and_ppl.py --test_ground_truth 1\n\n# test the SFT model with the reward model\npython test_generate_and_reward_and_ppl.py --sft_model_path sft/checkpoint\n\n# one2many generation from the SFT model\npython dailydialog_one2many_dataset.py\n\n# annotate the intent probs in one2many generation with the reward model\npython dailydialog_comparison_dataset.py', './LPO-sentiment\\config_and_utils.py': 'import os\nimport re\n\noffline_mode = True\n# this mode requires datasets and base models are pre-downloaded and put into this project\n# our experiments are all carried in this mode since our machines are not connected to Internet\n#   offline_mode = False\n# this mode has not been tested yet\n\nif offline_mode:\n    BASE_MODEL_PATH = "./gpt2"\n    RWD_MODEL_PATH = "distilbert-imdb"\nelse:\n    BASE_MODEL_PATH = "openai-community/gpt2"\n    RWD_MODEL_PATH = "lvwerra/distilbert-imdb"\n\ninput_max_length = 64\noutput_max_length = 48\n\ndef get_tokenizer():\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n    return tokenizer\n\ndef get_sft_dataset(split="train"):\n    from datasets import load_dataset, load_from_disk\n    if offline_mode:\n        dataset = load_from_disk(f"./dataset-imdb/{split}")\n    else:\n        dataset = load_dataset("stanfordnlp/imdb")[split]\n    return dataset\n\ndef get_trainer(args, model, tokenizer, train_dataset, valid_dataset, collate_fn, **kwargs):\n    from transformers import TrainingArguments, Trainer\n    import inspect\n    gradient_accumulation_steps = args.global_batch_size // (args.n_devices * args.mini_batch_size)\n    total_training_steps = args.epochs * len(train_dataset) // args.global_batch_size\n    warmup_steps = int(total_training_steps * 0.1)\n    eval_steps = int(total_training_steps * 0.1)\n    save_steps = eval_steps\n\n    deepspeed = None if args.no_deepspeed else {\n        "train_micro_batch_size_per_gpu": args.mini_batch_size,\n        "gradient_accumulation_steps": gradient_accumulation_steps,\n        "fp16": {\n            "enabled": True,\n            "min_loss_scale": 1,\n            "opt_level": "O2"\n        },\n        "zero_optimization": {\n            "stage": 2,\n            "offload_optimizer": {\n                "device": "cpu",\n                "pin_memory": True\n            },\n        },\n        "optimizer": {\n            "type": "AdamW",\n            "params": {\n                "lr": args.learning_rate,\n            }\n        },\n        "scheduler": {\n            "type": "WarmupLR",\n            "params": {\n                "warmup_min_lr": "auto",\n                "warmup_max_lr": "auto",\n                "warmup_num_steps": "auto",\n            }\n        }\n    }\n\n    training_args = TrainingArguments(\n        output_dir=args.output_dir,\n        evaluation_strategy="steps",\n        eval_accumulation_steps=1,\n        learning_rate=args.learning_rate,\n        per_device_train_batch_size=args.mini_batch_size,\n        per_device_eval_batch_size=args.mini_batch_size,\n        fp16=True,\n        adam_beta1=0.9,\n        adam_beta2=0.95,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        num_train_epochs=args.epochs,\n        warmup_steps=warmup_steps,\n        eval_steps=eval_steps,\n        save_steps=save_steps,\n        save_total_limit=1,\n        logging_steps=min(10, max(1, eval_steps // 10)),\n        deepspeed=deepspeed,\n        load_best_model_at_end=True,\n        **{k:v for k,v in kwargs.items() if k in inspect.signature(TrainingArguments.__init__).parameters},\n    )\n\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        data_collator=collate_fn,\n        **{k:v for k,v in kwargs.items() if k in inspect.signature(Trainer.__init__).parameters}\n    )\n    return trainer\n\ndef auto_find_checkpoint_dir(path):\n    if path is None or path == "None":\n        return path\n    def find_checkpoint_dir(path):\n        probable_checkpoint_dirs = [dir for dir in os.listdir(path) if re.search(r"checkpoint-(\\d+)", dir)]\n        if len(probable_checkpoint_dirs) == 1:\n            return probable_checkpoint_dirs[0]\n        elif len(probable_checkpoint_dirs) > 1:\n            print(f"multiple checkpoint dir in {path}, choose the last one")\n            probable_checkpoint_dirs = sorted(probable_checkpoint_dirs, key=lambda dir:int(re.search(r"checkpoint-(\\d+)", dir).group(1)))\n            return probable_checkpoint_dirs[-1]\n        else:\n            raise Exception(f"no checkpoint under {path} is found")\n    path_parts = os.path.normpath(path).split("/")\n    for i, part in enumerate(path_parts):\n        if part == "checkpoint":\n            parent_path = "/".join(path_parts[:i])\n            path_parts[i] = find_checkpoint_dir(parent_path)\n    return "/".join(path_parts)\n\ndef load_checkpoint(path, device):\n    import torch\n    device = torch.device(device) if type(device) is str else device\n    if os.path.exists(os.path.join(path, "pytorch_model.bin")):\n        state_dict = torch.load(os.path.join(path, "pytorch_model.bin"), map_location=device)\n    elif os.path.exists(os.path.join(path, "model.safetensors")):\n        from safetensors.torch import load_file\n        state_dict = load_file(os.path.join(path, "model.safetensors"), device="cpu")\n        state_dict = {k:v.to(device) for k,v in state_dict.items()}\n    else:\n        raise ValueError(f"no checkpoint file found in {path}")\n    return state_dict\n\ndef pad_and_concat(list_tensors, pad_token_id, padding_side="right", dim=-1):\n    import torch\n    max_length = max([tensor.shape[dim] for tensor in list_tensors])\n    list_padded_tensors = []\n    for tensor in list_tensors:\n        tensor_shape = list(tensor.shape)\n        padding_shape = tensor_shape\n        padding_shape[dim] = max_length - tensor_shape[dim]\n        padding = torch.empty(size=padding_shape, dtype=tensor.dtype).fill_(pad_token_id)\n        if padding_side == "right":\n            list_padded_tensors.append(torch.cat([tensor, padding], dim=dim))\n        elif padding_side == "left":\n            list_padded_tensors.append(torch.cat([padding, tensor], dim=dim))\n        else:\n            raise NotImplementedError(padding_side)\n    concated_tensors = torch.cat(list_padded_tensors, dim=0)\n    return concated_tensors', './LPO-sentiment\\gpu_map_reduce.py': '"""\nThis Python file implements a Map-Reduce framework with a TaskProducer abstract class and a\nTaskConsumer abstract class. Each TaskConsumer instance independently utilizes a GPU to load\na Torch model for data processing, i.e., in data-parallel.\n\nClasses:\n- TaskProducer:\n    load tasks from file\n    load and check cached results from file\n    clear the present tasks and results in redis queue (may be left by killed programmes)\n    launch consumers and send tasks to them\n    recieve results from consumers and save them to file\n    stop consumers at the end\n    - abstractmethod:\n        load_tasks\n        cached_result_is_valid\n- TaskConsumer:\n    init model on corresponding gpu (specified by local_rank)\n    recieve tasks from producer\n    process tasks with model\n    send results to producer\n    - abstractmethod:\n        init_model\n        process_task\nFunctions:\n- parse_args (demo):\n    parse args for TaskProducer and TaskConsumer\n- main (demo):\n    the programme entrance\n\nNote:\nThis script requires redis server running in the background for Inter-Process Communication.\nYou can install redis server and run it through the following commands:\n# apt-get install redis-server\n# redis-server --port PORT\n"""\nimport os\nimport re\nimport sys\nimport json\nimport time\nimport argparse\nimport threading\nimport concurrent\n\nimport redis\nimport torch\nfrom tqdm import tqdm\nfrom abc import ABC, abstractmethod\n\nclass TaskProducer(ABC):\n    def __init__(self, args):\n        self.args = args\n        self.producer = redis.Redis(host=args.host, port=args.port, db=0)\n        self.consumer = redis.Redis(host=args.host, port=args.port, db=1)\n        self.task_queue_name = f\'{self.args.task_name}_task\'\n        self.result_queue_name = f\'{self.args.task_name}_result\'\n        self.has_slept_since_last_save = False\n\n    def clear_queue(self):\n        num_tasks_cleared = 0\n        task = self.producer.lpop(self.task_queue_name)\n        while task is not None:\n            num_tasks_cleared += 1\n            task = self.producer.lpop(self.task_queue_name)\n\n        num_results_cleared = 0\n        result = self.consumer.lpop(self.result_queue_name)\n        while result is not None:\n            num_results_cleared += 1\n            result = self.consumer.lpop(self.result_queue_name)\n\n        print(f"clear {num_tasks_cleared} tasks and {num_results_cleared} results in queue")\n        return num_tasks_cleared, num_results_cleared\n\n    def auto_clear_queue(self):\n        stop_clearing = False\n        while not stop_clearing:\n            num_tasks_cleared, num_results_cleared = self.clear_queue()\n            def input_choice():\n                time.sleep(10)\n                return num_tasks_cleared == 0 and num_results_cleared == 0\n            executor = concurrent.futures.ThreadPoolExecutor()\n            future = executor.submit(input_choice)\n            try:\n                stop_clearing = future.result(timeout=30)\n            except concurrent.futures.TimeoutError:\n                stop_clearing = num_tasks_cleared == 0 and num_results_cleared == 0\n            finally:\n                executor.shutdown(wait=False)\n\n    def wait_for_next_result(self):\n        while True:\n            result = self.consumer.lpop(self.result_queue_name)\n            if result is not None:\n                result = json.loads(result.decode(\'utf-8\'))\n                return result\n            time.sleep(1)\n            self.has_slept_since_last_save = True\n\n    @abstractmethod\n    def load_tasks(self) -> list[dict]:\n        tasks = []\n        return tasks\n\n    def save_results(self):\n        num_valid_results = len([_ for _ in self.results if _ is not None])\n        print(f"save results ({num_valid_results}/{len(self.tasks)}) into {self.args.output_file}")\n        with open(self.args.output_file, "w", encoding="utf-8") as f:\n            f.write(json.dumps(self.results, ensure_ascii=False, indent=4))\n\n    def load_cached_results(self):\n        if os.path.exists(self.args.output_file) and not self.args.erase:\n            with open(self.args.output_file, "r", encoding="utf-8") as f:\n                results = json.loads(f.read())\n            if len(results) == len(self.tasks):\n                return results\n        results = [None for index in range(len(self.tasks))]\n        return results\n\n    @abstractmethod\n    def cached_result_is_valid(self, task, result) -> bool:\n        return True\n\n    def push_tasks(self):\n        num_cached_results, num_pushed_tasks = 0, 0\n        for index, task in enumerate(tqdm(self.tasks, desc=f"pushing tasks into {self.task_queue_name}...")):\n            if "index" not in task:\n                task["index"] = index\n            if self.results[index] is not None:\n                if self.cached_result_is_valid(task, self.results[index]):\n                    num_cached_results += 1\n                    continue\n                else:\n                    self.results[index] = None\n            self.producer.rpush(self.task_queue_name, json.dumps(task, ensure_ascii=False))\n            num_pushed_tasks += 1\n        print(f"read {num_cached_results} results from cache_file and push {num_pushed_tasks} tasks into {self.task_queue_name}")\n        return num_cached_results, num_pushed_tasks\n\n    def recieve_results(self):\n        first_result = self.wait_for_next_result()\n        for num_received_results in tqdm(range(self.num_pushed_tasks), desc=f"receiving results from {self.result_queue_name}..."):\n            if num_received_results == 0:\n                result = first_result\n            else:\n                result = self.wait_for_next_result()\n            index = result["index"]\n            while not self.cached_result_is_valid(self.tasks[index], result):\n                print(f"invalid result is received and ignored:\\n{json.dumps(result, ensure_ascii=False, indent=4)}")\n                result = self.wait_for_next_result()\n                index = result["index"]\n            self.results[index] = result\n            num_valid_results = self.num_cached_results + num_received_results + 1\n            if num_valid_results % max(len(self.tasks)//100, 100) == 0 and self.has_slept_since_last_save:\n                self.save_results()\n                self.has_slept_since_last_save = False\n\n    def run(self):\n        self.tasks = self.load_tasks()\n        self.results = self.load_cached_results()\n        self.auto_clear_queue()\n        self.num_cached_results, self.num_pushed_tasks = self.push_tasks()\n\n        if self.num_pushed_tasks > 0:\n            self.launch_consumers()\n            self.recieve_results()\n            self.save_results()\n            self.stop_consumers()\n\n    def launch_consumers(self):\n        def launch_consumers(args):\n            args_input = vars(args)\n            args_input = {k:v for k,v in args_input.items() if v is not None}\n            for local_rank in range(torch.cuda.device_count()):\n                args_input["local_rank"] = local_rank\n                argv = \' \'.join([f\'--{k} "{v}"\' for k, v in args_input.items()])\n                cmd = f\'nohup python {args.py_file_name} {argv}\' \\\n                      f\' > nohup.{args.py_file_name}.rank_{local_rank}.txt 2>&1 &\'\n                print(f"{cmd}")\n                os.system(cmd)\n        launch_consumers(self.args)\n\n    def stop_consumers(self):\n        for local_rank in range(torch.cuda.device_count()):\n            task = {"signal": "exit"}\n            self.producer.rpush(self.task_queue_name, json.dumps(task, ensure_ascii=False))\n\nclass TaskConsumer(ABC):\n    def __init__(self, args):\n        self.args = args\n        self.consumer = redis.Redis(host=args.host, port=args.port, db=0)\n        self.producer = redis.Redis(host=args.host, port=args.port, db=1)\n        self.task_queue_name = f\'{self.args.task_name}_task\'\n        self.result_queue_name = f\'{self.args.task_name}_result\'\n        self.device = torch.device(f"cuda:{args.local_rank}")\n        self.local_task_queue, self.task_queue_lock = [], threading.Lock()\n        self.local_result_queue, self.result_queue_lock = [], threading.Lock()\n        self.exit_signal = threading.Event()\n\n    @abstractmethod\n    def init_model(self) -> None:\n        pass\n\n    def listening(self):\n        num_tasks_per_sleep = 0\n        num_tasks_before_sleep = 0\n        while not self.exit_signal.is_set():\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            num_tasks_this_sleep = num_tasks_before_sleep - num_local_tasks\n            num_tasks_per_sleep = max(int(num_tasks_per_sleep * 0.8), num_tasks_this_sleep)\n            num_tasks_expected_in_queue = max(10, num_tasks_per_sleep * 2)\n            if num_local_tasks >= num_tasks_expected_in_queue:\n                num_tasks_before_sleep = num_local_tasks\n                time.sleep(1)\n                continue\n            for _ in range(num_tasks_expected_in_queue-num_local_tasks):\n                task = self.consumer.lpop(self.task_queue_name)\n                if task is None:\n                    break\n                task = json.loads(task.decode(\'utf-8\'))\n                if "signal" in task and task["signal"] == "exit":\n                    self.exit_signal.set()\n                    break\n                with self.task_queue_lock:\n                    self.local_task_queue.append(task)\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            num_tasks_before_sleep = num_local_tasks\n            time.sleep(1)\n\n    def sending(self):\n        while not self.exit_signal.is_set():\n            with self.result_queue_lock:\n                num_local_results = len(self.local_result_queue)\n            if num_local_results == 0:\n                time.sleep(1)\n                continue\n            with self.result_queue_lock:\n                result = self.local_result_queue.pop(0)\n            self.producer.rpush(self.result_queue_name, result)\n\n    def start_communication_threads(self):\n        self.threads = [\n            threading.Thread(target=self.listening),\n            threading.Thread(target=self.sending)\n        ]\n        for thread in self.threads:\n            thread.start()\n\n    def join_communication_threads(self):\n        for thread in self.threads:\n            thread.join()\n\n    @abstractmethod\n    def process_task(self, task: dict) -> dict:\n        result = task\n        return result\n\n    def processing_task_loop(self):\n        while not self.exit_signal.is_set():\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            if num_local_tasks == 0:\n                time.sleep(1)\n                continue\n            with self.task_queue_lock:\n                task = self.local_task_queue.pop(0)\n            result = self.process_task(task)\n            with self.result_queue_lock:\n                self.local_result_queue.append(json.dumps(result, ensure_ascii=False))\n\n    def run(self):\n        self.init_model()\n        self.start_communication_threads()\n        self.processing_task_loop()\n        self.join_communication_threads()\n\nclass BatchTaskConsumer(TaskConsumer):\n    def __init__(self, args):\n        super().__init__(args)\n        self.batch_size = getattr(args, "batch_size", 2)\n\n    def process_task(self, task: dict) -> dict:\n        pass\n\n    @abstractmethod\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        results = tasks\n        return results\n\n    def processing_task_loop(self):\n        while not self.exit_signal.is_set():\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            if num_local_tasks == 0:\n                time.sleep(1)\n                continue\n            with self.task_queue_lock:\n                tasks = self.local_task_queue[:self.batch_size]\n                self.local_task_queue = self.local_task_queue[self.batch_size:]\n            results = self.process_tasks(tasks)\n            with self.result_queue_lock:\n                for result in results:\n                    self.local_result_queue.append(json.dumps(result, ensure_ascii=False))\n\n\n# implementation examples\nclass MyProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        tasks = []\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return True\n\n\nclass MyConsumer(TaskConsumer):\n    def init_model(self) -> None:\n        pass\n\n    def process_task(self, task: dict) -> dict:\n        result = task\n        return result\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce processing on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="inference")\n    parser.add_argument("--output_file", type=str, default=None)\n    parser.add_argument("--erase", type=int, default=0)\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    return args\n\n\ndef main():\n    args = parse_args()\n    if args.local_rank == -1:\n        producer = TaskProducer(args)\n        producer.run()\n    else:\n        consumer = TaskConsumer(args)\n        consumer.run()\n\n\nif __name__ == "__main__":\n    main()\n', './LPO-sentiment\\imdb_comparison_dataset.py': 'import os\nimport json\nimport argparse\n\nimport torch\n\nfrom config_and_utils import RWD_MODEL_PATH, get_tokenizer, output_max_length, pad_and_concat\nfrom gpu_map_reduce import TaskConsumer, TaskProducer\nfrom collections import defaultdict\n\ndef pad_or_truncate(ids, target_length, pad_id):\n    if ids.shape[1] > target_length:\n        ids = ids[:,:target_length]\n    elif ids.shape[1] < target_length:\n        padding_ids = torch.LongTensor(size=(ids.shape[0], target_length-ids.shape[1]))\n        padding_ids.fill_(pad_id)\n        padding_ids = padding_ids.to(ids.device)\n        ids = torch.cat([ids, padding_ids], dim=1)\n    return ids\n\n\npreference_to_reward_fn = {\n    "positive": lambda logits:logits,\n    "negative": lambda logits:-logits,\n    "neutral": lambda logits:-torch.abs(logits)\n}\navailable_preferences = list(preference_to_reward_fn.keys())\n\n\nclass IMDBComparisonDataset:\n    def __init__(self, usage="train", preference="positive"):\n        cache_file = f"one2many_inference_with_reward.json"\n        with open(cache_file, "r", encoding="utf-8") as f:\n            inference = json.loads(f.read())\n        assert not any([data is None for data in inference]), f"the data in cache file contains \'None\': {cache_file}"\n        self.data = [data for data in inference if data["usage"]==usage]\n        self.usage = usage\n        self.preference = preference\n        self.tokenizer = get_tokenizer()\n        self.many = 32\n        self.num_pairs_per_group = 2\n\n    def __len__(self):\n        return len(self.data) * self.num_pairs_per_group\n\n    def __getitem__(self, item):\n        item, bias = item // self.num_pairs_per_group, item % self.num_pairs_per_group\n\n        data = self.data[item]\n        prompt_ids = data["input_ids"]\n        prompt_ids = torch.LongTensor(prompt_ids)\n        logits = torch.Tensor(data["reward_logits"])\n        reward = preference_to_reward_fn[self.preference](logits)\n\n        sorted_reward, indices = torch.sort(reward, descending=True)\n        indices = indices.tolist()\n        chosen = data["generation"][indices[bias]]\n        rejected = data["generation"][indices[bias-self.num_pairs_per_group]]\n        chosen_ids = self.tokenizer(chosen, return_tensors="pt").input_ids\n        rejected_ids = self.tokenizer(rejected, return_tensors="pt").input_ids\n        return prompt_ids, chosen_ids, rejected_ids\n\n    def lpo_collate_fn(self, batch_items):\n        batch_prompt_ids, batch_chosen_ids, batch_rejected_ids = [], [], []\n        for prompt_ids, chosen_ids, rejected_ids in batch_items:\n            batch_prompt_ids.append(prompt_ids)\n            batch_chosen_ids.append(chosen_ids)\n            batch_rejected_ids.append(rejected_ids)\n\n        pad_token_id = self.tokenizer.pad_token_id\n        return {\n            "prompt_ids": pad_and_concat(batch_prompt_ids, pad_token_id),\n            "chosen_ids": pad_and_concat(batch_chosen_ids, pad_token_id),\n            "rejected_ids": pad_and_concat(batch_rejected_ids, pad_token_id),\n        }\n\n    def dpo_collate_fn(self, batch_items):\n        input_ids, labels = [], []\n        for prompt_ids, chosen_ids, rejected_ids in batch_items:\n            chosen_input_ids = torch.cat([prompt_ids, chosen_ids], dim=1)\n            rejected_input_ids = torch.cat([prompt_ids, rejected_ids], dim=1)\n            chosen_labels = torch.cat([prompt_ids.clone().fill_(-100), chosen_ids], dim=1)\n            rejected_labels = torch.cat([prompt_ids.clone().fill_(-100), rejected_ids], dim=1)\n            input_ids.append(chosen_input_ids)\n            input_ids.append(rejected_input_ids)\n            labels.append(chosen_labels)\n            labels.append(rejected_labels)\n        inputs = {\n            "input_ids": pad_and_concat(input_ids, self.tokenizer.pad_token_id).long(),\n            "labels": pad_and_concat(labels, -100).long()\n        }\n        return inputs\n\n\n    def logits_analysis(self):\n        num_positive_counts = {num:0 for num in range(33)}\n        for item in range(len(self.data)):\n            data = self.data[item]\n            logits = data["reward_logits"]\n            num_positive = len([_ for _ in logits if _ > 0])\n            num_positive_counts[num_positive] += 1\n        print(f"{self.usage}: {num_positive_counts}")\n        import matplotlib.pyplot as plt\n        x = list(num_positive_counts.keys())\n        y = list(num_positive_counts.values())\n        y = [counts / len(self.data) for counts in y]\n        plt.bar(x, y)\n        plt.savefig(f"{self.usage}_comparison_logits_analysis.png")\n        plt.clf()\n\n\nclass RewardProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        input_file = f"one2many_inference.json"\n        with open(input_file, "r", encoding="utf-8") as f:\n            tasks = json.loads(f.read())\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None and \\\n               all([task[key]==result[key]\n                    for key in ["input_ids", "usage", "index", "prompt", "generation"]])\n\n\nclass RewardConsumer(TaskConsumer):\n    def init_model(self) -> None:\n        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            RWD_MODEL_PATH,\n            device_map=f"cuda:{self.args.local_rank}",\n        ).eval()\n        self.tokenizer = AutoTokenizer.from_pretrained(RWD_MODEL_PATH)\n\n    def process_task(self, task: dict) -> dict:\n        input_sentences = [task["prompt"] + generation for generation in task["generation"]]\n        with torch.no_grad():\n            inputs = self.tokenizer(input_sentences, return_tensors="pt", padding="longest", truncation=True,\n                                    max_length=self.model.config.max_position_embeddings)\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            outputs = self.model(**inputs)\n            logits = outputs.logits[:, 1] - outputs.logits[:, 0]\n            logits = logits.tolist()\n        task["reward_logits"] = logits\n        return task\n\n\nclass ReferProducer(TaskProducer):\n    pass\n\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce processing on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="one2many_inference_reward")\n    parser.add_argument("--output_file", type=str, default=f"one2many_inference_with_reward.json")\n    parser.add_argument("--erase", type=int, default=0)\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    if args.local_rank == -1:\n        producer = RewardProducer(args)\n        producer.run()\n        for usage in ["train", "validation", "test"]:\n            dataset = IMDBComparisonDataset(usage)\n            dataset.logits_analysis()\n    else:\n        consumer = RewardConsumer(args)\n        consumer.run()', './LPO-sentiment\\imdb_dataset.py': 'import torch\nimport numpy as np\n\nfrom config_and_utils import (\n    get_tokenizer, get_sft_dataset,\n    input_max_length, output_max_length, pad_and_concat\n)\n\n\ndef pad_or_truncate(ids, target_length, pad_id):\n    if ids.shape[1] > target_length:\n        ids = ids[:,:target_length]\n    elif ids.shape[1] < target_length:\n        padding_ids = torch.LongTensor(size=(ids.shape[0], target_length-ids.shape[1]))\n        padding_ids.fill_(pad_id)\n        padding_ids = padding_ids.to(ids.device)\n        ids = torch.cat([ids, padding_ids], dim=1)\n    return ids\n\n\nclass IMDBDataset:\n    def __init__(self, usage="train"):\n        if usage=="train":\n            self.dataset = get_sft_dataset(split="train")\n        else:\n            dataset = get_sft_dataset(split="test")\n            dataset_negative = [item for item in dataset if item["label"] == 0]\n            dataset_positive = [item for item in dataset if item["label"] == 1]\n            if usage=="validation":\n                self.dataset = dataset_negative[:2500] + dataset_positive[:2500]\n            elif usage=="test":\n                self.dataset = dataset_negative[-2500:] + dataset_positive[-2500:]\n            else:\n                raise NotImplementedError(usage)\n        self.usage = usage\n        self.tokenizer = get_tokenizer()\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, item):\n        text, label = self.dataset[item]["text"], self.dataset[item]["label"]\n        input_ids = self.tokenizer([text], return_tensors="pt", truncation=True,\n                                   max_length=input_max_length+output_max_length).input_ids\n        label = torch.Tensor([label]).long()\n        return {"input_ids": input_ids, "label": label}\n\n    def sft_collate_fn(self, batch_items):\n        input_ids = pad_and_concat(\n            [item["input_ids"] for item in batch_items],\n            pad_token_id=self.tokenizer.pad_token_id,\n            padding_side="left", dim=1\n        )\n        labels = torch.where(input_ids==self.tokenizer.pad_token_id, -100, input_ids)\n        return {\n            "input_ids": input_ids,\n            "labels": labels,\n        }\n\n    def vae_collate_fn(self, batch_items):\n        input_ids = pad_and_concat(\n            [item["input_ids"] for item in batch_items],\n            pad_token_id=self.tokenizer.pad_token_id,\n            padding_side="left", dim=1\n        )\n        labels = torch.where(input_ids==self.tokenizer.pad_token_id, -100, input_ids)\n        labels[:,:input_max_length] = -100\n        post_ids = input_ids[:,input_max_length:]\n        return {\n            "post_ids": post_ids,\n            "input_ids": input_ids,\n            "labels": labels,\n        }\n\n    def label_analysis(self):\n        labels = [self.dataset[item]["label"] for item in range(len(self.dataset))]\n        print(f"{self.usage}:")\n        print(f"--positive: {len([_ for _ in labels if _ == 1])}")\n        print(f"--negative: {len([_ for _ in labels if _ == 0])}")\n        print(f"--average: {np.mean([labels])*100:.2f}%")\n\n\nif __name__ == "__main__":\n    for usage in ["train", "validation", "test"]:\n        dataset = IMDBDataset(usage)\n        dataset.label_analysis()\n', './LPO-sentiment\\imdb_one2many_dataset.py': 'import os\nimport json\nimport argparse\n\nimport torch\nimport numpy as np\n\nfrom config_and_utils import get_tokenizer, input_max_length, output_max_length, \\\n    auto_find_checkpoint_dir, pad_and_concat\nfrom imdb_dataset import IMDBDataset\n\nfrom gpu_map_reduce import BatchTaskConsumer, TaskConsumer, TaskProducer\n\n\ndef pad_or_truncate(ids, target_length, pad_id):\n    if ids.shape[1] > target_length:\n        ids = ids[:,:target_length]\n    elif ids.shape[1] < target_length:\n        padding_ids = torch.LongTensor(size=(ids.shape[0], target_length-ids.shape[1]))\n        padding_ids.fill_(pad_id)\n        padding_ids = padding_ids.to(ids.device)\n        ids = torch.cat([ids, padding_ids], dim=1)\n    return ids\n\n\nclass IMDBDatasetForInference(IMDBDataset):\n    def __getitem__(self, item):\n        text = self.dataset[item]["text"]\n        input_ids = self.tokenizer([text], return_tensors="pt", truncation=True,\n                                   max_length=input_max_length).input_ids\n        return {"input_ids": input_ids, "reference": text}\n\n\nclass IMDBOne2ManyDataset:\n    def __init__(self, usage="train", many=32):\n        cache_file = f"one2many_inference.json"\n        with open(cache_file, "r", encoding="utf-8") as f:\n            inference = json.loads(f.read())\n        assert not any([data is None for data in inference]), f"the data in cache file contains \'None\': {cache_file}"\n        self.data = [data for data in inference if data["usage"]==usage]\n        self.usage = usage\n        self.tokenizer = get_tokenizer()\n        self.many = many\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        data = self.data[item]\n        assert len(data["generation"]) >= self.many\n        data["generation"] = data["generation"][:self.many]\n\n        prompt_ids = torch.LongTensor(data["input_ids"])\n        prompt_len = prompt_ids.shape[1]\n        pred_ids: torch.Tensor = self.tokenizer(data["generation"], return_tensors="pt", padding="max_length",\n                                                truncation=True, max_length=output_max_length).input_ids\n        input_ids = prompt_ids\n        # [1, prompt_len]\n        labels = torch.where(pred_ids == self.tokenizer.pad_token_id, -100, pred_ids)\n        # [many, output_max_length]\n        prior_ids = pad_or_truncate(prompt_ids, input_max_length, self.tokenizer.pad_token_id)\n        # [1, input_max_length]\n        post_ids = pad_or_truncate(pred_ids, output_max_length, self.tokenizer.pad_token_id)\n        # [many, output_max_length]\n\n        assert input_ids.shape == (1, prompt_len), input_ids.shape\n        assert labels.shape == (self.many, output_max_length), labels.shape\n        assert prior_ids.shape == (1, input_max_length), prior_ids.shape\n        assert post_ids.shape == (self.many, output_max_length), post_ids.shape\n        assert not torch.any(input_ids==self.tokenizer.pad_token_id), input_ids\n\n        return {\n            "input_ids": input_ids,\n            "labels": labels,\n            "prior_ids": prior_ids,\n            "post_ids": post_ids\n        }\n\n    def collate_fn(self, batch_items):\n        assert len(batch_items) == 1, f"batch_size {len(batch_items)} > 1 " \\\n                                      f"is not supported in one2many training"\n        return batch_items[0]\n\n    def vae_collate_fn(self, batch_items):\n        assert len(batch_items) == 1, f"batch_size {len(batch_items)} > 1 " \\\n                                      f"is not supported in one2many training"\n        post_ids = batch_items[0]["post_ids"]\n        prompt_ids = batch_items[0]["input_ids"]\n        pred_labels = batch_items[0]["labels"]\n        prompt_ids = prompt_ids.repeat(pred_labels.shape[0], 1)\n        input_ids = torch.cat([prompt_ids, torch.where(pred_labels==-100, self.tokenizer.pad_token_id, pred_labels)], dim=1)\n        labels = torch.cat([torch.zeros_like(prompt_ids).fill_(-100), pred_labels], dim=1)\n        return {\n            "post_ids": post_ids,\n            "input_ids": input_ids,\n            "labels": labels,\n        }\n\n    def sft_collate_fn(self, batch_items):\n        assert len(batch_items) == 1, f"batch_size {len(batch_items)} > 1 " \\\n                                      f"is not supported in one2many training"\n        prompt_ids = batch_items[0]["input_ids"]\n        pred_labels = batch_items[0]["labels"]\n        prompt_ids = prompt_ids.repeat(pred_labels.shape[0], 1)\n        input_ids = torch.cat([prompt_ids, torch.where(pred_labels==-100, self.tokenizer.pad_token_id, pred_labels)], dim=1)\n        labels = torch.cat([torch.zeros_like(prompt_ids).fill_(-100), pred_labels], dim=1)\n        return {\n            "input_ids": input_ids,\n            "labels": labels,\n        }\n\n\ndef extend_one2many_dataset_with_mini_many(one2many_dataset_class):\n    class one2many_dataset_class_with_across_batch(one2many_dataset_class):\n        def __init__(self, total_many, mini_many, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.total_many = total_many\n            self.mini_many = mini_many\n            self.index_queue = []\n            self.present_index = None\n            self.present_order = None\n            self.present_bias = 0\n\n        def __getitem__(self, index):\n            self.index_queue.append(index)\n            if self.present_bias == 0:\n                self.present_index = self.index_queue.pop(0)\n                self.present_order = torch.from_numpy(np.random.permutation(self.total_many))\n            else:\n                self.present_order = torch.roll(self.present_order, self.mini_many, dims=0)\n            self.present_bias = (self.present_bias + self.mini_many) % self.total_many\n\n            item = super().__getitem__(self.present_index)\n            item_new = {}\n            for k,v in item.items():\n                if v.shape[0] == self.many:\n                    v = v[self.present_order]\n                    v = v.view(self.total_many // self.mini_many, self.mini_many, *v.shape[1:])\n                    item_new[k] = v\n                else:\n                    item_new[k] = v\n            return item_new\n\n    return one2many_dataset_class_with_across_batch\n\n\nclass One2ManyInferenceProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        tasks = []\n        for usage in ["train", "validation", "test"]:\n            dataset = IMDBDatasetForInference(usage)\n            for item in dataset:\n                task = {"input_ids": item["input_ids"].tolist(),\n                        "usage": usage, "from_model": self.args.sft_model_path}\n                tasks.append(task)\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None and \\\n               all([task[key]==result[key]\n                    for key in ["input_ids", "usage", "from_model"]])\n\n\nclass One2ManyInferenceConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        from transformers import GPT2LMHeadModel\n        self.model = GPT2LMHeadModel.from_pretrained(\n            self.args.sft_model_path,\n            device_map=f"cuda:{self.args.local_rank}",\n            torch_dtype=torch.float16,\n        ).eval()\n        self.tokenizer = get_tokenizer()\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        many = 32\n        prompt_len = max([len(task["input_ids"][0]) for task in tasks])\n        input_ids = pad_and_concat(\n            [torch.LongTensor(task["input_ids"]) for task in tasks],\n            pad_token_id=self.tokenizer.pad_token_id,\n            padding_side="left"\n        ).to(self.device)\n        assert input_ids.shape[1] == prompt_len\n        outputs = self.model.generate(\n            input_ids=input_ids,\n            do_sample=True,\n            top_k=50,\n            min_length=input_max_length+output_max_length,\n            max_new_tokens=output_max_length,\n            num_return_sequences=many,\n            eos_token_id=self.tokenizer.eos_token_id,\n            pad_token_id=self.tokenizer.pad_token_id,\n        )\n        generations = self.tokenizer.batch_decode(\n            outputs[:,prompt_len:], skip_special_tokens=True\n        )\n        prompts = self.tokenizer.batch_decode(\n            input_ids, skip_special_tokens=True\n        )\n        for i,task in enumerate(tasks):\n            task["prompt"] = prompts[i]\n            task["generation"] = generations[i*many:(i+1)*many]\n            assert torch.all(outputs[i*many:(i+1)*many,:prompt_len]==input_ids[i:i+1,:])\n        return tasks\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce processing on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="one2many_inference")\n    parser.add_argument("--output_file", type=str, default=f"one2many_inference.json")\n    parser.add_argument("--erase", type=int, default=0)\n    parser.add_argument("--sft_model_path", type=auto_find_checkpoint_dir,\n                        default=auto_find_checkpoint_dir(f"sft/checkpoint"))\n    parser.add_argument("--batch_size", type=int, default=32)\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    if args.local_rank == -1:\n        producer = One2ManyInferenceProducer(args)\n        producer.run()\n    else:\n        consumer = One2ManyInferenceConsumer(args)\n        consumer.run()\n', './LPO-sentiment\\modeling_gpt2_cvae.py': 'import copy\n\nimport torch.nn.functional as F\nfrom transformers.models.gpt2.modeling_gpt2 import *\n\nfrom config_and_utils import BASE_MODEL_PATH, load_checkpoint\n\n\ndef sampling(mean, logvar, n_samples=1):\n    mu = torch.stack([mean] * n_samples, dim=-1)\n    sigma = torch.stack([torch.exp(logvar * 0.5)] * n_samples, dim=-1)\n    eps = torch.zeros_like(sigma).normal_()\n    zs = eps * sigma + mu\n    return zs\n\n\ndef log_pdf(mean, logvar, zs) -> torch.Tensor:\n    import numpy as np\n    if len(zs.shape) == len(mean.shape) + 1:\n        mean = mean.unsqueeze(-1)\n        logvar = logvar.unsqueeze(-1)\n    return -0.5 * np.log(2 * np.pi) - 0.5 * logvar - \\\n           (zs - mean).pow(2) / (2 * torch.exp(logvar) + 1e-4)\n\n\ndef dgkld(config, prior_mean, prior_logvar, post_mean, post_logvar):\n    batch_size, dim_z = prior_mean.shape\n\n    n_samples = config.kl_sampling_times\n    zs = sampling(post_mean, post_logvar, n_samples)\n\n    priors_mean = prior_mean.unsqueeze(-1).repeat(1, 1, n_samples)\n    priors_logvar = prior_logvar.unsqueeze(-1).repeat(1, 1, n_samples)\n    # [batch_size, dim_z, n_samples]\n    logp_priors_zs = log_pdf(priors_mean, priors_logvar, zs)\n    # e.g.\n    #         z1 z2 z3\n    # prior   .. .. ..\n    # [batch_size, dim_z, n_samples]\n\n    zs = zs.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\n    posts_mean = post_mean.unsqueeze(1).unsqueeze(-1).repeat(1, batch_size, 1, n_samples)\n    posts_logvar = post_logvar.unsqueeze(1).unsqueeze(-1).repeat(1, batch_size, 1, n_samples)\n    # [batch_size, batch_size, dim_z, n_samples]\n    # the first "batch_size" is of prior/post, and the second "batch_size" is of zs\n    # in another perspective, the first / second is of aggreagation / stratified sampling\n    logp_posts_zs = log_pdf(posts_mean, posts_logvar, zs)\n    # e.g.\n    #        z1 z2 z3\n    # post1  .. .. ..\n    # post2  .. .. ..\n    # post3  .. .. ..\n\n    if config.marginal_kl:\n        # regularization on each dimension respectively\n        logp_posts_zs = logp_posts_zs.view(batch_size, batch_size, dim_z, n_samples)\n        # [batch_size, batch_size, dim_z, n_samples]\n        logp_priors_zs = logp_priors_zs.view(batch_size, dim_z, n_samples)\n        # [batch_size, dim_z, n_samples]\n    else:\n        # regularization in the high-dimensional joint latent space\n        logp_posts_zs = logp_posts_zs.sum(dim=-2, keepdims=True)\n        # [batch_size, batch_size, 1, n_samples]\n        logp_priors_zs = logp_priors_zs.sum(dim=-2, keepdims=True)\n        # [batch_size, 1, n_samples]\n\n    # aggregation: post1(z), post2(z), post3(z) -> post_agg(z)\n\n    logp_posts_max = logp_posts_zs.max(dim=0).values\n    logp_agg_post_zs = (logp_posts_zs - logp_posts_max).exp().mean(dim=0).log() + logp_posts_max\n    # [batch_size, 1 or dim_z, n_samples]\n    # e.g. (the dim of post-agg is removed by mean with keepdims=False)\n    #           z1 z2 z3\n    # post-agg  .. .. ..\n\n    # mote carlo with stratified sampling: post_agg(z1), post_agg(z2), post_agg(z3) -> post_agg(z_agg)\n\n    # mote carlo with stratified sampling: prior(z1), prior(z2), prior(z3) -> prior(z_agg)\n    density_gaps = logp_agg_post_zs - logp_priors_zs\n    # [batch_size, 1 or dim_z, n_samples]\n    kl = density_gaps.mean(dim=0)\n    # [1 or dim_z, n_samples]\n\n    kl = kl.sum(dim=0).mean(dim=-1)\n\n    # []\n    return kl\n\n\nclass GPT2CVAEConfig(GPT2Config):\n    def __init__(\n        self,\n        base_model_path=BASE_MODEL_PATH,\n        num_q=4,\n        dim_z=32,\n        num_p=4,\n        latent_aggregator_layers=2,\n        post_with_x=0,\n        frozen_pretrained=True,\n        use_standard_prior=True,\n        marginal_kl=True,\n        lm_sampling_times=1,\n        kl_sampling_times=16,\n        lpo_sampling_times=64,\n        latent_weight_init_norm=0.1,\n        without_contra=0,\n        without_dg_kld=0,\n        with_bn=0,\n        contra_loss_weight=1.0,\n        two_stage_contra=0,\n        add_skip_connection=0,\n        add_skip_residue=0,\n        add_contra_loss=0,\n        add_skip_residue_contra=0,\n        lpo_post_sampling=0,\n        lpo_evidence_ratio=0.5,\n        marginal_lpo=0,\n        expectation_lpo=0,\n        beta=0.5,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        # for model structure\n        self.base_model_path = base_model_path\n        self.num_q = num_q\n        self.dim_z = dim_z\n        self.num_p = num_p\n        self.latent_aggregator_layers = latent_aggregator_layers\n        self.post_with_x = post_with_x\n        # for training and inference\n        self.frozen_pretrained = frozen_pretrained\n        self.use_standard_prior = use_standard_prior\n        self.marginal_kl = marginal_kl\n        self.lm_sampling_times = lm_sampling_times\n        self.kl_sampling_times = kl_sampling_times\n        self.lpo_sampling_times = lpo_sampling_times\n        self.latent_weight_init_norm = latent_weight_init_norm\n        self.without_contra = without_contra\n        self.without_dg_kld = without_dg_kld\n        self.with_bn = with_bn\n        self.contra_loss_weight = contra_loss_weight\n        self.two_stage_contra = two_stage_contra\n        self.add_skip_connection = add_skip_connection\n        self.add_skip_residue = add_skip_residue\n        self.add_contra_loss = add_contra_loss\n        self.add_skip_residue_contra = add_skip_residue_contra\n        self.lpo_post_sampling = lpo_post_sampling\n        self.lpo_evidence_ratio = lpo_evidence_ratio\n        self.marginal_lpo = marginal_lpo\n        self.expectation_lpo = expectation_lpo\n        self.beta = beta\n        # others\n        self.pad_token_id = self.eos_token_id\n\n\nclass LatentAggregator(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.queries = nn.Parameter(torch.Tensor(1, config.num_q, config.n_embd))\n        self.queries.data.normal_(mean=0.0, std=config.initializer_range)\n        self.ln_input = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.blocks = nn.ModuleList([\n            GPT2Block(config)\n            for _ in range(config.latent_aggregator_layers)\n        ])\n        self.ln_output = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.h2z = nn.Linear(config.n_embd, config.dim_z * 2 // config.num_q)\n\n    def standardize(self):\n        # regularize the output latent distribution to standard gaussian\n        self.h2z.weight.data.zero_()\n        self.h2z.bias.data.zero_()\n        return\n\n    def forward(\n        self,\n        input_embeds: torch.FloatTensor,\n        input_embeds_mask: torch.FloatTensor = None,\n        fix_entropy: bool = False,\n    ):\n        dtype, device = self.queries.dtype, self.queries.device\n        batch_size, seq_len, hidden_size = input_embeds.shape\n        if input_embeds_mask is not None:\n            input_embeds_mask = input_embeds_mask.to(dtype=dtype, device=device)  # fp16 compatibility\n        else:\n            input_embeds_mask = torch.ones([batch_size, seq_len], dtype=dtype, device=device)\n\n        self_attn_mask = torch.ones([batch_size, self.config.num_q], dtype=dtype, device=device)\n\n        attention_mask = torch.cat([input_embeds_mask, self_attn_mask], dim=1)  # [batch_size, to_seq_length]\n        attention_mask = attention_mask[:, None, None, :]  # [batch_size, num_heads, from_seq_length, to_seq_length]\n        attention_mask = (1.0 - attention_mask) * torch.finfo(dtype).min\n\n        hidden_states = torch.cat([input_embeds, self.queries.repeat(batch_size, 1, 1)], dim=1)\n\n        hidden_states = self.ln_input(hidden_states)\n\n        for block in self.blocks:\n            hidden_states = block(\n                hidden_states,\n                attention_mask=attention_mask,\n            )[0]\n        latent = self.h2z(self.ln_output(hidden_states[:, -self.config.num_q:, :]))\n        mean = latent[:, :, :latent.shape[-1]//2].reshape(batch_size, self.config.dim_z)\n        logvar = latent[:, :, latent.shape[-1]//2:].reshape(batch_size, self.config.dim_z)\n\n        if fix_entropy:\n            logvar = logvar - logvar.sum(dim=-1, keepdim=True) / self.config.dim_z\n            # enforce the entropy of prior distribution equal\n            # to that of standard gaussian distribution\n\n        return mean, logvar\n\n\nclass LatentEncoder(nn.Module):\n    def __init__(self, config, fix_entropy=False):\n        super().__init__()\n        self.config = config\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.aggregator = LatentAggregator(config)\n        self.fix_entropy = fix_entropy\n        self.aggregator.standardize()\n\n    def load_pretrained(self, pretrained_model):\n        self.wte.load_state_dict(pretrained_model.transformer.wte.state_dict())\n        for i in range(self.config.latent_aggregator_layers):\n            self.aggregator.blocks[i].load_state_dict(pretrained_model.transformer.h[i].state_dict())\n        self.aggregator.standardize()\n\n    def standardize(self):\n        self.aggregator.standardize()\n\n    def init_wte(self, wte):\n        self.wte.load_state_dict(wte.state_dict())\n\n    def init_wte_from_lm_head(self, lm_head):\n        self.wte.load_state_dict(lm_head.state_dict(), strict=False)\n\n    def roll_right_padding_to_left(self, right_padded_ids):\n        batch_size, seq_len = right_padded_ids.shape\n        last_valid_idx = (right_padded_ids != self.config.pad_token_id).sum(dim=1)\n        left_padded_ids = right_padded_ids.clone()\n        for i in range(batch_size):\n            left_padded_ids[i] = torch.roll(left_padded_ids[i], seq_len - last_valid_idx[i].item(), dims=0)\n        return left_padded_ids\n\n    def forward(self, input_ids):\n        input_ids = self.roll_right_padding_to_left(input_ids).long()\n        input_embeds = self.wte(input_ids)\n        input_embeds_mask = (input_ids != self.config.pad_token_id).float()\n        mean, logvar = self.aggregator(\n            input_embeds=input_embeds,\n            input_embeds_mask=input_embeds_mask,\n            fix_entropy=self.fix_entropy,\n        )\n        return mean, logvar\n\n\nclass LatentDecoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.z2hs = nn.ModuleList([nn.Sequential(\n            nn.Linear(config.dim_z, config.n_embd),\n            nn.GELU(),\n            nn.Linear(config.n_embd, config.n_embd * config.num_p)\n        ) for i in range(config.n_layer)])\n        """\n        self.z2hs = nn.ModuleList([nn.Linear(config.dim_z, config.n_embd * config.num_p)\n                                  for i in range(config.n_layer)])\n        """\n        self.lns = nn.ModuleList([nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n                                  for i in range(config.n_layer)])\n        self.attns = nn.ModuleList([GPT2Attention(config, layer_idx=i)\n                                    for i in range(config.n_layer)])\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n\n    def load_pretrained(self, pretrained_model):\n        for i in range(self.config.n_layer):\n            self.attns[i].load_state_dict(pretrained_model.transformer.h[i].attn.state_dict())\n\n    def decode(self, zs):\n        # input_ps mimic past_key_values in gptj\n        # past_key_values: list (in length of n_layers) of past_key and past_value\n        # past_key and past_value: (batch_size, num_heads, seq_length, head_features)\n        batch_size = zs.shape[0]\n\n        past_key_values = []\n        for i in range(self.config.n_layer):\n            z2h, ln, attn = self.z2hs[i], self.lns[i], self.attns[i]\n            hidden_states = z2h(zs)\n            hidden_states = hidden_states.view(batch_size, self.config.num_p, self.config.n_embd)\n            hidden_states = ln(hidden_states)\n            present = attn(\n                hidden_states=hidden_states,\n                use_cache=True,\n            )[1]\n            past_key_values.append(present)\n        \n        input_ps = past_key_values\n        return input_ps\n\n    def forward(self, mean, logvar):\n        # mean, logvar -> zs -> input_ps\n        if self.config.lm_sampling_times == 0:\n            zs = mean\n        else:\n            zs = sampling(mean, logvar).squeeze(-1)\n        input_ps = self.decode(zs)\n        return input_ps\n\n    def get_latent_nlls(self, zs, mean, logvar):\n        return -log_pdf(mean, logvar, zs).sum(dim=-1)\n\n\nclass GPT2ForVAE(GPT2PreTrainedModel):\n    config_class = GPT2CVAEConfig\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config: GPT2CVAEConfig):\n        super().__init__(config)\n        self.config = config\n        self.latent_encoder = LatentEncoder(config)\n        self.latent_decoder = LatentDecoder(config)\n        self.base = GPT2LMHeadModel(config)\n        self.skip_connection = nn.Sequential(\n            nn.Linear(self.config.dim_z, self.config.n_embd),\n            nn.GELU(),\n            nn.Linear(self.config.n_embd, self.config.n_embd)\n        )\n\n        if self.config.with_bn:\n            self.bn = nn.BatchNorm1d(self.config.dim_z)\n            self.bn.weight.requires_grad = False\n            self.bn.weight.data.fill_(0.5)\n\n        self.update_requires_grad_()\n\n    def update_requires_grad_(self):\n        self.base.requires_grad_(not self.config.frozen_pretrained)\n        self.latent_decoder.attns.requires_grad_(not self.config.frozen_pretrained)\n\n    def load_pretrained(self, pretrained_model=None):\n        if pretrained_model is None:\n            pretrained_model = GPT2LMHeadModel.from_pretrained(self.config.base_model_path)\n        self.base.load_state_dict(pretrained_model.state_dict(), strict=False)\n        self.latent_encoder.load_pretrained(pretrained_model)\n        self.latent_decoder.load_pretrained(pretrained_model)\n\n    def skip_grad_lm_loss(self, post_mean, post_logvar, input_ps, input_ids, labels, grad_weight=1.0, reduction="mean"):\n        last_hidden_state = self.base.transformer(\n            input_ids=input_ids,\n            past_key_values=input_ps,\n            return_dict=True,\n        ).last_hidden_state\n        if self.config.lm_sampling_times == 0:\n            zs = post_mean.unsqueeze(-1).repeat(1, 1, last_hidden_state.shape[1])\n        else:\n            zs = sampling(post_mean, post_logvar, n_samples=last_hidden_state.shape[1])\n        # [batch_size, dim_z, seq_len]\n        skip_hidden_state = F.layer_norm(self.skip_connection(zs.transpose(1, 2)),\n                                         normalized_shape=[self.config.n_embd],\n                                         eps=self.config.layer_norm_epsilon)\n        skip_residue = (skip_hidden_state - skip_hidden_state.detach()) \\\n            if not self.config.add_skip_residue else skip_hidden_state\n        last_hidden_state = last_hidden_state + skip_residue * grad_weight\n        lm_logits = self.base.lm_head(last_hidden_state)\n        nlls_skip = F.cross_entropy(\n            input=lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n            target=labels[:, 1:].contiguous(),\n            reduction=\'none\'\n        ).sum(dim=-1)\n\n        if reduction == "mean" and self.config.add_skip_residue_contra:\n            loss_skip_residue_contra = self.skip_residue_contra_loss(last_hidden_state, skip_hidden_state, labels)\n            return nlls_skip.mean(), loss_skip_residue_contra\n\n        if reduction == "mean":\n            loss_lm_skip = nlls_skip.mean()\n            return loss_lm_skip\n        elif reduction == "none" or reduction is None:\n            return nlls_skip\n        else:\n            raise NotImplementedError(f"reduction: {reduction}")\n\n    def skip_residue_contra_loss(self, last_hidden_state, skip_hidden_state, labels):\n        self.base.requires_grad_(False)\n        in_batch_skip_nlls = []\n        batch_size = last_hidden_state.shape[0]\n        for bias in range(batch_size):\n            biased_skip_hidden_state = torch.roll(skip_hidden_state, shifts=bias, dims=0)\n            biased_skip_last_hidden_state = last_hidden_state.detach() + biased_skip_hidden_state\n            biased_skip_lm_logits = self.base.lm_head(biased_skip_last_hidden_state)\n            biased_skip_nlls = F.cross_entropy(\n                input=biased_skip_lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n                target=labels[:, 1:].contiguous(),\n                reduction=\'none\'\n            ).sum(dim=-1)\n            in_batch_skip_nlls.append(biased_skip_nlls)\n        in_batch_skip_nlls = torch.stack(in_batch_skip_nlls, dim=0)\n        loss_skip_residue_contra = F.cross_entropy(\n            input=-in_batch_skip_nlls.T,\n            target=torch.zeros(batch_size, dtype=torch.int64, device=in_batch_skip_nlls.device),\n            reduction=\'mean\'\n        )\n        self.base.requires_grad_(not self.config.frozen_pretrained)\n        return loss_skip_residue_contra\n\n    def nlls(self, input_ps, input_ids, labels, skip_hidden_state=None):\n        last_hidden_state = self.base.transformer(\n            input_ids=input_ids,\n            past_key_values=input_ps,\n            return_dict=True,\n        ).last_hidden_state\n        if skip_hidden_state is not None:\n            last_hidden_state = last_hidden_state + skip_hidden_state\n        lm_logits = self.base.lm_head(last_hidden_state)\n        nlls = F.cross_entropy(\n            input=lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n            target=labels[:, 1:].contiguous(),\n            reduction=\'none\'\n        ).sum(dim=-1)\n        return nlls\n\n    def lm_loss(self, input_ps, input_ids, labels):\n        return self.nlls(input_ps, input_ids, labels).mean()\n\n    def contra_loss(self, input_ps, input_ids, labels, positive_skip_zs=None):\n        batch_size = labels.shape[0]\n        prefix_length = torch.stack([torch.nonzero(labels[i, :] != -100)[0][0] for i in range(batch_size)], dim=0)\n        assert torch.all(prefix_length == prefix_length[0]), f"unexpected data with variable prefix length: " \\\n                                                             f"{prefix_length}\\n{labels.tolist()}"\n        prefix_length = prefix_length[0].item()\n        assert prefix_length > 0, f"unexpected data with no prefix:\\n{labels.tolist()}"\n        assert torch.all(input_ids[:, :prefix_length] == input_ids[0, :prefix_length]), \\\n            f"unexpected data with non-shared prefix:\\n{input_ids[:, :prefix_length].tolist()}"\n\n        prefix_length -= 1  # the labels will be shifted\n        prefix_ids = input_ids[:, :prefix_length]\n        input_ids = input_ids[:, prefix_length:]\n        labels = labels[:, prefix_length:]\n\n        prompted_prefix_outputs = self.base.transformer(\n            input_ids=prefix_ids,\n            past_key_values=input_ps,\n            use_cache=True,\n            return_dict=True\n        )\n        past_key_values = prompted_prefix_outputs.past_key_values\n\n        in_batch_nlls = []\n        for bias in range(batch_size):\n            biased_past_key_values = [\n                (torch.roll(past_keys, shifts=bias, dims=0),\n                 torch.roll(past_values, shifts=bias, dims=0))\n                for past_keys, past_values in past_key_values\n            ]\n            if positive_skip_zs is not None and bias == 0:\n                skip_hidden_state = F.layer_norm(\n                    self.skip_connection(positive_skip_zs.transpose(1, 2)),\n                    normalized_shape=[self.config.n_embd],\n                    eps=self.config.layer_norm_epsilon\n                )[:, prefix_length:, :]\n                skip_hidden_state = skip_hidden_state - skip_hidden_state.detach()\n            else:\n                skip_hidden_state = None\n            in_batch_nlls.append(self.nlls(biased_past_key_values, input_ids, labels,\n                                           skip_hidden_state=skip_hidden_state))\n        in_batch_nlls = torch.stack(in_batch_nlls, dim=0)\n        loss_contra = F.cross_entropy(\n            input=-in_batch_nlls.T,\n            target=torch.zeros(batch_size, dtype=torch.int64, device=in_batch_nlls.device),\n            reduction=\'mean\'\n        )\n        return loss_contra\n\n    def forward(\n        self,\n        post_ids: torch.LongTensor,  # [batch, ans_seq_len]\n        input_ids: torch.LongTensor,  # [batch, seq_len]\n        labels: torch.LongTensor,  # [batch, seq_len]\n        return_loss=True,\n        **kwargs,\n    ):\n        if self.config.post_with_x:\n            post_mean, post_logvar = self.latent_encoder(input_ids)\n        else:\n            post_mean, post_logvar = self.latent_encoder(post_ids)\n        if self.config.with_bn:\n            post_mean = self.bn(post_mean)\n\n        prior_mean, prior_logvar = torch.zeros_like(post_mean), torch.zeros_like(post_logvar)\n        loss_kld = dgkld(self.config, prior_mean, prior_logvar, post_mean, post_logvar)\n\n        # only compute lm loss for the (currently) concerned output\n        input_ps = self.latent_decoder(post_mean, post_logvar)\n        # n_layer * [mini_many, num_p, n_embd] <- [mini_many, dim_z]\n\n        if self.config.add_contra_loss and self.config.add_skip_connection:\n            positive_skip_zs = sampling(post_mean, post_logvar, n_samples=labels.shape[1])\n            loss_contra = self.contra_loss(input_ps, input_ids, labels, positive_skip_zs=positive_skip_zs)\n            loss_lm = self.lm_loss(input_ps, input_ids, labels)\n        elif self.config.add_contra_loss:\n            loss_contra = self.contra_loss(input_ps, input_ids, labels, positive_skip_zs=None)\n            loss_lm = self.lm_loss(input_ps, input_ids, labels)\n        elif self.config.add_skip_connection:\n            loss_contra = torch.zeros_like(input_ps[0][0].sum())\n            loss_lm = self.skip_grad_lm_loss(post_mean, post_logvar, input_ps, input_ids, labels)\n        else:\n            loss_contra = torch.zeros_like(input_ps[0][0].sum())\n            loss_lm = self.lm_loss(input_ps, input_ids, labels)\n\n        if self.config.without_dg_kld:\n            standard_kld = 0.5 * (post_mean.pow(2) + post_logvar.exp() - post_logvar - 1).sum(dim=1).mean(dim=0)\n            loss_vae = loss_lm + standard_kld + loss_contra\n        else:\n            loss_vae = loss_lm + loss_kld + loss_contra\n\n        return loss_vae, loss_lm, loss_kld, loss_contra\n\n    def load_lpo_policy_latent_encoder(self, lpo_model_path):\n        lpo_model = LPOModel(self.config)\n        lpo_model.load_state_dict(load_checkpoint(lpo_model_path, device=self.device))\n        self.latent_encoder.load_state_dict(lpo_model.policy_latent_encoder.state_dict())\n        self.config.use_standard_prior = False\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=None,\n        streamer=None,\n        post_labels: torch.LongTensor = None,\n        post_latent: Tuple[torch.FloatTensor] = None,\n        standard_prior: bool = False,\n        latent_sampling: bool = None,\n        **kwargs,\n    ):\n        input_ids = kwargs["input_ids"]\n        if latent_sampling is None:\n            latent_sampling = self.config.use_standard_prior or standard_prior\n\n        prior_mean, prior_logvar = self.latent_encoder(input_ids)\n        if post_latent is not None:\n            mean, logvar = post_latent\n            mean = mean.to(device=prior_mean.device, dtype=prior_mean.dtype)\n            logvar = logvar.to(device=prior_logvar.device, dtype=prior_logvar.dtype)\n        elif post_labels is not None:\n            mean, logvar = self.latent_encoder(post_labels)\n        elif self.config.use_standard_prior or standard_prior:\n            mean, logvar = torch.zeros_like(prior_mean), torch.zeros_like(prior_logvar)\n        else:\n            mean, logvar = prior_mean, prior_logvar\n\n        if not latent_sampling:\n            logvar.fill_(-100)\n\n        input_ps = self.latent_decoder(mean, logvar)\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids[:, :-1] != self.config.pad_token_id).long()\n        ], dim=1)\n\n        outputs = self.base.transformer(\n            input_ids=input_ids[:, :-1],\n            past_key_values=input_ps,\n            attention_mask=attention_mask,\n            use_cache=True,\n            return_dict=True\n        )\n        kwargs["past_key_values"] = outputs.past_key_values\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids != self.config.pad_token_id).long()\n        ], dim=1)\n        kwargs["attention_mask"] = attention_mask\n        kwargs["use_cache"] = True\n\n        if self.config.add_skip_connection and self.config.add_skip_residue:\n            if self.config.lm_sampling_times == 0:\n                zs = mean.unsqueeze(-1)\n            else:\n                zs = sampling(mean, logvar, n_samples=1)\n            skip_hidden_state = F.layer_norm(self.skip_connection(zs.transpose(1, 2)),\n                                             normalized_shape=[self.config.n_embd],\n                                             eps=self.config.layer_norm_epsilon)\n            kwargs["skip_residue"] = skip_hidden_state\n\n        return self.base.generate(\n            inputs=inputs,\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            stopping_criteria=stopping_criteria,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            synced_gpus=synced_gpus,\n            streamer=streamer,\n            **kwargs,\n        )\n\n\nclass LPOModel(nn.Module):\n    main_input_name = "prompt_ids"\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.prior_latent_encoder = LatentEncoder(config)\n        self.prior_latent_encoder.requires_grad_(False)\n        self.post_latent_encoder = LatentEncoder(config)\n        self.post_latent_encoder.requires_grad_(False)\n        self.policy_latent_encoder = LatentEncoder(config)\n\n    @classmethod\n    def from_vae(cls, base_model_path, **kwargs):\n        gpt2_vae = GPT2ForVAE.from_pretrained(base_model_path)\n        config = gpt2_vae.config\n        config.update(kwargs)\n        config.use_standard_prior = True\n        model = cls(config)\n        model.policy_latent_encoder.load_state_dict(gpt2_vae.latent_encoder.state_dict())\n        model.policy_latent_encoder.standardize()\n        model.post_latent_encoder.load_state_dict(gpt2_vae.latent_encoder.state_dict())\n        return model\n\n    def forward(\n        self,\n        prompt_ids: torch.LongTensor,  # x\n        chosen_ids: torch.LongTensor,  # y_w\n        rejected_ids: torch.LongTensor,  # y_l\n    ):\n        prior_ids, chosen_post_ids, rejected_post_ids = prompt_ids, chosen_ids, rejected_ids\n        # the latent distribution of pi_theta(z|x), pi_ref(z|x)\n        # and approximation of pi_ref(z|x,y_w), pi_ref(z|x,y_l) (by their posterior distributions)\n        policy_mean, policy_logvar = self.policy_latent_encoder(prior_ids)\n        with torch.no_grad():\n            if self.config.use_standard_prior:\n                prior_mean, prior_logvar = torch.zeros_like(policy_mean), torch.zeros_like(policy_logvar)\n            else:\n                prior_mean, prior_logvar = self.prior_latent_encoder(prior_ids)\n            if self.config.post_with_x:\n                chosen_post_mean, chosen_post_logvar = self.post_latent_encoder(\n                    torch.cat([prior_ids, chosen_post_ids], dim=1)\n                )\n                rejected_post_mean, rejected_post_logvar = self.post_latent_encoder(\n                    torch.cat([prior_ids, rejected_post_ids], dim=1)\n                )\n            else:\n                chosen_post_mean, chosen_post_logvar = self.post_latent_encoder(chosen_post_ids)\n                rejected_post_mean, rejected_post_logvar = self.post_latent_encoder(rejected_post_ids)\n        # batch_size, dim_z\n\n        n_samples = self.config.lpo_sampling_times\n        if self.config.expectation_lpo:\n            zs = sampling(policy_mean, policy_logvar, n_samples=n_samples)\n        else:\n            prior_zs = sampling(prior_mean, prior_logvar, n_samples=n_samples)\n            if self.config.lpo_post_sampling:\n                chosen_zs = sampling(chosen_post_mean, chosen_post_logvar, n_samples=n_samples)\n                rejected_zs = sampling(rejected_post_mean, rejected_post_logvar, n_samples=n_samples)\n                zs = torch.cat([prior_zs, chosen_zs, rejected_zs], dim=-1)\n            else:\n                zs = prior_zs\n        # batch_size, dim_z, n_samples\n\n        # -log pi_ref(z_d|x,y_w)\n        zs_marginal_nlls_given_chosen = -log_pdf(chosen_post_mean, chosen_post_logvar, zs)\n        # -log pi_ref(z_d|x,y_l)\n        zs_marginal_nlls_given_rejected = -log_pdf(rejected_post_mean, rejected_post_logvar, zs)\n        # batch_size, dim_z, n_samples\n\n        if self.config.marginal_lpo:\n            batch_size, dim_z, n_samples = zs_marginal_nlls_given_chosen.shape\n            zs_nlls_given_chosen = zs_marginal_nlls_given_chosen.view(batch_size * dim_z, n_samples)\n            zs_nlls_given_rejected = zs_marginal_nlls_given_rejected.view(batch_size * dim_z, n_samples)\n        else:\n            # -log pi_ref(z|x,y_w)\n            zs_nlls_given_chosen = zs_marginal_nlls_given_chosen.sum(dim=1)\n            # -log pi_ref(z|x,y_l)\n            zs_nlls_given_rejected = zs_marginal_nlls_given_rejected.sum(dim=1)\n\n        # r(x,z) = (w_win*r_win + w_loss*r_loss) / (w_win + w_loss)\n        # w \\propto exp(-zs_nlls_given_chosen/rejected)\n        importance_weights = F.softmax(\n            torch.stack([-zs_nlls_given_chosen, -zs_nlls_given_rejected], dim=-1),\n            dim=-1\n        )\n        win_lose_rewards = torch.Tensor([1, 0]).to(\n            device=importance_weights.device, dtype=importance_weights.dtype\n        )[None, None, :]\n        latent_rewards = (importance_weights * win_lose_rewards).sum(dim=-1)\n        score_zs = latent_rewards\n        # batch_size, n_samples\n\n        zs_marginal_refer_nlls = -log_pdf(prior_mean, prior_logvar, zs)\n        zs_marginal_policy_nlls = -log_pdf(policy_mean, policy_logvar, zs)\n        if self.config.marginal_lpo:\n            zs_refer_nlls = zs_marginal_refer_nlls.view(batch_size * dim_z, n_samples)\n            zs_policy_nlls = zs_marginal_policy_nlls.view(batch_size * dim_z, n_samples)\n        else:\n            zs_refer_nlls = zs_marginal_refer_nlls.sum(dim=1)\n            zs_policy_nlls = zs_marginal_policy_nlls.sum(dim=1)\n        # batch_size, n_samples\n\n        if self.config.expectation_lpo:\n            # maximize E_{z \\sim \\pi_{\\theta}(z|x)} r(x,z) - \\beta [log \\pi_{\\theta}(z|x) - log \\pi_{ref}(z|x)]\n            loss = self.config.beta * (zs_refer_nlls - zs_policy_nlls) - latent_rewards\n            loss = loss.mean()\n            acc = torch.zeros_like(loss)\n            return loss, acc\n\n        comparison_matrix = score_zs[:, :, None] - score_zs[:, None, :]\n        # import pdb;pdb.set_trace()\n        comparison_matrix = comparison_matrix.detach()\n        # batch_size, n_samples, n_samples\n\n        zs_policy_reward = (zs_refer_nlls.detach() - zs_policy_nlls)\n        # batch_size, n_samples\n\n        batch_size, n_samples = zs_policy_reward.shape\n\n        zs_policy_reward_left = zs_policy_reward[:, :, None].repeat(1, 1, n_samples)\n        zs_policy_reward_right = zs_policy_reward[:, None, :].repeat(1, n_samples, 1)\n        # batch_size, n_samples, n_samples\n\n        evidence_thresh = torch.quantile(comparison_matrix.view(batch_size, -1).float(),\n                                         1 - self.config.lpo_evidence_ratio / 2, dim=1)\n        left_win = comparison_matrix >= evidence_thresh[:, None, None]\n        # batch_size, n_samples, n_samples\n        comparison_mask = 1 - torch.eye(n_samples).float()\n        comparison_mask = comparison_mask.unsqueeze(0).repeat(batch_size, 1, 1)\n        comparison_mask = comparison_mask.to(comparison_matrix.device)\n        # batch_size, n_samples, n_samples\n        comparison_mask = comparison_mask * left_win\n\n        disadvantage = (zs_policy_reward_right - zs_policy_reward_left) * self.config.beta  # to minimize\n        loss = -F.logsigmoid(-disadvantage)\n        acc = (loss < 0.6931).float()\n        loss = (loss * comparison_mask).sum() / comparison_mask.sum()\n        acc = (acc * comparison_mask).sum() / comparison_mask.sum()\n\n        return loss, acc\n\n\nclass DPOModel(GPT2PreTrainedModel):\n    config_class = GPT2Config\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.base = GPT2LMHeadModel(config)\n        self.beta = config.beta\n\n    def load_pretrained(self, pretrained_model=None):\n        if pretrained_model is None:\n            pretrained_model = GPT2LMHeadModel.from_pretrained(self.config.base_model_path)\n        self.base.load_state_dict(pretrained_model.state_dict())\n\n    def prepare_refer_model(self):\n        self.refer_model = copy.deepcopy(self)\n        self.refer_model.requires_grad_(False)\n        self.refer_model.eval()\n\n    def compute_nlls(self, input_ids, labels):\n        hidden_states = self.base.transformer(input_ids=input_ids)[0]\n        lm_logits = self.base.lm_head(hidden_states).to(torch.float32)\n        labels = labels.to(lm_logits.device)\n        shift_logits = lm_logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        nlls = F.cross_entropy(\n            input=shift_logits.transpose(1, 2),\n            target=shift_labels,\n            reduction=\'none\'\n        ).sum(dim=-1)\n        return nlls\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        labels: torch.LongTensor,\n        refer_nlls: torch.FloatTensor = None,\n        reward: torch.FloatTensor = None,  # r(x,y)\n    ):\n        theta_nlls = self.compute_nlls(input_ids=input_ids, labels=labels)\n        if refer_nlls is None:\n            assert hasattr(self, "refer_model")\n            with torch.no_grad():\n                refer_nlls = self.refer_model.compute_nlls(input_ids=input_ids, labels=labels)\n        log_iw = refer_nlls.detach() - theta_nlls\n        chosen_log_iw = log_iw[::2]\n        rejected_log_iw = log_iw[1::2]\n        disadvantage = (rejected_log_iw - chosen_log_iw) * self.beta\n        loss = -F.logsigmoid(-disadvantage)\n        acc = (loss < 0.6931).float()\n        return loss.mean(), acc.mean()\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=None,\n        streamer=None,\n        **kwargs,\n    ):\n        return self.base.generate(\n            inputs=inputs,\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            stopping_criteria=stopping_criteria,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            synced_gpus=synced_gpus,\n            streamer=streamer,\n            **kwargs,\n        )\n\n\nclass PTModel(GPT2PreTrainedModel):\n    config_class = GPT2CVAEConfig\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config: GPT2CVAEConfig):\n        super().__init__(config)\n        self.base = GPT2LMHeadModel(config)\n        self.base.requires_grad_(False)\n        self.hs = torch.nn.ParameterList([\n            torch.nn.Parameter(torch.Tensor(1, config.num_q, config.n_embd))\n            for i in range(config.n_layer)\n        ])\n        for param in self.hs:\n            param.data.normal_(mean=0.0, std=config.initializer_range)\n        self.hs_dropout = nn.Dropout(p=0.1)\n        self.lns = nn.ModuleList([nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n                                  for i in range(config.n_layer)])\n        self.attns = nn.ModuleList([GPT2Attention(config, layer_idx=i)\n                                    for i in range(config.n_layer)])\n        self.attns.requires_grad_(False)\n        self.beta = config.beta\n        self.mode = "sft"\n\n    def load_pretrained(self, pretrained_model=None):\n        if pretrained_model is None:\n            pretrained_model = GPT2LMHeadModel.from_pretrained(self.config.base_model_path)\n        self.base.load_state_dict(pretrained_model.state_dict())\n        for i in range(self.config.n_layer):\n            self.attns[i].load_state_dict(pretrained_model.transformer.h[i].attn.state_dict())\n\n    def switch_into_dpo_mode(self):\n        self.mode = "dpo"\n        self.refer_model = copy.deepcopy(self)\n        self.refer_model.requires_grad_(False)\n\n    def get_input_ps(self, input_ids):\n        batch_size = input_ids.shape[0]\n        past_key_values = []\n        for i in range(self.config.n_layer):\n            hidden_states, ln, attn = self.hs[i], self.lns[i], self.attns[i]\n            hidden_states = hidden_states.repeat(batch_size, 1, 1)\n            hidden_states = self.hs_dropout(ln(hidden_states))\n            present = attn(\n                hidden_states=hidden_states,\n                use_cache=True,\n            )[1]\n            past_key_values.append(present)\n        input_ps = past_key_values\n        return input_ps\n\n    def compute_nlls(self, input_ids, labels):\n        input_ps = self.get_input_ps(input_ids)\n        hidden_states = self.base.transformer.forward(\n            input_ids=input_ids,\n            past_key_values=input_ps\n        )[0]\n        lm_logits = self.base.lm_head(hidden_states).to(torch.float32)\n        labels = labels.to(lm_logits.device)\n        shift_logits = lm_logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        nlls = F.cross_entropy(\n            input=shift_logits.transpose(1, 2),\n            target=shift_labels,\n            reduction=\'none\'\n        ).sum(dim=-1)\n        return nlls\n\n    def forward(self, input_ids, labels, refer_nlls=None):\n        if self.mode == "sft":\n            # supervised fine-tuning\n            input_ps = self.get_input_ps(input_ids)\n            return self.base(\n                input_ids=input_ids,\n                past_key_values=input_ps,\n                labels=labels,\n                return_dict=False\n            )\n        elif self.mode == "dpo":\n            # dpo\n            theta_nlls = self.compute_nlls(input_ids=input_ids, labels=labels)\n            if refer_nlls is None:\n                assert hasattr(self, "refer_model")\n                self.refer_model.eval()\n                with torch.no_grad():\n                    refer_nlls = self.refer_model.compute_nlls(input_ids=input_ids, labels=labels)\n            log_iw = refer_nlls.detach() - theta_nlls\n            chosen_log_iw = log_iw[::2]\n            rejected_log_iw = log_iw[1::2]\n            disadvantage = (rejected_log_iw - chosen_log_iw) * self.beta\n            loss = -F.logsigmoid(-disadvantage)\n            acc = (loss < 0.6931).float()\n            return loss.mean(), acc.mean()\n        else:\n            raise NotImplementedError(f"mode: {self.mode}")\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=None,\n        streamer=None,\n        **kwargs,\n    ):\n        input_ids = kwargs["input_ids"]\n        input_ps = self.get_input_ps(input_ids)\n\n        outputs = self.base.transformer(\n            input_ids=input_ids[:, :-1],\n            past_key_values=input_ps,\n            use_cache=True,\n            return_dict=True\n        )\n        kwargs["past_key_values"] = outputs.past_key_values\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids != self.config.pad_token_id).long()\n        ], dim=1)\n        kwargs["attention_mask"] = attention_mask\n\n        return self.base.generate(\n            inputs=inputs,\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            stopping_criteria=stopping_criteria,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            synced_gpus=synced_gpus,\n            streamer=streamer,\n            **kwargs,\n        )', './LPO-sentiment\\test_generate_and_reward_and_ppl.py': 'import os\nimport json\n\nimport torch\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom gpu_map_reduce import BatchTaskConsumer, TaskConsumer, TaskProducer\nfrom config_and_utils import (\n    get_tokenizer, BASE_MODEL_PATH, load_checkpoint,\n    input_max_length, output_max_length, pad_and_concat,\n)\nfrom modeling_gpt2_cvae import (\n    GPT2Config, GPT2ForVAE,\n    GPT2LMHeadModel, PTModel, DPOModel\n)\n\nclass PPLProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        with open(self.args.input_file, "r", encoding="utf-8") as f:\n            tasks = json.loads(f.read())\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None \\\n               and all([(key in result and task[key]==result[key])\n                        for key in ["prompt", "output", "reference", "logits"]]) \\\n               and all([key in result\n                        for key in ["logits", "logits_along_length", "nll", "num_tokens",\n                                    "more_positive", "more_negative", "more_neutral"]])\n\nclass PPLConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        from transformers import GPT2LMHeadModel\n        from config_and_utils import BASE_MODEL_PATH, get_tokenizer\n        self.tokenizer = get_tokenizer()\n        self.model = GPT2LMHeadModel.from_pretrained(\n            BASE_MODEL_PATH, device_map=self.device,\n        ).eval()\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        input_sentences = [task["prompt"][0] + task["output"][0] for task in tasks]\n        inputs = self.tokenizer(input_sentences, return_tensors="pt", padding="longest")\n        inputs = {k:v.to(self.device) for k,v in inputs.items()}\n        with torch.no_grad():\n            lm_logits = self.model(**inputs).logits\n            lm_logits = lm_logits[:,:-1,:].contiguous()\n            labels = inputs["input_ids"][:,1:].contiguous()\n            labels = torch.where(labels==self.tokenizer.pad_token_id, -100, labels)\n            nlls = torch.nn.functional.cross_entropy(\n                input=lm_logits.transpose(1,2),\n                target=labels,\n                reduction=\'none\'\n            ).sum(dim=-1).tolist()\n            num_tokens = (labels!=-100).sum(dim=-1).tolist()\n        for i,task in enumerate(tasks):\n            task["nll"] = nlls[i]\n            task["num_tokens"] = num_tokens[i]\n        return tasks\n\nclass RewardProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        with open(self.args.input_file, "r", encoding="utf-8") as f:\n            tasks = json.loads(f.read())\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None \\\n               and all([(key in result and task[key]==result[key])\n                        for key in ["prompt", "output", "reference"]]) \\\n               and all([key in result\n                        for key in ["logits", "logits_along_length",\n                                    "more_positive", "more_negative", "more_neutral"]])\n\nclass RewardConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n        from config_and_utils import RWD_MODEL_PATH\n        self.tokenizer = AutoTokenizer.from_pretrained(RWD_MODEL_PATH)\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            RWD_MODEL_PATH, device_map=self.device,\n        ).eval()\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        input_sentences = [task["prompt"][i] + task["output"][i]\n                           for i in range(self.args.best_of_N)\n                           for task in tasks]\n        input_references = [task["reference"] for task in tasks]\n        inputs = self.tokenizer(input_sentences+input_references, return_tensors="pt",\n                                padding="longest", truncation=True, max_length=512)\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n        # for visualization of reward along length\n        logits_along_length = []\n        min_length, max_length, stride_length = 8, 112, 8\n        for length in range(min_length, max_length+1, stride_length):\n            with torch.no_grad():\n                outputs = self.model(**{k:v[:,:length] for k,v in inputs.items()})\n                logits = outputs.logits[:, 1] - outputs.logits[:, 0]\n                logits_along_length.append(logits)\n        logits_along_length = torch.stack(logits_along_length, dim=1)\n        for i,task in enumerate(tasks):\n            task["logits_along_length"] = logits_along_length[i, :].tolist()\n            task["logits_along_length_reference"] = logits_along_length[i-len(tasks), :].tolist()\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        assert outputs.logits.shape[0] == len(tasks) * (1 + self.args.best_of_N)\n        logits = outputs.logits[:, 1] - outputs.logits[:, 0]\n        sentences_logits = logits[:len(tasks)*self.args.best_of_N].view(self.args.best_of_N, len(tasks))\n        most_positive_of_N_logits = sentences_logits.max(dim=0).values\n        most_negative_of_N_logits = (-sentences_logits).max(dim=0).values\n        most_neutral_of_N_logits = (-torch.abs(sentences_logits)).max(dim=0).values\n        references_logits = logits[-len(tasks):]\n        for i,task in enumerate(tasks):\n            task["logits"] = sentences_logits[0, i].item()\n            task["more_positive"] = (most_positive_of_N_logits[i] > references_logits[i]).item()\n            task["more_negative"] = (most_negative_of_N_logits[i] > -references_logits[i]).item()\n            task["more_neutral"] = (most_neutral_of_N_logits[i] > -torch.abs(references_logits[i])).item()\n        return tasks\n\nclass GenerateProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        from imdb_one2many_dataset import IMDBDatasetForInference\n        test_dataset = IMDBDatasetForInference(usage="test")\n        tasks = [\n            {"input_ids":inputs["input_ids"].tolist(),\n             "reference": inputs["reference"]}\n            for inputs in test_dataset\n        ]\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None and all([(key in result and task[key]==result[key])\n                                           for key in ["input_ids", "reference"]])\n\nclass GenerateConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        tokenizer = get_tokenizer()\n        if self.args.vae_model_path is not None:\n            model = GPT2ForVAE.from_pretrained(self.args.vae_model_path)\n            model.config.eos_token_id = tokenizer.eos_token_id\n            model.config.pad_token_id = tokenizer.eos_token_id\n            if self.args.lpo_model_path is not None:\n                model.load_lpo_policy_latent_encoder(self.args.lpo_model_path)\n            model = model.half().to(self.device).eval()\n        elif self.args.dpo_model_path is not None:\n            model = DPOModel.from_pretrained(self.args.dpo_model_path, device_map=self.device)\n            model = model.base\n        elif self.args.lora_model_path is not None:\n            from peft import AutoPeftModelForCausalLM\n            model = AutoPeftModelForCausalLM.from_pretrained(self.args.lora_model_path)\n            model = model.eval().half().to(self.device)\n        elif self.args.pt_model_path is not None:\n            model = PTModel.from_pretrained(self.args.pt_model_path, device_map=self.device)\n            model = model.half().to(self.device).eval()\n        elif self.args.ppo_model_path is not None:\n            model = GPT2LMHeadModel(GPT2Config.from_pretrained(BASE_MODEL_PATH))\n            sd = load_checkpoint(self.args.ppo_model_path, device=self.device)\n            sd = {k[len("base_model."):]: v for k, v in sd.items() if k.startswith("base_model.")}\n            model.load_state_dict(sd)\n            model = model.half().to(self.device).eval()\n        else:\n            model = GPT2LMHeadModel.from_pretrained(\n                self.args.sft_model_path,\n                device_map=self.device,\n                torch_dtype=torch.float16,\n            ).eval()\n        self.tokenizer = tokenizer\n        self.model = model\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        inputs_len = max([len(task["input_ids"][0]) for task in tasks])\n        input_ids = pad_and_concat(\n            [torch.LongTensor(task["input_ids"]) for task in tasks],\n            pad_token_id=self.tokenizer.pad_token_id,\n            padding_side="left"\n        ).to(self.device)\n        assert input_ids.shape[1] == inputs_len\n        for task in tasks:\n            task[\'output\'] = []\n            task[\'prompt\'] = []\n        for i in range(self.args.best_of_N):\n            with torch.no_grad():\n                output_ids = self.model.generate(\n                    input_ids=input_ids,\n                    do_sample=True,\n                    top_k=50,\n                    min_length=input_max_length + output_max_length,\n                    max_new_tokens=output_max_length,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                )\n            outputs = self.tokenizer.batch_decode(output_ids[:,inputs_len:], skip_special_tokens=True)\n            prompts = self.tokenizer.batch_decode(output_ids[:,:inputs_len], skip_special_tokens=True)\n            for task,output,prompt in zip(tasks,outputs,prompts):\n                task[\'output\'].append(output)\n                task[\'prompt\'].append(prompt)\n        return tasks\n\ndef visualize_reward_and_ppl(input_file):\n    import matplotlib.pyplot as plt\n    output_dir = os.path.dirname(input_file)\n    with open(input_file, "r", encoding="utf-8") as f:\n        results = json.loads(f.read())\n    # global reward distribution\n    def visualize_values(values, x_min, x_max, x_num, window_size, name):\n        x = np.linspace(x_min, x_max, x_num)\n        x_left, x_right = x - window_size, x + window_size\n        x_left = np.where(x_left<x_min, x_min, x_left)\n        x_right = np.where(x_right>x_max, x_max, x_right)\n        y = ((x_left[:,None]<values[None,:]) & (values[None,:]<x_right[:,None])).sum(axis=-1) / (x_right-x_left)\n        records_file = os.path.join(output_dir,f"{name}_density.json")\n        with open(records_file, "w", encoding="utf-8") as f:\n            f.write(json.dumps({"x":x.tolist(), "y":y.tolist()}))\n        plot_png_file = os.path.join(output_dir,f"{name}_density.png")\n        plt.plot(x, y)\n        plt.xlim(x_min, x_max)\n        plt.ylim(bottom=0.0)\n        plt.savefig(plot_png_file)\n        plt.clf()\n    logits = np.array([result["logits"] for result in results])\n    visualize_values(logits, x_min=-6.0, x_max=6.0, x_num=100, window_size=0.3, name="reward_logits")\n    probs = 1 / (1 + np.exp(-logits))\n    visualize_values(probs, x_min=0.0, x_max=1.0, x_num=100, window_size=0.01, name="positive_probs")\n    print(f"positive expectation: {probs.mean()*100:.2f}%")\n    print(f"positive standard deviation: {probs.std()*100:.2f}%")\n    print(f"positive case proportion: {(logits>0).mean()*100:.2f}%")\n    for preference in ["positive", "negative", "neutral"]:\n        print(f"{preference} win-rate: {np.mean([result[\'more_\'+preference] for result in results])*100:.2f}%")\n\n    # reward along lengths\n    logits_along_length = np.array([result["logits_along_length"]\n                                    for result in results])\n    logits_along_length_reference = np.array([result["logits_along_length_reference"]\n                                              for result in results])\n    min_length, max_length, stride_length = 8, 112, 8\n    lengths = list(range(min_length, max_length + 1, stride_length))\n    p_bottom, p_top = 25, 75\n    plt.plot(lengths, logits_along_length_reference.mean(axis=0), color="blue")\n    plt.fill_between(lengths,\n                     np.percentile(logits_along_length_reference, p_bottom, axis=0),\n                     np.percentile(logits_along_length_reference, p_top, axis=0),\n                     color=\'blue\', alpha=0.1)\n    plt.plot(lengths, logits_along_length.mean(axis=0), color="red")\n    plt.fill_between(lengths,\n                     np.percentile(logits_along_length, p_bottom, axis=0),\n                     np.percentile(logits_along_length, p_top, axis=0),\n                     color=\'red\', alpha=0.1)\n    plt.xlim(min_length, max_length)\n    plt.ylim(-6.0, 6.0)\n    plt.savefig(os.path.join(output_dir, f"logits_along_length.png"))\n    plt.clf()\n\n    total_nll = sum([result["nll"] for result in results])\n    total_num_tokens = sum([result["num_tokens"] for result in results])\n    ppl = np.exp(total_nll / total_num_tokens)\n    print(f"ppl: {ppl:.2f}")\n    return probs.mean(), ppl\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce generation and reward on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="generate")\n    parser.add_argument("--output_file", type=str, default=None)\n    parser.add_argument("--erase", type=int, default=0)\n\n    # checkpoint path of generation model to test\n    from config_and_utils import auto_find_checkpoint_dir\n    parser.add_argument(\'--sft_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--pt_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--dpo_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--vae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--cvae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--lpo_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--ppo_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--lora_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument("--batch_size", type=int, default=16)\n    parser.add_argument(\'--best_of_N\', type=int, default=1)\n    parser.add_argument(\'--num_run_times\', type=int, default=1)\n\n    # the input file of generation to reward\n    parser.add_argument("--input_file", type=str, default=None)\n\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    if args.local_rank == -1:\n        if args.pt_model_path is not None:\n            args.output_file = os.path.join(args.pt_model_path, "inference.json")\n        elif args.dpo_model_path is not None:\n            args.output_file = os.path.join(args.dpo_model_path, "inference.json")\n        elif args.cvae_model_path is not None:\n            if args.lpo_model_path is not None:\n                args.output_file = os.path.join(args.lpo_model_path, "inference.json")\n            else:\n                args.output_file = os.path.join(args.cvae_model_path, "inference.json")\n        elif args.vae_model_path is not None:\n            if args.lpo_model_path is not None:\n                args.output_file = os.path.join(args.lpo_model_path, "inference.json")\n            else:\n                args.output_file = os.path.join(args.vae_model_path, "inference.json")\n        elif args.sft_model_path is not None:\n            args.output_file = os.path.join(args.sft_model_path, "inference.json")\n        elif args.ppo_model_path is not None:\n            args.output_file = os.path.join(args.ppo_model_path, "inference.json")\n        elif args.lora_model_path is not None:\n            args.output_file = os.path.join(args.lora_model_path, "inference.json")\n        else:\n            raise NotImplementedError(f"at least one kind of model path should be specified")\n\n    return args\n\ndef main():\n    args = parse_args()\n    args.erase = int(args.erase or args.num_run_times > 1)\n\n    if args.local_rank == -1:\n        positive_scores, ppl_scores = [], []\n        input_file, output_file = args.input_file, args.output_file\n        for i in range(args.num_run_times):\n            print(f"run - {i+1}/{args.num_run_times}")\n            # step1. generate output\n            args.task_name = "generate"\n            args.input_file, args.output_file = input_file, output_file\n            producer = GenerateProducer(args)\n            producer.run()\n            # step2. reward output\n            args.task_name = "reward"\n            args.input_file = args.output_file\n            args.output_file = args.output_file.replace("inference.json", "inference_and_reward.json")\n            producer = RewardProducer(args)\n            producer.run()\n            # step3. ppl\n            args.task_name = "ppl"\n            args.input_file = args.output_file\n            args.output_file = args.output_file.replace("inference_and_reward.json", "inference_and_reward_and_ppl.json")\n            producer = PPLProducer(args)\n            producer.run()\n            # step4. visualize output reward and ppl\n            positive_score, ppl_score = visualize_reward_and_ppl(input_file=args.output_file)\n            positive_scores.append(positive_score)\n            ppl_scores.append(ppl_score)\n        print(f"positive score: {np.mean(positive_scores):.3f}{np.std(positive_scores):.3f}")\n        print(f"ppl score: {np.mean(ppl_scores):.2f}{np.std(ppl_scores):.2f}")\n    else:\n        if args.task_name == "generate":\n            consumer = GenerateConsumer(args)\n            consumer.run()\n        elif args.task_name == "reward":\n            consumer = RewardConsumer(args)\n            consumer.run()\n        elif args.task_name == "ppl":\n            consumer = PPLConsumer(args)\n            consumer.run()\n        else:\n            raise NotImplementedError(args.task_name)\n\nif __name__ == "__main__":\n    main()\n', './LPO-sentiment\\train_dpo.py': 'import json\nimport os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\nfrom transformers import GPT2LMHeadModel\n\nfrom config_and_utils import get_tokenizer, get_trainer, auto_find_checkpoint_dir\nfrom modeling_gpt2_cvae import DPOModel, PTModel\nfrom imdb_comparison_dataset import (\n    IMDBComparisonDataset,\n    available_preferences,\n)\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n\ndef compute_metrics(eval_preds):\n    acc = eval_preds.predictions\n    return {"acc": acc.mean()}\n\n\ntokenizer = get_tokenizer()\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--sft_model_path\', type=auto_find_checkpoint_dir,\n                        default=auto_find_checkpoint_dir(f"sft/checkpoint"))\n    parser.add_argument(\'--pt_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument("--preference", type=str, default="positive",\n                        choices=available_preferences)\n    parser.add_argument(\'--beta\', type=float, default=0.1)\n    parser.add_argument(\'--lora_dim\', type=int, default=0)\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-6)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=2)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    if args.output_dir is None:\n        if args.pt_model_path is not None:\n            args.output_dir = os.path.join(args.pt_model_path,\n                                           f"dpo_{args.preference}_preference_beta_{args.beta}")\n        else:\n            if args.lora_dim:\n                args.output_dir = os.path.join(args.sft_model_path,\n                                               f"lora-{args.lora_dim}_dpo_{args.preference}_preference_beta_{args.beta}")\n            else:\n                args.output_dir = os.path.join(args.sft_model_path,\n                                               f"dpo_{args.preference}_preference_beta_{args.beta}")\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.mkdir(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    if args.local_rank == 0:\n        print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n        with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n            f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n\n    set_seed(args.seed)\n\n    train_dataset = IMDBComparisonDataset(usage="train", preference=args.preference)\n    valid_dataset = IMDBComparisonDataset(usage="validation", preference=args.preference)\n\n    device = f"cuda:{args.local_rank}"\n    if args.pt_model_path is not None:\n        model = PTModel.from_pretrained(args.pt_model_path, device_map=device)\n        model.base.config.update({\n            "attn_pdrop": 0.0, "embd_pdrop": 0.0, "resid_pdrop": 0.0,\n        })\n        model.switch_into_dpo_mode()\n        model.config.beta = args.beta\n        model.beta = args.beta\n        if args.learning_rate == 1e-6:\n            args.learning_rate = 1e-4\n    else:\n        pretrained_model = GPT2LMHeadModel.from_pretrained(args.sft_model_path, device_map=device)\n        config = pretrained_model.config\n        config.update({\n            "attn_pdrop": 0.0, "embd_pdrop": 0.0, "resid_pdrop": 0.0,\n            "beta": args.beta, "lora_dim": args.lora_dim\n        })\n        model = DPOModel(config).to(device)\n        model.load_pretrained(pretrained_model=pretrained_model)\n        model.prepare_refer_model()\n\n        if args.lora_dim:\n            from peft import get_peft_model, LoraConfig, TaskType\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                inference_mode=False,\n                r=config.lora_dim, lora_alpha=config.lora_dim, lora_dropout=0.0\n            )\n            model.base = get_peft_model(model.base, peft_config)\n            if args.local_rank == 0:\n                model.base.print_trainable_parameters()\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.dpo_collate_fn,\n        compute_metrics=compute_metrics\n    )\n    trainer.train()\n    trainer.save_state()\n\n    if args.lora_dim and args.local_rank == 0:\n        model.base.save_pretrained(os.path.join(args.output_dir, "best_model"))', './LPO-sentiment\\train_lpo.py': 'import os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\n\nfrom config_and_utils import get_tokenizer, get_trainer, auto_find_checkpoint_dir\nfrom modeling_gpt2_cvae import LPOModel\nfrom imdb_comparison_dataset import (\n    IMDBComparisonDataset,\n    available_preferences\n)\n\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n\ndef compute_metrics(eval_preds):\n    acc = eval_preds.predictions\n    return {"acc": acc.mean()}\n\n\ntokenizer = get_tokenizer()\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--vae_model_path\', type=auto_find_checkpoint_dir, required=True)\n    parser.add_argument("--preference", type=str, default="positive",\n                        choices=available_preferences)\n    parser.add_argument(\'--beta\', type=float, default=0.1)\n    parser.add_argument(\'--lpo_sampling_times\', type=int, default=16)\n    parser.add_argument(\'--lpo_post_sampling\', type=int, default=1)\n    parser.add_argument(\'--lpo_evidence_ratio\', type=float, default=0.5)\n    parser.add_argument(\'--expectation_lpo\', type=int, default=0)\n    parser.add_argument(\'--marginal_lpo\', type=int, default=0)\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-4)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=8)\n\n    parser.add_argument(\'--output_dir\', type=auto_find_checkpoint_dir, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    lpo_kwargs = {\n        key:getattr(args, key)\n        for key in [\n            "beta", "lpo_sampling_times", "lpo_post_sampling", "lpo_evidence_ratio",\n            "marginal_lpo", "expectation_lpo"\n        ]\n    }\n    model = LPOModel.from_vae(args.vae_model_path, **lpo_kwargs)\n\n    if args.output_dir is None:\n        args.output_dir = os.path.join(args.vae_model_path, f"lpo_{args.preference}_preference")\n        if args.expectation_lpo:\n            args.output_dir = args.output_dir.replace("lpo_", "exp_lpo_")\n            args.output_dir += f"_policy_sampling_{args.lpo_sampling_times}"\n        else:\n            if args.lpo_post_sampling:\n                args.output_dir += f"_post_sampling_{args.lpo_sampling_times}"\n            else:\n                args.output_dir += f"_sampling_{args.lpo_sampling_times}"\n            args.output_dir += f"_evidence_{str(args.lpo_evidence_ratio).replace(\'.\',\'\')}"\n        if args.marginal_lpo:\n            args.output_dir += f"_mrg"\n        if args.beta != 0.1:\n            args.output_dir += f"_beta_{args.beta}"\n        if args.learning_rate != 1e-4:\n            args.output_dir += f"_lr_{args.learning_rate:.0e}"\n        if args.epochs != 1:\n            args.output_dir += f"_epochs_{args.epochs}"\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.mkdir(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    set_seed(args.seed)\n\n    train_dataset = IMDBComparisonDataset(usage="train", preference=args.preference)\n    valid_dataset = IMDBComparisonDataset(usage="validation", preference=args.preference)\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.lpo_collate_fn,\n        compute_metrics=compute_metrics,\n        label_names=["prompt_ids"],\n        metric_for_best_model="eval_loss",\n    )\n    trainer.train()\n    trainer.save_state()\n', './LPO-sentiment\\train_sft.py': 'import json\nimport os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\n\nfrom config_and_utils import get_tokenizer, get_trainer, BASE_MODEL_PATH, auto_find_checkpoint_dir\nfrom imdb_one2many_dataset import IMDBOne2ManyDataset\nfrom modeling_gpt2_cvae import GPT2LMHeadModel, PTModel, GPT2CVAEConfig\nfrom imdb_dataset import IMDBDataset\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\ntokenizer = get_tokenizer()\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--base_model_path\', type=auto_find_checkpoint_dir, default=BASE_MODEL_PATH)\n    parser.add_argument(\'--p_tuning\', action="store_true")\n    parser.add_argument(\'--p_tuning_on_generation\', action="store_true")\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=10)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-5)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=8)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    if args.output_dir is None:\n        if args.p_tuning:\n            args.output_dir = f"pt-sft"\n        if args.p_tuning_on_generation:\n            args.output_dir = f"pt-sft-on-gen"\n        else:\n            args.output_dir = f"sft"\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.makedirs(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    if args.local_rank == 0:\n        print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n        with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n            f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n\n    set_seed(args.seed)\n\n    if args.p_tuning:\n        train_dataset = IMDBDataset(usage="train")\n        valid_dataset = IMDBDataset(usage="validation")\n        sft_model = GPT2LMHeadModel.from_pretrained(args.base_model_path,\n                                                    device_map=f"cuda:{args.local_rank}")\n        config = GPT2CVAEConfig(\n            **vars(args),\n            **vars(sft_model.config)\n        )\n        config.update({\n            "attn_pdrop": 0.0, "embd_pdrop": 0.0, "resid_pdrop": 0.0,\n        })\n        model = PTModel(config)\n        model.load_pretrained(sft_model)\n    elif args.p_tuning_on_generation:\n        train_dataset = IMDBOne2ManyDataset(usage="train")\n        valid_dataset = IMDBOne2ManyDataset(usage="validation")\n        sft_model = GPT2LMHeadModel.from_pretrained(args.base_model_path,\n                                                    device_map=f"cuda:{args.local_rank}")\n        config = GPT2CVAEConfig(\n            **vars(args),\n            **vars(sft_model.config)\n        )\n        config.update({\n            "attn_pdrop": 0.0, "embd_pdrop": 0.0, "resid_pdrop": 0.0,\n        })\n        model = PTModel(config)\n        model.load_pretrained(sft_model)\n    else:\n        train_dataset = IMDBDataset(usage="train")\n        valid_dataset = IMDBDataset(usage="validation")\n        model = GPT2LMHeadModel.from_pretrained(args.base_model_path)\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.sft_collate_fn\n    )\n    trainer.train()\n    trainer.save_state()\n', './LPO-sentiment\\train_vae.py': 'import os\nimport json\nimport torch\n\nimport random\nimport argparse\nimport numpy as np\n\nfrom modeling_gpt2_cvae import GPT2LMHeadModel, GPT2ForVAE, GPT2CVAEConfig\nfrom imdb_one2many_dataset import IMDBOne2ManyDataset\nfrom config_and_utils import get_tokenizer, get_trainer, auto_find_checkpoint_dir, BASE_MODEL_PATH\n\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n\ndef compute_metrics(eval_preds):\n    loss_lm, loss_kld, loss_contra = eval_preds.predictions\n\n    result = {\n        "loss_lm": loss_lm.mean(),\n        "loss_kld": loss_kld.mean(),\n        "loss_contra": loss_contra.mean(),\n    }\n\n    return result\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n\n    parser.add_argument(\'--vae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--base_model_path\', type=auto_find_checkpoint_dir, default=BASE_MODEL_PATH)\n\n    parser.add_argument(\'--num_q\', type=int, default=4)\n    parser.add_argument(\'--dim_z\', type=int, default=32)\n    parser.add_argument(\'--num_p\', type=int, default=4)\n    parser.add_argument(\'--latent_aggregator_layers\', type=int, default=2)\n    parser.add_argument(\'--post_with_x\', type=int, default=1)\n    parser.add_argument(\'--many\', type=int, default=32)\n\n    parser.add_argument(\'--frozen_pretrained\', type=int, default=0)\n    parser.add_argument(\'--marginal_kl\', type=int, default=1)\n    parser.add_argument(\'--lm_sampling_times\', type=int, default=1)\n    parser.add_argument(\'--kl_sampling_times\', type=int, default=16)\n    parser.add_argument(\'--without_dg_kld\', type=int, default=0)\n    parser.add_argument(\'--add_skip_connection\', type=int, default=0)\n    parser.add_argument(\'--add_contra_loss\', type=int, default=0)\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-4)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=8)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=1)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    set_seed(args.seed)\n\n    train_dataset = IMDBOne2ManyDataset(usage="train", many=args.many)\n    valid_dataset = IMDBOne2ManyDataset(usage="validation", many=args.many)\n\n    if args.vae_model_path is None:\n        # construct with base model\n        sft_model = GPT2LMHeadModel.from_pretrained(args.base_model_path,\n                                                    device_map=f"cuda:{args.local_rank}")\n        config = GPT2CVAEConfig(\n            **vars(args),\n            **vars(sft_model.config)\n        )\n        config.update({\n            "attn_pdrop": 0.0, "embd_pdrop": 0.0, "resid_pdrop": 0.0,\n        })\n        model = GPT2ForVAE(config)\n        model.load_pretrained(sft_model)\n    else:\n        model = GPT2ForVAE.from_pretrained(args.vae_model_path)\n        model.config.update({\n            key: getattr(args, key) for key in [\n                "frozen_pretrained", "marginal_kl", "without_dg_kld",\n                "add_skip_connection", "add_contra_loss"\n            ]\n        })\n        model.update_requires_grad_()\n\n    if args.output_dir is None:\n        args.output_dir = f"gpt2_vae" if args.lm_sampling_times > 0 else f"gpt2_ae"\n        args.output_dir += f"_{args.latent_aggregator_layers}-layers_"\n        if (args.num_q, args.dim_z, args.num_p) != (4, 32, 4):\n            args.output_dir += f"_{args.num_q}q_{args.dim_z}z_{args.num_p}p"\n        if args.frozen_pretrained:\n            args.output_dir += f"_frozen_base"\n        if args.many != 32:\n            args.output_dir += f"_1-to-{args.many}"\n        if args.with_bn:\n            args.output_dir += f"_with_bn"\n        if args.post_with_x:\n            args.output_dir += f"_post_with_x"\n        if args.add_skip_residue_contra:\n            args.output_dir += f"_skip_residue_based_contra"\n        else:\n            if args.add_skip_connection:\n                args.output_dir += f"_skip"\n            if args.add_contra_loss:\n                args.output_dir += f"_contra"\n        if args.without_dg_kld:\n            args.output_dir += "_w.o.dg_kld"\n        if not args.learning_rate == 1e-4:\n            args.output_dir += f"_lr{args.learning_rate}"\n        if not args.epochs == 1:\n            args.output_dir += f"_{args.epochs}epochs"\n    tokenizer = get_tokenizer()\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.makedirs(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    if args.local_rank == 0:\n        print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n        with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n            f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.vae_collate_fn,\n        compute_metrics=compute_metrics,\n        label_names=[],\n        metric_for_best_model="loss",\n        greater_is_better=False,\n    )\n    trainer.train()\n    trainer.save_state()\n\n', './LPO-sentiment\\scripts\\exp_dpo.sh': 'beta=0.1\nfor preference in positive negative neutral\ndo\n  deepspeed train_dpo.py --preference ${preference}\n  python test_generate_and_reward_and_ppl.py --dpo_model_path sft/checkpoint/dpo_${preference}_preference_beta_${beta}/checkpoint\ndone', './LPO-sentiment\\scripts\\exp_lora_dpo.sh': 'beta=0.1\nlora_dim=8\nfor preference in positive negative neutral\ndo\n  deepspeed train_dpo.py --preference ${preference} --lora_dim ${lora_dim} --learning_rate 1e-5\n  python test_generate_and_reward_and_ppl.py --lora_model_path sft/checkpoint/lora-${lora_dim}_dpo_${preference}_preference_beta_${beta}/best_model\ndone', './LPO-sentiment\\scripts\\exp_lpo.sh': 'deepspeed train_vae.py --frozen_pretrained 1 --add_skip_connection 1 --output_dir vae_frozen\n\ndeepspeed train_vae.py --frozen_pretrained 0 --add_contra_loss 1 --vae_model_path vae_frozen/checkpoint --output_dir vae_unfrozen\n\nfor preference in positive negative neutral\ndo\n  deepspeed train_lpo.py --preference ${preference} --vae_model_path vae_unfrozen/checkpoint --output_dir vae_unfrozen/lpo_${preference}\n  python test_generate_and_reward_and_ppl.py --vae_model_path vae_unfrozen/checkpoint --lpo_model_path vae_unfrozen/lpo_${preference}/checkpoint\ndone', './LPO-sentiment\\scripts\\exp_lpo_frozen.sh': 'deepspeed train_vae.py --frozen_pretrained 1 --add_skip_connection 1 --output_dir vae_frozen\n\nfor preference in positive negative neutral\ndo\n  for seed in 333 666 999\n  do\n    deepspeed train_lpo.py --preference ${preference} --seed ${seed} --vae_model_path vae_frozen/checkpoint --output_dir vae_frozen/lpo_${preference}_seed_${seed}\n    python test_generate_and_reward_and_ppl.py --vae_model_path vae_frozen/checkpoint --lpo_model_path vae_frozen/lpo_${preference}_seed_${seed}/checkpoint\n  done\ndone', './LPO-sentiment\\scripts\\exp_pt_dpo.sh': '# train the P-Tuning SFT model\ndeepspeed train_sft.py --p_tuning --base_model_path sft/checkpoint --learning_rate 1e-3\npython test_generate_and_reward_and_ppl.py --pt_model_path pt-sft/checkpoint\n# ppl:  30.86\n\n# train the P-Tuning DPO model\nbeta=0.5\n# beta  ppl\n# 0.1   43.25\n# 0.5   32.88\nfor preference in positive negative neutral\ndo\n  deepspeed train_dpo.py --beta ${beta} --preference ${preference} --pt_model_path pt-sft/checkpoint\n  python test_generate_and_reward_and_ppl.py --pt_model_path pt-sft/checkpoint/dpo_${preference}_preference_beta_${beta}/checkpoint\ndone', './LPO-sentiment\\scripts\\exp_pt_dpo_on_gen.sh': '# train the P-Tuning SFT model\ndeepspeed train_sft.py --p_tuning_on_generation --base_model_path sft/checkpoint --mini_batch_size 1 --global_batch_size 8 --epochs 1 --learning_rate 1e-3\npython test_generate_and_reward_and_ppl.py --pt_model_path pt-sft-on-gen/checkpoint\n# ppl:  30.86\n\n# train the P-Tuning DPO model\nbeta=0.5\n# beta  ppl\n# 0.1   43.25\n# 0.5   32.88\nfor preference in positive negative neutral\ndo\n  deepspeed train_dpo.py --beta ${beta} --preference ${preference} --pt_model_path pt-sft-on-gen/checkpoint\n  python test_generate_and_reward_and_ppl.py --pt_model_path pt-sft-on-gen/checkpoint/dpo_${preference}_preference_beta_${beta}/checkpoint\ndone', './LPO-sentiment\\scripts\\prepare_sft_and_data.sh': '# train the SFT model\ndeepspeed train_sft.py\npython test_generate_and_reward_and_ppl.py --sft_model_path sft/checkpoint\n\n# one2many generation from the SFT model\npython imdb_one2many_dataset.py\n\n# annotate the sentiment scores in one2many generation\npython imdb_comparison_dataset.py', './LPO-summary\\config_and_utils.py': 'import os\nimport re\n\noffline_mode = True\n# this mode requires datasets and base models are pre-downloaded and put into this project\n# our experiments are all carried in this mode since our machines are not connected to Internet\n#   offline_mode = False\n# this mode has not been tested yet\n\nif offline_mode:\n    BASE_MODEL_PATH = "./openai_summarize_tldr_sft"\n    RWD_MODEL_PATH = "./openai_summarize_tldr_rwd"\nelse:\n    BASE_MODEL_PATH = "CarperAI/openai_summarize_tldr_sft"\n    RWD_MODEL_PATH = "CarperAI/openai_summarize_tldr_rwd"\n\ninput_max_length = 500\noutput_max_length = 50\n\ndef get_tokenizer():\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n    return tokenizer\n\ndef get_sft_dataset():\n    from datasets import load_dataset, load_from_disk\n    if offline_mode:\n        dataset = load_from_disk("./openai_summarize_tldr")\n    else:\n        dataset = load_dataset("CarperAI/openai_summarize_tldr")\n    return dataset\n\ndef get_comparison_dataset():\n    from datasets import load_dataset, load_from_disk\n    if offline_mode:\n        dataset = load_from_disk("./openai_summarize_comparisons")\n    else:\n        dataset = load_dataset("CarperAI/openai_summarize_comparisons")\n    return dataset\n\ndef get_trainer(args, model, tokenizer, train_dataset, valid_dataset, collate_fn,\n                early_eval=False, **kwargs):\n    from transformers import Trainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n    import inspect\n    gradient_accumulation_steps = args.global_batch_size // (args.n_devices * args.mini_batch_size)\n    total_training_steps = args.epochs * len(train_dataset) // args.global_batch_size\n    warmup_steps = int(total_training_steps * 0.1)\n    eval_steps = int(total_training_steps * 0.1)\n    save_steps = eval_steps\n\n    deepspeed = None if args.no_deepspeed else {\n        "train_micro_batch_size_per_gpu": args.mini_batch_size,\n        "gradient_accumulation_steps": gradient_accumulation_steps,\n        "fp16": {\n            "enabled": True,\n            "min_loss_scale": 1,\n            "opt_level": "O2"\n        },\n        "zero_optimization": {\n            "stage": 2,\n            "offload_optimizer": {\n                "device": "cpu",\n                "pin_memory": True\n            },\n        },\n        "optimizer": {\n            "type": "AdamW",\n            "params": {\n                "lr": args.learning_rate,\n            }\n        },\n        "scheduler": {\n            "type": "WarmupLR",\n            "params": {\n                "warmup_min_lr": "auto",\n                "warmup_max_lr": "auto",\n                "warmup_num_steps": "auto",\n            }\n        }\n    }\n\n    training_args = TrainingArguments(\n        output_dir=args.output_dir,\n        evaluation_strategy="steps",\n        eval_accumulation_steps=1,\n        learning_rate=args.learning_rate,\n        per_device_train_batch_size=args.mini_batch_size,\n        per_device_eval_batch_size=args.mini_batch_size,\n        fp16=True,\n        adam_beta1=0.9,\n        adam_beta2=0.95,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        num_train_epochs=args.epochs,\n        warmup_steps=warmup_steps,\n        eval_steps=eval_steps,\n        save_steps=save_steps,\n        save_total_limit=1,\n        logging_steps=min(10, max(1, eval_steps // 10)),\n        deepspeed=deepspeed,\n        load_best_model_at_end=True,\n        **{k:v for k,v in kwargs.items() if k in inspect.signature(TrainingArguments.__init__).parameters},\n    )\n\n    class EarlyEvaluationCallback(TrainerCallback):\n        def on_step_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n            if state.global_step == 0:\n                control.should_evaluate = True\n            else:\n                control.should_training_stop = True\n\n    callbacks = []\n    if early_eval:\n        callbacks.append(EarlyEvaluationCallback())\n\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        data_collator=collate_fn,\n        callbacks=callbacks,\n        **{k:v for k,v in kwargs.items() if k in inspect.signature(Trainer.__init__).parameters}\n    )\n    return trainer\n\ndef auto_find_checkpoint_dir(path):\n    if path is None or path == "None":\n        return None\n    def find_checkpoint_dir(path):\n        probable_checkpoint_dirs = [dir for dir in os.listdir(path) if re.search(r"checkpoint-(\\d+)", dir)]\n        if len(probable_checkpoint_dirs) == 1:\n            return probable_checkpoint_dirs[0]\n        elif len(probable_checkpoint_dirs) > 1:\n            print(f"multiple checkpoint dir in {path}, choose the last one")\n            probable_checkpoint_dirs = sorted(probable_checkpoint_dirs, key=lambda dir:int(re.search(r"checkpoint-(\\d+)", dir).group(1)))\n            return probable_checkpoint_dirs[-1]\n        else:\n            raise Exception(f"no checkpoint under {path} is found")\n    path_parts = os.path.normpath(path).split("/")\n    for i, part in enumerate(path_parts):\n        if part == "checkpoint":\n            parent_path = "/".join(path_parts[:i])\n            path_parts[i] = find_checkpoint_dir(parent_path)\n    return "/".join(path_parts)\n\ndef load_checkpoint(path, device):\n    import torch\n    device = torch.device(device) if type(device) is str else device\n    if os.path.exists(os.path.join(path, "pytorch_model.bin")):\n        state_dict = torch.load(os.path.join(path, "pytorch_model.bin"), map_location=device)\n    elif os.path.exists(os.path.join(path, "model.safetensors")):\n        from safetensors.torch import load_file\n        state_dict = load_file(os.path.join(path, "model.safetensors"), device="cpu")\n        state_dict = {k:v.to(device) for k,v in state_dict.items()}\n    else:\n        raise ValueError(f"no checkpoint file found in {path}")\n    return state_dict\n\ndef pad_and_concat(list_tensors, pad_token_id, padding_side="right", dim=-1):\n    import torch\n    max_length = max([tensor.shape[dim] for tensor in list_tensors])\n    list_padded_tensors = []\n    for tensor in list_tensors:\n        tensor_shape = list(tensor.shape)\n        padding_shape = tensor_shape\n        padding_shape[dim] = max_length - tensor_shape[dim]\n        padding = torch.empty(size=padding_shape, dtype=tensor.dtype).fill_(pad_token_id)\n        if padding_side == "right":\n            list_padded_tensors.append(torch.cat([tensor, padding], dim=dim))\n        elif padding_side == "left":\n            list_padded_tensors.append(torch.cat([padding, tensor], dim=dim))\n        else:\n            raise NotImplementedError(padding_side)\n    concated_tensors = torch.cat(list_padded_tensors, dim=0)\n    return concated_tensors', './LPO-summary\\gpu_map_reduce.py': '"""\nThis Python file implements a Map-Reduce framework with a TaskProducer abstract class and a\nTaskConsumer abstract class. Each TaskConsumer instance independently utilizes a GPU to load\na Torch model for data processing, i.e., in data-parallel.\n\nClasses:\n- TaskProducer:\n    load tasks from file\n    load and check cached results from file\n    clear the present tasks and results in redis queue (may be left by killed programmes)\n    launch consumers and send tasks to them\n    recieve results from consumers and save them to file\n    stop consumers at the end\n    - abstractmethod:\n        load_tasks\n        cached_result_is_valid\n- TaskConsumer:\n    init model on corresponding gpu (specified by local_rank)\n    recieve tasks from producer\n    process tasks with model\n    send results to producer\n    - abstractmethod:\n        init_model\n        process_task\nFunctions:\n- parse_args (demo):\n    parse args for TaskProducer and TaskConsumer\n- main (demo):\n    the programme entrance\n\nNote:\nThis script requires redis server running in the background for Inter-Process Communication.\nYou can install redis server and run it through the following commands:\n# apt-get install redis-server\n# redis-server --port PORT\n"""\nimport os\nimport re\nimport sys\nimport json\nimport time\nimport socket\nimport argparse\nimport threading\nimport concurrent\n\nimport redis\nimport torch\nfrom tqdm import tqdm\nfrom abc import ABC, abstractmethod\n\ndef get_local_ip():\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    s.connect(("8.8.8.8", 80))\n    return s.getsockname()[0]\n\n\nclass TaskProducer(ABC):\n    def __init__(self, args):\n        self.args = args\n        self.producer = redis.Redis(host=args.host, port=args.port, db=0)\n        self.consumer = redis.Redis(host=args.host, port=args.port, db=1)\n        self.task_queue_name = f\'{self.args.task_name}_task\'\n        self.result_queue_name = f\'{self.args.task_name}_result\'\n        self.has_slept_since_last_save = False\n\n    def clear_queue(self):\n        num_tasks_cleared = 0\n        task = self.producer.lpop(self.task_queue_name)\n        while task is not None:\n            num_tasks_cleared += 1\n            task = self.producer.lpop(self.task_queue_name)\n\n        num_results_cleared = 0\n        result = self.consumer.lpop(self.result_queue_name)\n        while result is not None:\n            num_results_cleared += 1\n            result = self.consumer.lpop(self.result_queue_name)\n\n        print(f"clear {num_tasks_cleared} tasks and {num_results_cleared} results in queue")\n        return num_tasks_cleared, num_results_cleared\n\n    def auto_clear_queue(self):\n        stop_clearing = False\n        while not stop_clearing:\n            num_tasks_cleared, num_results_cleared = self.clear_queue()\n            def input_choice():\n                time.sleep(10)\n                return num_tasks_cleared == 0 and num_results_cleared == 0\n            executor = concurrent.futures.ThreadPoolExecutor()\n            future = executor.submit(input_choice)\n            try:\n                stop_clearing = future.result(timeout=30)\n            except concurrent.futures.TimeoutError:\n                stop_clearing = num_tasks_cleared == 0 and num_results_cleared == 0\n            finally:\n                executor.shutdown(wait=False)\n\n    def wait_for_next_result(self):\n        while True:\n            result = self.consumer.lpop(self.result_queue_name)\n            if result is not None:\n                result = json.loads(result.decode(\'utf-8\'))\n                return result\n            time.sleep(1)\n            self.has_slept_since_last_save = True\n\n    @abstractmethod\n    def load_tasks(self) -> list[dict]:\n        tasks = []\n        return tasks\n\n    def save_results(self):\n        num_valid_results = len([_ for _ in self.results if _ is not None])\n        print(f"save results ({num_valid_results}/{len(self.tasks)}) into {self.args.output_file}")\n        with open(self.args.output_file, "w", encoding="utf-8") as f:\n            f.write(json.dumps(self.results, ensure_ascii=False, indent=4))\n\n    def load_cached_results(self):\n        if os.path.exists(self.args.output_file) and not self.args.erase:\n            with open(self.args.output_file, "r", encoding="utf-8") as f:\n                results = json.loads(f.read())\n            if len(results) == len(self.tasks):\n                return results\n        results = [None for index in range(len(self.tasks))]\n        return results\n\n    @abstractmethod\n    def cached_result_is_valid(self, task, result) -> bool:\n        return True\n\n    def push_tasks(self):\n        num_cached_results, num_pushed_tasks = 0, 0\n        for index, task in enumerate(tqdm(self.tasks, desc=f"pushing tasks into {self.task_queue_name}...")):\n            if "index" not in task:\n                task["index"] = index\n            if self.results[index] is not None:\n                if self.cached_result_is_valid(task, self.results[index]):\n                    num_cached_results += 1\n                    continue\n                else:\n                    self.results[index] = None\n            self.producer.rpush(self.task_queue_name, json.dumps(task, ensure_ascii=False))\n            num_pushed_tasks += 1\n        print(f"read {num_cached_results} results from cache_file and push {num_pushed_tasks} tasks into {self.task_queue_name}")\n        return num_cached_results, num_pushed_tasks\n\n    def recieve_results(self):\n        first_result = self.wait_for_next_result()\n        for num_received_results in tqdm(range(self.num_pushed_tasks), desc=f"receiving results from {self.result_queue_name}..."):\n            if num_received_results == 0:\n                result = first_result\n            else:\n                result = self.wait_for_next_result()\n            index = result["index"]\n            while not self.cached_result_is_valid(self.tasks[index], result):\n                print(f"invalid result is received and ignored:\\n{json.dumps(result, ensure_ascii=False, indent=4)}")\n                result = self.wait_for_next_result()\n                index = result["index"]\n            self.results[index] = result\n            num_valid_results = self.num_cached_results + num_received_results + 1\n            if num_valid_results % max(len(self.tasks)//100, 100) == 0 and self.has_slept_since_last_save:\n                self.save_results()\n                self.has_slept_since_last_save = False\n\n    def run(self):\n        self.localrank = get_local_ip()\n        if self.args.host == "localhost" or self.args.host == self.localrank:\n            print(f"localrank match the host: {self.localrank} == {self.args.host}")\n            print(f"run the main procedure")\n            self.tasks = self.load_tasks()\n            self.results = self.load_cached_results()\n            self.auto_clear_queue()\n            self.num_cached_results, self.num_pushed_tasks = self.push_tasks()\n\n            if self.num_pushed_tasks > 0:\n                self.launch_consumers()\n                self.recieve_results()\n                self.save_results()\n                self.stop_consumers()\n        else:\n            print(f"localrank do not match the host: {self.localrank} != {self.args.host}")\n            print(f"just launch the consumers")\n            self.launch_consumers()\n\n\n    def launch_consumers(self):\n        def launch_consumers(args):\n            args_input = vars(args)\n            args_input = {k:v for k,v in args_input.items() if v is not None}\n            for local_rank in range(torch.cuda.device_count()):\n                args_input["local_rank"] = local_rank\n                argv = \' \'.join([f\'--{k} "{v}"\' for k, v in args_input.items()])\n                cmd = f\'nohup python {args.py_file_name} {argv}\' \\\n                      f\' > nohup.{args.py_file_name}.rank_{local_rank}.txt 2>&1 &\'\n                print(f"{cmd}")\n                os.system(cmd)\n        launch_consumers(self.args)\n\n    def stop_consumers(self):\n        for local_rank in range(torch.cuda.device_count()):\n            task = {"signal": "exit"}\n            self.producer.rpush(self.task_queue_name, json.dumps(task, ensure_ascii=False))\n\nclass TaskConsumer(ABC):\n    def __init__(self, args):\n        self.args = args\n        self.consumer = redis.Redis(host=args.host, port=args.port, db=0)\n        self.producer = redis.Redis(host=args.host, port=args.port, db=1)\n        self.task_queue_name = f\'{self.args.task_name}_task\'\n        self.result_queue_name = f\'{self.args.task_name}_result\'\n        self.device = torch.device(f"cuda:{args.local_rank}")\n        self.local_task_queue, self.task_queue_lock = [], threading.Lock()\n        self.local_result_queue, self.result_queue_lock = [], threading.Lock()\n        self.exit_signal = threading.Event()\n\n    @abstractmethod\n    def init_model(self) -> None:\n        pass\n\n    def listening(self):\n        num_tasks_per_sleep = 0\n        num_tasks_before_sleep = 0\n        while not self.exit_signal.is_set():\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            num_tasks_this_sleep = num_tasks_before_sleep - num_local_tasks\n            num_tasks_per_sleep = max(int(num_tasks_per_sleep * 0.8), num_tasks_this_sleep)\n            num_tasks_expected_in_queue = max(10, num_tasks_per_sleep * 2)\n            if num_local_tasks >= num_tasks_expected_in_queue:\n                num_tasks_before_sleep = num_local_tasks\n                time.sleep(1)\n                continue\n            for _ in range(num_tasks_expected_in_queue-num_local_tasks):\n                task = self.consumer.lpop(self.task_queue_name)\n                if task is None:\n                    break\n                task = json.loads(task.decode(\'utf-8\'))\n                if "signal" in task and task["signal"] == "exit":\n                    self.exit_signal.set()\n                    break\n                with self.task_queue_lock:\n                    self.local_task_queue.append(task)\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            num_tasks_before_sleep = num_local_tasks\n            time.sleep(1)\n\n    def sending(self):\n        while not self.exit_signal.is_set():\n            with self.result_queue_lock:\n                num_local_results = len(self.local_result_queue)\n            if num_local_results == 0:\n                time.sleep(1)\n                continue\n            with self.result_queue_lock:\n                result = self.local_result_queue.pop(0)\n            self.producer.rpush(self.result_queue_name, result)\n\n    def start_communication_threads(self):\n        self.threads = [\n            threading.Thread(target=self.listening),\n            threading.Thread(target=self.sending)\n        ]\n        for thread in self.threads:\n            thread.start()\n\n    def join_communication_threads(self):\n        for thread in self.threads:\n            thread.join()\n\n    @abstractmethod\n    def process_task(self, task: dict) -> dict:\n        result = task\n        return result\n\n    def processing_task_loop(self):\n        while not self.exit_signal.is_set():\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            if num_local_tasks == 0:\n                time.sleep(1)\n                continue\n            with self.task_queue_lock:\n                task = self.local_task_queue.pop(0)\n            result = self.process_task(task)\n            with self.result_queue_lock:\n                self.local_result_queue.append(json.dumps(result, ensure_ascii=False))\n\n    def run(self):\n        self.init_model()\n        self.start_communication_threads()\n        self.processing_task_loop()\n        self.join_communication_threads()\n\nclass BatchTaskConsumer(TaskConsumer):\n    def __init__(self, args):\n        super().__init__(args)\n        self.batch_size = getattr(args, "batch_size", 2)\n\n    def process_task(self, task: dict) -> dict:\n        pass\n\n    @abstractmethod\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        results = tasks\n        return results\n\n    def processing_task_loop(self):\n        while not self.exit_signal.is_set():\n            with self.task_queue_lock:\n                num_local_tasks = len(self.local_task_queue)\n            if num_local_tasks == 0:\n                time.sleep(1)\n                continue\n            with self.task_queue_lock:\n                tasks = self.local_task_queue[:self.batch_size]\n                self.local_task_queue = self.local_task_queue[self.batch_size:]\n            results = self.process_tasks(tasks)\n            with self.result_queue_lock:\n                for result in results:\n                    self.local_result_queue.append(json.dumps(result, ensure_ascii=False))\n\n\n# implementation examples\nclass MyProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        tasks = []\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return True\n\n\nclass MyConsumer(TaskConsumer):\n    def init_model(self) -> None:\n        pass\n\n    def process_task(self, task: dict) -> dict:\n        result = task\n        return result\n\n\nclass MyBatchConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        pass\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        results = tasks\n        return results\n\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce processing on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="inference")\n    parser.add_argument("--output_file", type=str, default=None)\n    parser.add_argument("--erase", type=int, default=0)\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    return args\n\n\ndef main():\n    args = parse_args()\n    if args.local_rank == -1:\n        producer = TaskProducer(args)\n        producer.run()\n    else:\n        consumer = TaskConsumer(args)\n        consumer.run()\n\n\nif __name__ == "__main__":\n    main()\n', './LPO-summary\\modeling_gptj_cvae.py': 'import os\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.models.gptj.modeling_gptj import *\n\nfrom config_and_utils import BASE_MODEL_PATH\n\n\ndef sampling(mean, logvar, n_samples=1):\n    mu = torch.stack([mean] * n_samples, dim=-1)\n    sigma = torch.stack([torch.exp(logvar * 0.5)] * n_samples, dim=-1)\n    eps = torch.zeros_like(sigma).normal_()\n    zs = eps * sigma + mu\n    return zs\n\n\ndef log_pdf(mean, logvar, zs):\n    import numpy as np\n    while len(zs.size()) > len(mean.size()):\n        mean = mean.unsqueeze(-1)\n        logvar = logvar.unsqueeze(-1)\n    return -0.5 * np.log(2 * np.pi) - 0.5 * logvar - \\\n        (zs - mean).pow(2) / (2 * torch.exp(logvar) + 1e-4)\n\n\ndef dg_kld(config, prior_mean, prior_logvar, post_mean, post_logvar):\n    batch_size, dim_z = prior_mean.shape\n\n    n_samples = config.kl_sampling_times\n    zs = sampling(post_mean, post_logvar, n_samples)\n\n    priors_mean = prior_mean.unsqueeze(-1).repeat(1, 1, n_samples)\n    priors_logvar = prior_logvar.unsqueeze(-1).repeat(1, 1, n_samples)\n    # [batch_size, dim_z, n_samples]\n    logp_priors_zs = log_pdf(priors_mean, priors_logvar, zs)\n    # e.g.\n    #         z1 z2 z3\n    # prior   .. .. ..\n    # [batch_size, dim_z, n_samples]\n\n    zs = zs.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\n    posts_mean = post_mean.unsqueeze(1).unsqueeze(-1).repeat(1, batch_size, 1, n_samples)\n    posts_logvar = post_logvar.unsqueeze(1).unsqueeze(-1).repeat(1, batch_size, 1, n_samples)\n    # [batch_size, batch_size, dim_z, n_samples]\n    # the first "batch_size" is of prior/post, and the second "batch_size" is of zs\n    # in another perspective, the first / second is of aggreagation / stratified sampling\n    logp_posts_zs = log_pdf(posts_mean, posts_logvar, zs)\n    # e.g.\n    #        z1 z2 z3\n    # post1  .. .. ..\n    # post2  .. .. ..\n    # post3  .. .. ..\n\n    if config.marginal_kl:\n        # regularization on each dimension respectively\n        logp_posts_zs = logp_posts_zs.view(batch_size, batch_size, dim_z, n_samples)\n        # [batch_size, batch_size, dim_z, n_samples]\n        logp_priors_zs = logp_priors_zs.view(batch_size, dim_z, n_samples)\n        # [batch_size, dim_z, n_samples]\n    else:\n        # regularization in the high-dimensional joint latent space\n        logp_posts_zs = logp_posts_zs.sum(dim=-2, keepdims=True)\n        # [batch_size, batch_size, 1, n_samples]\n        logp_priors_zs = logp_priors_zs.sum(dim=-2, keepdims=True)\n        # [batch_size, 1, n_samples]\n\n    # aggregation: post1(z), post2(z), post3(z) -> post_agg(z)\n\n    logp_posts_max = logp_posts_zs.max(dim=0).values\n    logp_agg_post_zs = (logp_posts_zs - logp_posts_max).exp().mean(dim=0).log() + logp_posts_max\n    # [batch_size, 1 or dim_z, n_samples]\n    # e.g. (the dim of post-agg is removed by mean with keepdims=False)\n    #           z1 z2 z3\n    # post-agg  .. .. ..\n\n    # mote carlo with stratified sampling: post_agg(z1), post_agg(z2), post_agg(z3) -> post_agg(z_agg)\n\n    # mote carlo with stratified sampling: prior(z1), prior(z2), prior(z3) -> prior(z_agg)\n    density_gaps = logp_agg_post_zs - logp_priors_zs\n    # [batch_size, 1 or dim_z, n_samples]\n    kl = density_gaps.mean(dim=0)\n    # [1 or dim_z, n_samples]\n\n    kl = kl.sum(dim=0).mean(dim=-1)\n\n    # []\n    return kl\n\n\nclass GPTJCVAEConfig(GPTJConfig):\n    def __init__(\n        self,\n        base_pretrained_path=BASE_MODEL_PATH,\n        num_q=4,\n        dim_z=32,\n        num_p=4,\n        latent_aggregator_layers=2,\n        frozen_pretrained=True,\n        use_standard_prior=True,\n        marginal_kl=True,\n        lm_sampling_times=1,\n        kl_sampling_times=16,\n        lpo_sampling_times=64,\n        without_contra=0,\n        without_dg_kld=0,\n        add_skip_connection=0,\n        add_contra_loss=0,\n        beta=0.1,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        # for model structure\n        self.base_pretrained_path = base_pretrained_path\n        self.num_q = num_q\n        self.dim_z = dim_z\n        self.num_p = num_p\n        self.latent_aggregator_layers = latent_aggregator_layers\n        # for training and inference\n        self.frozen_pretrained = frozen_pretrained\n        self.use_standard_prior = use_standard_prior\n        self.marginal_kl = marginal_kl\n        self.lm_sampling_times = lm_sampling_times\n        self.kl_sampling_times = kl_sampling_times\n        self.lpo_sampling_times = lpo_sampling_times\n        self.without_contra = without_contra\n        self.without_dg_kld = without_dg_kld\n        self.add_skip_connection = add_skip_connection\n        self.add_contra_loss = add_contra_loss\n        self.beta = beta\n        # others\n        self.pad_token_id = self.eos_token_id\n\n\nclass LatentAggregator(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.queries = nn.Parameter(torch.Tensor(1, config.num_q, config.n_embd))\n        self.queries.data.normal_(mean=0.0, std=self.config.initializer_range)\n        self.ln_input = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.blocks = nn.ModuleList([\n            GPTJBlock(config)\n            for _ in range(config.latent_aggregator_layers)\n        ])\n        self.ln_output = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.ln_output.bias.data.zero_()\n        self.ln_output.weight.data.fill_(1.0)\n        self.h2z = nn.Linear(config.n_embd, config.dim_z * 2 // config.num_q)\n\n    def standardize(self):\n        # regularize the output latent distribution to standard gaussian\n        self.h2z.weight.data.zero_()\n        self.h2z.bias.data.zero_()\n        return\n\n    def forward(\n        self,\n        input_embeds: torch.FloatTensor,\n        input_embeds_mask: torch.FloatTensor = None,\n        fix_entropy: bool = False,\n    ):\n        dtype, device = self.queries.dtype, self.queries.device\n        batch_size, seq_len, hidden_size = input_embeds.shape\n        if input_embeds_mask is not None:\n            input_embeds_mask = input_embeds_mask.to(dtype=dtype, device=device)  # fp16 compatibility\n        else:\n            input_embeds_mask = torch.ones([batch_size, seq_len], dtype=dtype, device=device)\n\n        self_attn_mask = torch.ones([batch_size, self.config.num_q], dtype=dtype, device=device)\n\n        attention_mask = torch.cat([input_embeds_mask, self_attn_mask], dim=1)  # [batch_size, to_seq_length]\n        attention_mask = attention_mask[:, None, None, :]  # [batch_size, num_heads, from_seq_length, to_seq_length]\n        attention_mask = (1.0 - attention_mask) * torch.finfo(dtype).min\n\n        hidden_states = torch.cat([input_embeds, self.queries.repeat(batch_size, 1, 1)], dim=1)\n\n        hidden_states = self.ln_input(hidden_states)\n        position_ids = torch.arange(seq_len + self.config.num_q, dtype=torch.long, device=device).unsqueeze(0)\n\n        for block in self.blocks:\n            hidden_states = block(\n                hidden_states,\n                position_ids=position_ids,\n                attention_mask=attention_mask,\n            )[0]\n        latent = self.h2z(self.ln_output(hidden_states[:, -self.config.num_q:, :]))\n        mean = latent[:, :, :self.config.dim_z // self.config.num_q].reshape(batch_size, self.config.dim_z)\n        logvar = latent[:, :, self.config.dim_z // self.config.num_q:].reshape(batch_size, self.config.dim_z)\n\n        if fix_entropy:\n            logvar = logvar - logvar.sum(dim=-1, keepdim=True) / self.config.dim_z\n            # enforce the entropy of prior distribution equal\n            # to that of standard gaussian distribution\n\n        return mean, logvar#, hidden_states\n\n\nclass LatentEncoder(nn.Module):\n    def __init__(self, config, fix_entropy=False):\n        super().__init__()\n        self.config = config\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.aggregator = LatentAggregator(config)\n        self.fix_entropy = fix_entropy\n        self.standardize()\n\n    def load_pretrained(self, pretrained_model):\n        self.wte.load_state_dict(pretrained_model.transformer.wte.state_dict())\n        for i in range(self.config.latent_aggregator_layers):\n            self.aggregator.blocks[i].load_state_dict(pretrained_model.transformer.h[i].state_dict())\n        self.aggregator.standardize()\n\n    def standardize(self):\n        self.aggregator.standardize()\n\n    def init_wte(self, wte):\n        self.wte.load_state_dict(wte.state_dict())\n\n    def init_wte_from_lm_head(self, lm_head):\n        self.wte.load_state_dict(lm_head.state_dict(), strict=False)\n\n    def roll_right_padding_to_left(self, right_padded_ids):\n        batch_size, seq_len = right_padded_ids.shape\n        last_valid_idx = (right_padded_ids != self.config.pad_token_id).sum(dim=1)\n        left_padded_ids = right_padded_ids.clone()\n        for i in range(batch_size):\n            left_padded_ids[i] = torch.roll(left_padded_ids[i], seq_len - last_valid_idx[i].item(), dims=0)\n        return left_padded_ids\n\n    def forward(self, input_ids):\n        input_ids = self.roll_right_padding_to_left(input_ids).long()\n        input_embeds = self.wte(input_ids)\n        input_embeds_mask = (input_ids != self.config.eos_token_id).float()\n        mean, logvar = self.aggregator(\n            input_embeds=input_embeds,\n            input_embeds_mask=input_embeds_mask,\n            fix_entropy=self.fix_entropy,\n        )\n        return mean, logvar\n\n\nclass LatentDecoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.z2hs = nn.ModuleList([nn.Sequential(\n            nn.Linear(config.dim_z, config.n_embd),\n            nn.GELU(),\n            nn.Linear(config.n_embd, config.n_embd * config.num_p)\n        ) for i in range(config.n_layer)])\n        #self.z2hs = nn.ModuleList([nn.Linear(config.dim_z, config.n_embd * config.num_p)\n        #                           for i in range(config.n_layer)])\n        self.lns = nn.ModuleList([nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n                                  for i in range(config.n_layer)])\n        self.attns = nn.ModuleList([GPTJAttention(config)\n                                    for i in range(config.n_layer)])\n\n    def load_pretrained(self, pretrained_model):\n        for i in range(self.config.n_layer):\n            self.attns[i].load_state_dict(pretrained_model.transformer.h[i].attn.state_dict())\n\n    def decode(self, zs):\n        # input_ps mimic past_key_values in gptj\n        # past_key_values: list (in length of n_layers) of past_key and past_value\n        # past_key and past_value: (batch_size, num_heads, seq_length, head_features)\n        batch_size = zs.shape[0]\n\n        past_key_values = []\n        for i in range(self.config.n_layer):\n            z2h, ln, attn = self.z2hs[i], self.lns[i], self.attns[i]\n            hidden_states = z2h(zs)\n            hidden_states = hidden_states.view(batch_size, self.config.num_p, self.config.n_embd)\n            hidden_states = ln(hidden_states)\n            position_ids = torch.arange(self.config.num_p).unsqueeze(0).repeat(batch_size, 1).to(zs.device)\n            present = attn(\n                hidden_states=hidden_states,\n                position_ids=position_ids,\n                use_cache=True\n            )[1]\n            past_key_values.append(present)\n\n        input_ps = past_key_values\n        return input_ps\n\n    def forward(self, mean, logvar):\n        # mean, logvar -> zs -> input_ps\n        zs = sampling(mean, logvar).squeeze(-1)\n        input_ps = self.decode(zs)\n        return input_ps\n\n\nclass GPTJForVAE(GPTJPreTrainedModel):\n    config_class = GPTJCVAEConfig\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config: GPTJCVAEConfig):\n        super().__init__(config)\n        self.config = config\n        self.latent_encoder = LatentEncoder(config)\n        self.latent_decoder = LatentDecoder(config)\n        self.base = GPTJForCausalLM(config)\n        self.skip_connection = nn.Sequential(\n            nn.Linear(self.config.dim_z, self.config.n_embd),\n            nn.GELU(),\n            nn.Linear(self.config.n_embd, self.config.n_embd)\n        )\n        self.update_requires_grad_()\n\n    def update_requires_grad_(self):\n        self.base.requires_grad_(not self.config.frozen_pretrained)\n        self.latent_decoder.attns.requires_grad_(not self.config.frozen_pretrained)\n\n    def load_pretrained(self):\n        pretrained_model = GPTJForCausalLM.from_pretrained(self.config.base_pretrained_path)\n        self.base.load_state_dict(pretrained_model.state_dict(), strict=False)\n        self.latent_encoder.load_pretrained(pretrained_model)\n        self.latent_decoder.load_pretrained(pretrained_model)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,  # [1, prompt_len]\n        prior_ids: torch.LongTensor,  # [1, input_max_length]\n        labels: torch.LongTensor,  # [num_mini, mini_many, output_max_length]\n        post_ids: torch.LongTensor,  # [num_mini, mini_many, output_max_length]\n        return_loss=True,\n        **kwargs,\n    ):\n        num_mini, mini_many, seq_len = labels.shape\n        total_many = mini_many * num_mini\n        post_mean, post_logvar = self.latent_encoder(post_ids.view(total_many, post_ids.shape[-1]))\n        prior_mean, prior_logvar = torch.zeros_like(post_mean), torch.zeros_like(post_logvar)\n\n        if self.config.without_dg_kld:\n            loss_kld = 0.5 * (post_mean.pow(2) + post_logvar.exp() - post_logvar - 1).sum(dim=1).mean(dim=0)\n        else:\n            loss_kld = dg_kld(self.config, prior_mean, prior_logvar, post_mean, post_logvar)\n\n        # only compute lm loss for the (currently) concerned output\n        post_mean, post_logvar = post_mean[:mini_many, :], post_logvar[:mini_many, :]\n        input_ps = self.latent_decoder(post_mean, post_logvar)\n        # n_layer * [mini_many, num_p, n_embd] <- [mini_many, dim_z]\n\n        prompt_ids = input_ids.repeat(mini_many, 1)\n        labels = torch.cat([prompt_ids[:,-1:],labels[0, :, :]], dim=1)\n        prompt_ids = prompt_ids[:,:-1]\n\n        prompted_prefix_outputs = self.base.transformer(\n            input_ids=prompt_ids,\n            past_key_values=input_ps,\n            use_cache=True,\n            return_dict=True\n        )\n\n        if self.config.add_contra_loss or self.config.add_skip_connection:\n            unbiased_last_hidden_state = None\n            in_batch_nlls = []\n            for bias in range(mini_many if self.config.add_contra_loss else 1):\n                biased_past_key_values = [\n                    (torch.roll(past_keys, shifts=bias, dims=0),\n                     torch.roll(past_values, shifts=bias, dims=0))\n                    for past_keys, past_values in prompted_prefix_outputs.past_key_values\n                ]\n                biased_last_hidden_state = self.base.transformer(\n                    input_ids=torch.where(labels==-100, self.config.pad_token_id, labels),\n                    past_key_values=biased_past_key_values,\n                    return_dict=True,\n                ).last_hidden_state\n                if bias == 0:\n                    unbiased_last_hidden_state = biased_last_hidden_state\n                    if self.config.add_skip_connection:\n                        positive_skip_zs = sampling(post_mean, post_logvar, n_samples=labels.shape[1])\n                        skip_hidden_state = F.layer_norm(\n                            self.skip_connection(positive_skip_zs.transpose(1, 2)),\n                            normalized_shape=[self.config.n_embd],\n                            eps=self.config.layer_norm_epsilon\n                        )\n                        biased_last_hidden_state = biased_last_hidden_state + \\\n                                                   (skip_hidden_state - skip_hidden_state.detach())\n                biased_lm_logits = self.base.lm_head(biased_last_hidden_state)\n                nlls = F.cross_entropy(\n                    input=biased_lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n                    target=labels[:, 1:].contiguous(),\n                    reduction=\'none\'\n                ).sum(dim=-1)\n                in_batch_nlls.append(nlls)\n\n            if self.config.add_contra_loss:\n                assert len(in_batch_nlls) == mini_many\n                in_batch_nlls = torch.stack(in_batch_nlls, dim=0)\n                loss_contra = F.cross_entropy(\n                    input=-in_batch_nlls.T,\n                    target=torch.zeros(mini_many, dtype=torch.int64, device=in_batch_nlls.device),\n                    reduction=\'mean\'\n                )\n                lm_logits = self.base.lm_head(unbiased_last_hidden_state)\n                loss_lm = F.cross_entropy(\n                    input=lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n                    target=labels[:, 1:].contiguous(),\n                    reduction=\'none\'\n                ).sum(dim=-1).mean()\n            else:\n                assert self.config.add_skip_connection\n                loss_contra = torch.zeros_like(input_ps[0][0].sum())\n                loss_lm = in_batch_nlls[0].mean()\n        else:\n            loss_contra = torch.zeros_like(input_ps[0][0].sum())\n            unbiased_last_hidden_state = self.base.transformer(\n                input_ids=torch.where(labels==-100, self.config.pad_token_id, labels),\n                past_key_values=prompted_prefix_outputs.past_key_values,\n                return_dict=True,\n            ).last_hidden_state\n            lm_logits = self.base.lm_head(unbiased_last_hidden_state)\n            loss_lm = F.cross_entropy(\n                input=lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n                target=labels[:, 1:].contiguous(),\n                reduction=\'none\'\n            ).sum(dim=-1).mean()\n\n        loss_vae = loss_lm + loss_kld + loss_contra\n\n        return loss_vae, loss_lm, loss_kld, loss_contra\n\n    def load_lpo_policy_latent_encoder(self, path_to_lpo_model):\n        from config_and_utils import load_checkpoint\n        lpo_sd = load_checkpoint(path_to_lpo_model, device=self.latent_encoder.aggregator.queries.device)\n        self.latent_encoder.load_state_dict({\n            k[len("policy_latent_encoder."):]:v for k,v in lpo_sd.items()\n            if k.startswith("policy_latent_encoder.")\n        })\n        self.config.use_standard_prior = False\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=None,\n        streamer=None,\n        post_labels: torch.LongTensor=None,\n        post_latent: Tuple[torch.FloatTensor]=None,\n        standard_prior: bool=False,\n        latent_sampling: bool=False,\n        **kwargs,\n    ):\n        input_ids = kwargs["input_ids"]\n\n        if post_latent is not None:\n            mean, logvar = post_latent\n        elif post_labels is not None:\n            mean, logvar = self.latent_encoder(post_labels)\n        elif standard_prior:\n            mean, logvar = self.latent_encoder(input_ids)\n            mean, logvar = torch.zeros_like(mean), torch.zeros_like(logvar)\n        else:\n            mean, logvar = self.latent_encoder(input_ids)\n\n        if not latent_sampling:\n            logvar.fill_(-100)\n\n        input_ps = self.latent_decoder(mean, logvar)\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids[:, :-1]!=self.config.pad_token_id).long()\n        ], dim=1)\n        position_ids = attention_mask.long().cumsum(-1) - 1\n        position_ids.masked_fill_(attention_mask == 0, 1)\n        position_ids = position_ids[:, self.config.num_p:]\n        outputs = self.base.transformer(\n            input_ids=input_ids[:, :-1],\n            past_key_values=input_ps,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            use_cache=True,\n            return_dict=True\n        )\n        kwargs["past_key_values"] = outputs.past_key_values\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids!=self.config.pad_token_id).long()\n        ], dim=1)\n        kwargs["attention_mask"] = attention_mask\n        kwargs["use_cache"] = True\n\n        return self.base.generate(\n            inputs=inputs,\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            stopping_criteria=stopping_criteria,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            synced_gpus=synced_gpus,\n            streamer=streamer,\n            **kwargs,\n        )\n\n\nclass GPTJForCVAE(GPTJPreTrainedModel):\n    config_class = GPTJCVAEConfig\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config: GPTJCVAEConfig):\n        super().__init__(config)\n        self.config = config\n        self.base = GPTJForCausalLM(config)\n        if config.frozen_pretrained:\n            self.base.requires_grad_(False)\n        self.prior_latent_encoder = LatentEncoder(config)\n        self.post_latent_encoder = LatentEncoder(config)\n        self.latent_decoder = LatentDecoder(config)\n\n    def load_pretrained(self):\n        pretrained_model = GPTJForCausalLM.from_pretrained(self.config.base_model_path)\n        self.base.load_state_dict(pretrained_model.state_dict())\n        if self.config.frozen_pretrained:\n            self.base.requires_grad_(False)\n        self.prior_latent_encoder.init_wte(self.base.transformer.wte)\n        self.post_latent_encoder.init_wte(self.base.transformer.wte)\n        self.prior_latent_encoder.standardize()\n        self.post_latent_encoder.standardize()\n\n    def load_lpo_policy_latent_encoder(self, path_to_lpo_model):\n        lpo_model = LPOModel(self.config)\n        if os.path.exists(os.path.join(path_to_lpo_model, "pytorch_model.bin")):\n            lpo_model.load_state_dict(torch.load(os.path.join(path_to_lpo_model, "pytorch_model.bin")))\n        elif os.path.exists(os.path.join(path_to_lpo_model, "model.safetensors")):\n            from safetensors.torch import load_file\n            lpo_model.load_state_dict(load_file(os.path.join(path_to_lpo_model, "model.safetensors")))\n        else:\n            raise Exception(f"checkpoint file not found in {os.listdir(path_to_lpo_model)}")\n        self.prior_latent_encoder.load_state_dict(lpo_model.policy_latent_encoder.state_dict())\n        self.config.use_standard_prior = False\n\n    def load_pt_latent_decoder(self, pt_model_path):\n        pt_model = PTModel(config=self.config)\n        pt_model.load_state_dict(torch.load(os.path.join(pt_model_path, "pytorch_model.bin")))\n        for z2k in pt_model.latent_decoder.z2ks:\n            z2k.weight.data /= z2k.weight.data.norm(dim=-1).unsqueeze(-1)/self.config.latent_weight_init_norm\n        for z2v in pt_model.latent_decoder.z2vs:\n            z2v.weight.data /= z2v.weight.data.norm(dim=-1).unsqueeze(-1)/self.config.latent_weight_init_norm\n        self.latent_decoder.load_state_dict(pt_model.latent_decoder.state_dict())\n\n    def load_vae_state_dict(self, vae_model_path):\n        gptj_vae = GPTJForVAE.from_pretrained(vae_model_path)\n        self.post_latent_encoder.load_state_dict(gptj_vae.latent_encoder.state_dict())\n        self.latent_decoder.load_state_dict(gptj_vae.latent_decoder.state_dict(), strict=False)\n        self.base.load_state_dict(gptj_vae.base.state_dict(), strict=False)\n\n    # for training & evaluation only\n    def forward(\n        self,\n        input_ids: torch.LongTensor,  # [1, prompt_len]\n        labels: torch.LongTensor,  # [num_mini, mini_many, seq_len]\n        prior_ids: torch.LongTensor,  # [1, seq_len]\n        post_ids: torch.LongTensor,  # [num_mini, mini_many, ans_seq_len]\n        return_loss=True,\n        **kwargs,\n    ):\n        num_mini, mini_many, seq_len = labels.shape\n        total_many = mini_many * num_mini\n        post_mean, post_logvar = self.post_latent_encoder(post_ids.view(total_many, post_ids.shape[-1]))\n        if self.config.use_standard_prior:\n            prior_mean, prior_logvar = torch.zeros_like(post_mean), torch.zeros_like(post_logvar)\n        else:\n            prior_mean, prior_logvar = self.prior_latent_encoder(prior_ids)\n            prior_mean, prior_logvar = prior_mean.repeat(total_many, 1), prior_logvar.repeat(total_many, 1)\n\n        # only compute lm loss for the (currently) concerned output\n        input_ps = self.latent_decoder(post_mean[:mini_many, :], post_logvar[:mini_many, :])\n        # n_layer * [mini_many, num_p, n_embd] <- [mini_many, dim_z]\n        labels = labels[0, :, :]\n        # [mini_many, seq_len]\n        input_ids = input_ids.repeat(mini_many, 1)\n        # [mini_many, prompt_len]\n\n        outputs = self.base.transformer(\n            input_ids=input_ids,\n            past_key_values=input_ps,\n            use_cache=True,\n            return_dict=True\n        )\n        past_key_values = outputs.past_key_values\n        last_hidden_state = outputs.last_hidden_state\n\n        in_batch_nlls = torch.stack([\n            torch.roll(\n                self.get_conditional_nlls(\n                    input_ids,\n                    last_hidden_state,\n                    past_key_values,\n                    torch.roll(labels, shifts=i, dims=0)\n                ), shifts=-i, dims=0\n            ) for i in range(1 if self.config.without_contra else mini_many)\n        ], dim=0)\n        # -log p(y_j|x,z_(i+j))\n        # e.g.\n        # [[(y_0|x,z_0), (y_1|x,z_1), (y_2|x,z_2)], -> positive\n        #  [(y_0|x,z_1), (y_1|x,z_2), (y_2|x,z_0)], -> negative\n        #  [(y_0|x,z_2), (y_1|x,z_0), (y_2|x,z_1)]] -> negative\n\n        nlls_positive = in_batch_nlls[0]\n        if self.config.without_contra:\n            loss_contrastive = torch.zeros_like(nlls_positive.mean())\n        else:\n            loss_contrastive = torch.nn.functional.cross_entropy(\n                input=-in_batch_nlls.T,\n                target=torch.zeros(mini_many, dtype=torch.int64, device=in_batch_nlls.device)\n            ).mean()\n\n        loss_kld = dg_kld(self.config, prior_mean, prior_logvar, post_mean, post_logvar)\n        if self.config.without_dg_kld:\n            standard_kld = 0.5 * (post_mean.pow(2) + post_logvar.exp() - post_logvar - 1).sum(dim=1).mean(dim=0)\n            loss_cvae = nlls_positive.mean() + standard_kld\n        else:\n            loss_cvae = nlls_positive.mean() + loss_kld\n\n        loss = loss_cvae + loss_contrastive\n\n        return loss, loss_cvae, loss_contrastive, loss_kld\n\n    def get_conditional_nlls(self, input_ids, input_ids_hidden_state, past_key_values, labels):\n        labels_hidden_state = self.base.transformer(\n            input_ids=torch.where(labels == -100, self.config.pad_token_id, labels),\n            past_key_values=past_key_values,\n            return_dict=True,\n        ).last_hidden_state\n        hidden_state = torch.cat([input_ids_hidden_state, labels_hidden_state], dim=1)\n        lm_logits = self.base.lm_head(hidden_state)\n        nlls = F.cross_entropy(\n            input=lm_logits[:, :-1, :].contiguous().transpose(1, 2),\n            target=torch.cat([input_ids.clone().fill_(-100), labels], dim=1)[:, 1:].contiguous(),\n            reduction=\'none\'\n        ).sum(dim=-1)\n        return nlls\n\n    def posterior_evaluation(\n        self,\n        input_ids: torch.LongTensor,  # [1, prompt_len]\n        labels: torch.LongTensor,  # [num_mini, mini_many, seq_len]\n        prior_ids: torch.LongTensor,  # [1, seq_len]\n        post_ids: torch.LongTensor,  # [num_mini, mini_many, ans_seq_len])\n        n_samples = 32\n    ):\n        num_mini, mini_many, seq_len = labels.shape\n        total_many = mini_many * num_mini\n        post_mean, post_logvar = self.post_latent_encoder(post_ids.view(total_many, post_ids.shape[-1]))\n        if self.config.use_standard_prior:\n            prior_mean, prior_logvar = torch.zeros_like(post_mean), torch.zeros_like(post_logvar)\n        else:\n            prior_mean, prior_logvar = self.prior_latent_encoder(prior_ids)\n            prior_mean, prior_logvar = prior_mean.repeat(total_many, 1), prior_logvar.repeat(total_many, 1)\n\n        # now we begin to calculate KL(q(z|x,y)||p(z|x,y))\n        # where p(z|x,y) = p(y|x,z)p(z|x)/p(y|x)\n\n        # step1. sampling zs\n        zs_post = sampling(post_mean, post_logvar, n_samples=n_samples)\n        # [total_many, dim_z, n_samples]\n        zs_prior = sampling(prior_mean, prior_logvar, n_samples=n_samples)\n        # [total_many, dim_z, n_samples]\n        zs = torch.cat([zs_post, zs_prior], dim=-1)\n        # [total_many, dim_z, n_samples * 2]\n\n        # step2. log q(z|x,y)\n        log_qz_y = log_pdf(post_mean, post_logvar, zs).sum(dim=1)\n\n        # step3. log p(z|x,y)\n        # step3.1 log p(y|x,z)\n        log_py_z = []\n        input_ids = input_ids.repeat(total_many, 1)\n        labels = labels.view(total_many, labels.shape[-1])\n        for iz in range(zs.shape[-1]):\n            outputs = self.base.transformer(\n                input_ids=input_ids,\n                past_key_values=self.latent_decoder.decode(zs[:,:,iz]),\n                use_cache=True,\n                return_dict=True\n            )\n            log_py_zi = -self.get_conditional_nlls(\n                input_ids,\n                outputs.last_hidden_state,\n                outputs.past_key_values,\n                labels\n            )\n            log_py_z.append(log_py_zi)\n        log_py_z = torch.stack(log_py_z, dim=-1)\n        # step3.2 log p(z|x)\n        log_pz = log_pdf(prior_mean, prior_logvar, zs).sum(dim=1)\n        # step3.3 log p(y|x)\n        #log_py = log_py_z.exp().mean(dim=-1, keepdims=True).log()\n        bias = log_py_z.max(dim=1, keepdims=True).values\n        log_py = (log_py_z-bias).exp().mean(dim=-1, keepdims=True).log() + bias\n        # step3.4 \n        log_pz_y = log_py_z + log_pz - log_py\n\n        # step4. monte carlo estimation\n\n        # importance_weight = log_qz_y.exp() * 2 / (log_qz_y.exp() + log_pz.exp())\n        # importance_weight = (log_qz_y-bias).exp() * 2 / ((log_qz_y-bias).exp() + (log_pz-bias).exp())\n        bias = torch.where(log_qz_y>log_pz, log_qz_y, log_pz)\n        qz_y_ = (log_qz_y-bias).exp()\n        pz_ = (log_pz-bias).exp()\n        importance_weight = qz_y_ * 2 / (qz_y_ + pz_)\n\n        kld = ((log_qz_y - log_pz_y) * importance_weight).mean(dim=-1)\n\n        return kld\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=None,\n        streamer=None,\n        post_labels: torch.LongTensor=None,\n        post_latent: Tuple[torch.FloatTensor]=None,\n        standard_prior: bool=False,\n        latent_sampling: bool=False,\n        **kwargs,\n    ):\n        input_ids = kwargs["input_ids"]\n\n        if post_latent is not None:\n            mean, logvar = post_latent\n        elif post_labels is not None:\n            mean, logvar = self.post_latent_encoder(post_labels)\n        elif standard_prior:\n            mean, logvar = self.prior_latent_encoder(input_ids)\n            mean, logvar = torch.zeros_like(mean), torch.zeros_like(logvar)\n        else:\n            mean, logvar = self.prior_latent_encoder(input_ids)\n\n        if not latent_sampling:\n            logvar.fill_(-100)\n\n        input_ps = self.latent_decoder(mean, logvar)\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids[:, :-1]!=self.config.pad_token_id).long()\n        ], dim=1)\n        position_ids = attention_mask.long().cumsum(-1) - 1\n        position_ids.masked_fill_(attention_mask == 0, 1)\n        position_ids = position_ids[:, self.config.num_p:]\n        outputs = self.base.transformer(\n            input_ids=input_ids[:, :-1],\n            past_key_values=input_ps,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            use_cache=True,\n            return_dict=True\n        )\n        kwargs["past_key_values"] = outputs.past_key_values\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids!=self.config.pad_token_id).long()\n        ], dim=1)\n        kwargs["attention_mask"] = attention_mask\n        kwargs["use_cache"] = True\n\n        return self.base.generate(\n            inputs=inputs,\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            stopping_criteria=stopping_criteria,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            synced_gpus=synced_gpus,\n            streamer=streamer,\n            **kwargs,\n        )\n\n\nclass LPOModel(nn.Module):\n    main_input_name = "prompt_ids"\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.prior_latent_encoder = LatentEncoder(config)\n        self.prior_latent_encoder.requires_grad_(False)\n        self.post_latent_encoder = LatentEncoder(config)\n        self.post_latent_encoder.requires_grad_(False)\n        self.policy_latent_encoder = LatentEncoder(config)\n\n    @classmethod\n    def from_cvae(cls, cvae_model_path, **kwargs):\n        gptj_cvae = GPTJForCVAE.from_pretrained(cvae_model_path)\n        config = gptj_cvae.config\n        config.update(kwargs)\n        model = cls(config)\n        model.prior_latent_encoder.load_state_dict(gptj_cvae.prior_latent_encoder.state_dict())\n        model.policy_latent_encoder.load_state_dict(gptj_cvae.prior_latent_encoder.state_dict())\n        model.post_latent_encoder.load_state_dict(gptj_cvae.post_latent_encoder.state_dict())\n        return model\n\n    @classmethod\n    def from_vae(cls, vae_model_path, **kwargs):\n        gptj_vae = GPTJForVAE.from_pretrained(vae_model_path)\n        config = gptj_vae.config\n        config.update(kwargs)\n        model = cls(config)\n        model.policy_latent_encoder.load_state_dict(gptj_vae.latent_encoder.state_dict())\n        model.policy_latent_encoder.standardize()\n        model.post_latent_encoder.load_state_dict(gptj_vae.latent_encoder.state_dict())\n        return model\n\n    def forward(\n        self,\n        prompt_ids: torch.LongTensor, # x\n        chosen_ids: torch.LongTensor, # y_w\n        rejected_ids: torch.LongTensor, # y_l\n    ):\n        prior_ids, chosen_post_ids, rejected_post_ids = prompt_ids, chosen_ids, rejected_ids\n        batch_size = prior_ids.shape[0]\n\n        # pi_theta(z|x)\n        policy_mean, policy_logvar = self.policy_latent_encoder(prior_ids)\n        with torch.no_grad():\n            # pi_ref(z|x)\n            if self.config.use_standard_prior:\n                prior_mean, prior_logvar = torch.zeros_like(policy_mean), torch.zeros_like(policy_logvar)\n            else:\n                prior_mean, prior_logvar = self.prior_latent_encoder(prior_ids)\n            # q(z|x,y_w), and q(z|x,y_l)\n            chosen_post_mean, chosen_post_logvar = self.post_latent_encoder(chosen_post_ids)\n            rejected_post_mean, rejected_post_logvar = self.post_latent_encoder(rejected_post_ids)\n        # batch_size, dim_z\n\n        n_samples = self.config.lpo_sampling_times\n        prior_zs = sampling(prior_mean, prior_logvar, n_samples=n_samples)\n        chosen_zs = sampling(chosen_post_mean, chosen_post_logvar, n_samples=n_samples)\n        rejected_zs = sampling(rejected_post_mean, rejected_post_logvar, n_samples=n_samples)\n        zs = torch.cat([prior_zs, chosen_zs, rejected_zs], dim=-1)\n        n_samples = zs.shape[-1]\n        # batch_size, dim_z, n_samples\n\n        # compare different zs according to posterior likelihoods\n        with torch.no_grad():\n            # -log q(z|x,y_w)\n            zs_nlls_given_chosen = -log_pdf(chosen_post_mean, chosen_post_logvar, zs).sum(dim=1)\n            # -log q(z|x,y_l)\n            zs_nlls_given_rejected = -log_pdf(rejected_post_mean, rejected_post_logvar, zs).sum(dim=1)\n\n            # log q(z|x,y_w)/q(z|x,y_l), greater is better\n            score_zs = zs_nlls_given_rejected - zs_nlls_given_chosen\n            # batch_size, n_samples\n\n            # compare zs in pairs\n            comparison_matrix = score_zs[:, :, None] - score_zs[:, None, :]\n            comparison_matrix = comparison_matrix.detach()\n            # batch_size, n_samples, n_samples\n\n        # perform DPO on zs according to the comparison results\n        zs_refer_nlls = -log_pdf(prior_mean, prior_logvar, zs).sum(dim=1)\n        zs_policy_nlls = -log_pdf(policy_mean, policy_logvar, zs).sum(dim=1)\n        zs_policy_reward = zs_refer_nlls.detach() - zs_policy_nlls\n        # batch_size, n_samples\n\n        zs_policy_reward_left = zs_policy_reward[:, :, None].repeat(1, 1, n_samples)\n        zs_policy_reward_right = zs_policy_reward[:, None, :].repeat(1, n_samples, 1)\n        # batch_size, n_samples, n_samples\n\n        left_is_chosen = comparison_matrix > 0\n        reward_chosen = torch.where(left_is_chosen, zs_policy_reward_left, zs_policy_reward_right)\n        reward_rejected = torch.where(left_is_chosen, zs_policy_reward_right, zs_policy_reward_left)\n        # batch_size, n_samples, n_samples\n\n        loss = -F.logsigmoid((reward_chosen - reward_rejected) * self.config.beta)\n        acc = (reward_chosen > reward_rejected).float()\n\n        # mask self v.s. self\n        comparison_mask = 1 - torch.eye(n_samples).float()\n        comparison_mask = comparison_mask.unsqueeze(0).repeat(batch_size, 1, 1)\n        comparison_mask = comparison_mask.to(comparison_matrix.device)\n        # batch_size, n_samples, n_samples\n        loss = (loss * comparison_mask).sum() / comparison_mask.sum()\n        acc = (acc * comparison_mask).sum() / comparison_mask.sum()\n\n        return loss, acc\n\n\nclass DPOModel(GPTJPreTrainedModel):\n    config_class = GPTJConfig\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.base = GPTJForCausalLM(config)\n        self.beta = config.beta\n\n    def load_pretrained(self, pretrained_model=None):\n        if pretrained_model is None:\n            pretrained_model = GPTJForCausalLM.from_pretrained(self.config.base_model_path)\n        self.base.load_state_dict(pretrained_model.state_dict())\n\n    def prepare_refer_model(self):\n        self.refer_model = copy.deepcopy(self)\n        self.refer_model.requires_grad_(False)\n        self.refer_model.eval()\n\n    def compute_nlls(self, input_ids, labels):\n        hidden_states = self.base.transformer(input_ids=input_ids)[0]\n        lm_logits = self.base.lm_head(hidden_states).to(torch.float32)\n        labels = labels.to(lm_logits.device)\n        shift_logits = lm_logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        nlls = F.cross_entropy(\n            input=shift_logits.transpose(1, 2),\n            target=shift_labels,\n            reduction=\'none\'\n        ).sum(dim=-1)\n        return nlls\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        labels: torch.LongTensor,\n        refer_nlls: torch.FloatTensor = None,\n        reward: torch.FloatTensor = None,  # r(x,y)\n    ):\n        theta_nlls = self.compute_nlls(input_ids=input_ids, labels=labels)\n        if refer_nlls is None:\n            assert hasattr(self, "refer_model")\n            with torch.no_grad():\n                refer_nlls = self.refer_model.compute_nlls(input_ids=input_ids, labels=labels)\n        reward = refer_nlls.detach() - theta_nlls\n        chosen_reward = reward[::2]\n        rejected_reward = reward[1::2]\n        loss = -F.logsigmoid((chosen_reward - rejected_reward) * self.beta)\n        acc = (chosen_reward > rejected_reward).float()\n        return loss.mean(), acc.mean()\n\n\nclass PTModel(GPTJPreTrainedModel):\n    config_class = GPTJCVAEConfig\n    main_input_name = "input_ids"\n    is_parallelizable = False\n\n    def __init__(self, config: GPTJCVAEConfig):\n        super().__init__(config)\n        self.base = GPTJForCausalLM(config)\n        self.base.requires_grad_(False)\n        self.hs = torch.nn.ParameterList([\n            torch.nn.Parameter(torch.Tensor(1, config.num_q, config.n_embd))\n            for i in range(config.n_layer)\n        ])\n        for param in self.hs:\n            param.data.normal_(mean=0.0, std=config.initializer_range)\n        self.lns = nn.ModuleList([nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n                                  for i in range(config.n_layer)])\n        self.attns = nn.ModuleList([GPTJAttention(config)\n                                    for i in range(config.n_layer)])\n        self.attns.requires_grad_(False)\n        self.beta = config.beta\n        self.mode = "sft"\n\n    def load_pretrained(self, pretrained_model=None):\n        if pretrained_model is None:\n            pretrained_model = GPTJForCausalLM.from_pretrained(self.config.base_model_path)\n        self.base.load_state_dict(pretrained_model.state_dict())\n        for i in range(self.config.n_layer):\n            self.attns[i].load_state_dict(pretrained_model.transformer.h[i].attn.state_dict())\n\n    def switch_into_dpo_mode(self):\n        self.mode = "dpo"\n        self.refer_model = copy.deepcopy(self)\n        self.refer_model.requires_grad_(False)\n\n    def get_input_ps(self, input_ids):\n        batch_size = input_ids.shape[0]\n        past_key_values = []\n        for i in range(self.config.n_layer):\n            hidden_states, ln, attn = self.hs[i], self.lns[i], self.attns[i]\n            hidden_states = hidden_states.repeat(batch_size, 1, 1)\n            hidden_states = ln(hidden_states)\n            position_ids = torch.arange(self.config.num_p).unsqueeze(0).repeat(batch_size, 1).to(hidden_states.device)\n            present = attn(\n                hidden_states=hidden_states,\n                position_ids=position_ids,\n                use_cache=True\n            )[1]\n            past_key_values.append(present)\n        input_ps = past_key_values\n        return input_ps\n\n    def compute_nlls(self, input_ids, labels):\n        input_ps = self.get_input_ps(input_ids)\n        hidden_states = self.base.transformer.forward(\n            input_ids=input_ids,\n            past_key_values=input_ps\n        )[0]\n        lm_logits = self.base.lm_head(hidden_states).to(torch.float32)\n        labels = labels.to(lm_logits.device)\n        shift_logits = lm_logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        nlls = F.cross_entropy(\n            input=shift_logits.transpose(1, 2),\n            target=shift_labels,\n            reduction=\'none\'\n        ).sum(dim=-1)\n        return nlls\n\n    def forward(self, input_ids, labels, refer_nlls=None):\n        if self.mode == "sft":\n            # supervised fine-tuning\n            theta_nlls = self.compute_nlls(input_ids=input_ids, labels=labels)\n            return {"loss": theta_nlls.mean()}\n        elif self.mode == "dpo":\n            # dpo\n            theta_nlls = self.compute_nlls(input_ids=input_ids, labels=labels)\n            if refer_nlls is None:\n                assert hasattr(self, "refer_model")\n                self.refer_model.eval()\n                with torch.no_grad():\n                    refer_nlls = self.refer_model.compute_nlls(input_ids=input_ids, labels=labels)\n            log_iw = refer_nlls.detach() - theta_nlls\n            chosen_log_iw = log_iw[::2]\n            rejected_log_iw = log_iw[1::2]\n            disadvantage = (rejected_log_iw - chosen_log_iw) * self.beta\n            loss = -F.logsigmoid(-disadvantage)\n            acc = (loss < 0.6931).float()\n            return loss.mean(), acc.mean()\n        else:\n            raise NotImplementedError(f"mode: {self.mode}")\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=None,\n        streamer=None,\n        **kwargs,\n    ):\n        input_ids = kwargs["input_ids"]\n        input_ps = self.get_input_ps(input_ids)\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids[:, :-1]!=self.config.pad_token_id).long()\n        ], dim=1)\n        position_ids = attention_mask.long().cumsum(-1) - 1\n        position_ids.masked_fill_(attention_mask == 0, 1)\n        position_ids = position_ids[:, self.config.num_p:]\n        outputs = self.base.transformer(\n            input_ids=input_ids[:, :-1],\n            past_key_values=input_ps,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            use_cache=True,\n            return_dict=True\n        )\n        kwargs["past_key_values"] = outputs.past_key_values\n\n        attention_mask = torch.cat([\n            torch.ones(input_ids.shape[0], self.config.num_p).long().to(input_ids.device),\n            (input_ids!=self.config.pad_token_id).long()\n        ], dim=1)\n        kwargs["attention_mask"] = attention_mask\n        kwargs["use_cache"] = True\n\n        return self.base.generate(\n            inputs=inputs,\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            stopping_criteria=stopping_criteria,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            synced_gpus=synced_gpus,\n            streamer=streamer,\n            **kwargs,\n        )\n\n', './LPO-summary\\reward_model.py': 'import torch\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom config_and_utils import BASE_MODEL_PATH\n\nclass GPTRewardModel(nn.Module):\n    def __init__(self, model_path, device="cpu"):\n        super().__init__()\n        model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\n        self.config = model.config\n        # `gpt-neo(x)` models use `hidden_size` attribute names instead of `n_embd``\n        self.config.n_embd = self.config.hidden_size if hasattr(self.config, "hidden_size") else self.config.n_embd\n        self.transformer = model.transformer\n        self.v_head = nn.Linear(self.config.n_embd, 1, bias=False, device=device)\n        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.PAD_ID = self.tokenizer(self.tokenizer.pad_token)["input_ids"][0]\n\n    def forward(\n        self,\n        input_ids=None,\n        past_key_values=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        mc_token_ids=None,\n        labels=None,\n        return_dict=False,\n        output_attentions=False,\n        output_hidden_states=False,\n    ):\n        loss = None\n        transformer_outputs = self.transformer(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        hidden_states = transformer_outputs[0]\n\n        rewards = self.v_head(hidden_states).squeeze(-1)\n        chosen_end_scores = []\n        rejected_end_scores = []\n\n        # Split the inputs and rewards into two parts, chosen and rejected\n        assert len(input_ids.shape) == 2\n        bs = input_ids.shape[0] // 2\n        chosen = input_ids[:bs]\n        rejected = input_ids[bs:]\n        chosen_rewards = rewards[:bs]\n        rejected_rewards = rewards[bs:]\n\n        loss = 0\n        inference = False\n        for i in range(bs):\n            if torch.all(torch.eq(chosen[i], rejected[i])).item():\n                c_inds = (chosen[i] == self.PAD_ID).nonzero()\n                c_ind = c_inds[0].item() if len(c_inds) > 0 else chosen.shape[1]\n                chosen_end_scores.append(chosen_rewards[i, c_ind - 1])\n                inference = True\n                continue\n\n            # Check if there is any padding otherwise take length of sequence\n            c_inds = (chosen[i] == self.PAD_ID).nonzero()\n            c_ind = c_inds[0].item() if len(c_inds) > 0 else chosen.shape[1]\n            r_inds = (rejected[i] == self.PAD_ID).nonzero()\n            r_ind = r_inds[0].item() if len(r_inds) > 0 else rejected.shape[1]\n            end_ind = max(c_ind, r_ind)\n\n            # Retrieve first index where trajectories diverge\n            divergence_ind = (chosen[i] != rejected[i]).nonzero()[0]\n            assert divergence_ind > 0\n\n            # Index into the correct rewards\n            c_truncated_reward = chosen_rewards[i][divergence_ind:end_ind]\n            r_truncated_reward = rejected_rewards[i][divergence_ind:end_ind]\n\n            # Append the last rewards to the list of end scores\n            chosen_end_scores.append(c_truncated_reward[-1])\n            rejected_end_scores.append(r_truncated_reward[-1])\n\n            # Compute loss based on truncated rewards (ignore padding)\n            loss += -torch.log(torch.sigmoid(c_truncated_reward - r_truncated_reward)).mean()\n        loss = loss / bs\n\n        if not inference:\n            chosen_end_scores = torch.stack(chosen_end_scores)\n            rejected_end_scores = torch.stack(rejected_end_scores)\n\n        if inference:\n            chosen_end_scores = torch.stack(chosen_end_scores)\n            return {"chosen_end_scores": chosen_end_scores}\n\n        return {\n            "loss": loss,\n            "chosen_end_scores": chosen_end_scores,\n            "rejected_end_scores": rejected_end_scores,\n        }\n', './LPO-summary\\summary_comparison_dataset.py': 'import json\n\nimport torch\nfrom tqdm import tqdm\n\nfrom config_and_utils import get_tokenizer, get_comparison_dataset, input_max_length, output_max_length, pad_and_concat\n\ndef pad_or_truncate(ids, target_length, pad_id):\n    if ids.shape[1] > target_length:\n        ids = ids[:,:target_length]\n    elif ids.shape[1] < target_length:\n        padding_ids = torch.LongTensor(size=(ids.shape[0], target_length-ids.shape[1]))\n        padding_ids.fill_(pad_id)\n        padding_ids = padding_ids.to(ids.device)\n        ids = torch.cat([ids, padding_ids], dim=1)\n    return ids\n\n\nclass SummaryComparisonDataset:\n    def __init__(self, usage="train", preference="human"):\n        if preference=="human":\n            dataset = get_comparison_dataset()\n            if usage=="train":\n                dataset = dataset["train"]\n            elif usage=="validation":\n                dataset = list(dataset["valid1"]) + list(dataset["valid2"])\n                dataset = dataset[:len(dataset)//10]\n                # DPO peforms slowly on validation\n                # for fairness and generalizability, we only use 10% of the orginal validation data\n                # train / validation / test - sft\n                # 92534 / 8379 / 6553\n            elif usage=="test":\n                dataset = dataset["test"]\n            else:\n                raise NotImplementedError(usage)\n            self.dataset = [data for data in tqdm(dataset) if data["chosen"]!=data["rejected"]]\n        else:\n            jsonl_path = f"./openai_summarize_comparisons_{preference}/{usage}.jsonl"\n            with open(jsonl_path, "r", encoding="utf-8") as f:\n                lines = [line for line in f]\n            self.dataset = [json.loads(line) for line in tqdm(lines)]\n        self.usage = usage\n        self.preference = preference\n        self.tokenizer = get_tokenizer()\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def process_data(self, data):\n        prompt: str = data["prompt"]\n        chosen: str = data["chosen"]\n        rejected: str = data["rejected"]\n        tldr = "TL;DR:"\n        assert chosen.startswith(tldr), chosen\n        assert rejected.startswith(tldr), rejected\n\n        eos = "<|endoftext|>"\n        if eos in chosen:\n            chosen = chosen[:chosen.index(eos)]\n        if eos in rejected:\n            rejected = rejected[:rejected.index(eos)]\n\n        prompt = prompt.strip() + "\\n" + tldr + " "\n        chosen = chosen[len(tldr):].strip()\n        rejected = rejected[len(tldr):].strip()\n\n        return prompt, chosen, rejected\n\n    def __getitem__(self, item):\n        data = self.dataset[item]\n        prompt, chosen, rejected = self.process_data(data)\n        prompt_ids = self.tokenizer(prompt, return_tensors="pt").input_ids\n        chosen_ids = self.tokenizer(chosen, return_tensors="pt").input_ids\n        rejected_ids = self.tokenizer(rejected, return_tensors="pt").input_ids\n        return prompt_ids, chosen_ids, rejected_ids\n\n    def lpo_collate_fn(self, batch_items):\n        batch_prompt_ids, batch_chosen_ids, batch_rejected_ids = [], [], []\n        for prompt_ids, chosen_ids, rejected_ids in batch_items:\n            batch_prompt_ids.append(prompt_ids)\n            batch_chosen_ids.append(chosen_ids)\n            batch_rejected_ids.append(rejected_ids)\n\n        pad_token_id = self.tokenizer.pad_token_id\n        return {\n            "prompt_ids": pad_and_concat(batch_prompt_ids, pad_token_id)[:,:input_max_length].long(),\n            "chosen_ids": pad_and_concat(batch_chosen_ids, pad_token_id)[:,:output_max_length].long(),\n            "rejected_ids": pad_and_concat(batch_rejected_ids, pad_token_id)[:,:output_max_length].long(),\n        }\n\n    def dpo_collate_fn(self, batch_items):\n        input_ids, labels = [], []\n        for prompt_ids, chosen_ids, rejected_ids in batch_items:\n            chosen_input_ids = torch.cat([prompt_ids, chosen_ids], dim=1)\n            rejected_input_ids = torch.cat([prompt_ids, rejected_ids], dim=1)\n            chosen_labels = torch.cat([prompt_ids.clone().fill_(-100), chosen_ids], dim=1)\n            rejected_labels = torch.cat([prompt_ids.clone().fill_(-100), rejected_ids], dim=1)\n            input_ids.append(chosen_input_ids)\n            input_ids.append(rejected_input_ids)\n            labels.append(chosen_labels)\n            labels.append(rejected_labels)\n        max_length = input_max_length + output_max_length\n        inputs = {\n            "input_ids": pad_and_concat(input_ids, self.tokenizer.pad_token_id).long()[:,:max_length],\n            "labels": pad_and_concat(labels, -100).long()[:,:max_length]\n        }\n        return inputs', './LPO-summary\\summary_dataset.py': 'import torch\n\nfrom config_and_utils import (\n    get_tokenizer,\n    get_sft_dataset,\n    input_max_length,\n    output_max_length,\n    pad_and_concat\n)\n\n\ndef pad_or_truncate(ids, target_length, pad_id):\n    if ids.shape[1] > target_length:\n        ids = ids[:,:target_length]\n    elif ids.shape[1] < target_length:\n        padding_ids = torch.LongTensor(size=(ids.shape[0], target_length-ids.shape[1]))\n        padding_ids.fill_(pad_id)\n        padding_ids = padding_ids.to(ids.device)\n        ids = torch.cat([ids, padding_ids], dim=1)\n    return ids\n\n\nclass SummaryDataset:\n    def __init__(self, usage="train"):\n        dataset = get_sft_dataset()\n        if usage=="train":\n            self.dataset = dataset["train"]\n        elif usage=="validation":\n            self.dataset = dataset["valid"]\n        elif usage=="test":\n            self.dataset = dataset["test"]\n        else:\n            raise NotImplementedError(usage)\n        self.usage = usage\n        self.tokenizer = get_tokenizer()\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, item):\n        prompt, label = self.dataset[item]["prompt"], self.dataset[item]["label"]\n        input_ids = self.tokenizer([prompt + label], return_tensors="pt", truncation=True,\n                                   max_length=input_max_length+output_max_length).input_ids\n        return {"input_ids": input_ids}\n\n    def sft_collate_fn(self, batch_items):\n        input_ids = pad_and_concat(\n            [item["input_ids"] for item in batch_items],\n            pad_token_id=self.tokenizer.pad_token_id,\n            padding_side="left", dim=1\n        )\n        attention_mask = (input_ids==self.tokenizer.pad_token_id).long()\n        labels = torch.where(attention_mask, -100, input_ids)\n        return {\n            "input_ids": input_ids,\n            "labels": labels,\n            "attention_mask": attention_mask,\n        }\n', './LPO-summary\\summary_one2many_dataset.py': 'import argparse\nimport json\nimport os\nfrom collections import defaultdict\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom config_and_utils import (\n    input_max_length, output_max_length,\n    BASE_MODEL_PATH, get_tokenizer\n)\nfrom summary_comparison_dataset import SummaryComparisonDataset\nfrom summary_dataset import SummaryDataset\nfrom transformers import GPTJForCausalLM\n\n\ndef pad_or_truncate(ids, target_length, pad_id):\n    if ids.shape[1] > target_length:\n        ids = ids[:,:target_length]\n    elif ids.shape[1] < target_length:\n        padding_ids = torch.LongTensor(size=(ids.shape[0], target_length-ids.shape[1]))\n        padding_ids.fill_(pad_id)\n        padding_ids = padding_ids.to(ids.device)\n        ids = torch.cat([ids, padding_ids], dim=1)\n    return ids\n\n\nclass SummaryOne2ManyDataset(SummaryComparisonDataset):\n    def __init__(self, usage="train", preference="human"):\n        super().__init__(usage, preference)\n        prompt2generation = defaultdict(set)\n        for data in self.dataset:\n            prompt, chosen, rejected = self.process_data(data)\n            if len(chosen)>1: prompt2generation[prompt].add(chosen)\n            if len(rejected)>1: prompt2generation[prompt].add(rejected)\n        self.total_many = 4\n        self.prompt2generation = {prompt:list(generation)\n                                  for prompt,generation in prompt2generation.items() if len(generation)>=self.total_many}\n        self.prompts = list(self.prompt2generation.keys())\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __getitem__(self, item):\n        prompt = self.prompts[item]\n        generation = self.prompt2generation[prompt]\n        generation = [generation[i] for i in list(np.random.permutation(len(generation))[:self.total_many])]\n        prompt_ids = self.tokenizer([prompt], return_tensors="pt").input_ids\n        pred_ids: torch.Tensor = self.tokenizer(generation, return_tensors="pt", padding="max_length",\n                                                truncation=True, max_length=output_max_length).input_ids\n        prompt_len = prompt_ids.shape[1]\n        input_ids = prompt_ids\n        # [1, prompt_len]\n        labels = torch.where(pred_ids == self.tokenizer.pad_token_id, -100, pred_ids)\n        # [N, output_max_length]\n        prior_ids = pad_or_truncate(prompt_ids, input_max_length, self.tokenizer.pad_token_id)\n        # [1, input_max_length]\n        post_ids = pad_or_truncate(pred_ids, output_max_length, self.tokenizer.pad_token_id)\n        # [N, output_max_length]\n\n        N = len(generation)\n        assert input_ids.shape == (1, prompt_len), input_ids.shape\n        assert labels.shape == (N, output_max_length), labels.shape\n        assert prior_ids.shape == (1, input_max_length), prior_ids.shape\n        assert post_ids.shape == (N, output_max_length), post_ids.shape\n        assert not torch.any(input_ids==self.tokenizer.pad_token_id), input_ids\n\n        return {\n            "input_ids": input_ids,\n            "labels": labels,\n            "prior_ids": prior_ids,\n            "post_ids": post_ids\n        }\n\n    def collate_fn(self, batch_items):\n        assert len(batch_items) == 1, f"batch_size {len(batch_items)} > 1 " \\\n                                      f"is not supported in one2many training"\n        return batch_items[0]\n\n    def sft_collate_fn(self, batch_items):\n        assert len(batch_items) == 1, f"batch_size {len(batch_items)} > 1 " \\\n                                      f"is not supported in one2many training"\n        prompt_ids = batch_items[0]["input_ids"]\n        pred_labels = batch_items[0]["labels"]\n        prompt_ids = prompt_ids.repeat(pred_labels.shape[0], 1)\n        input_ids = torch.cat([prompt_ids, torch.where(pred_labels==-100, self.tokenizer.pad_token_id, pred_labels)], dim=1)\n        labels = torch.cat([torch.zeros_like(prompt_ids).fill_(-100), pred_labels], dim=1)\n        return {\n            "input_ids": input_ids,\n            "labels": labels,\n        }\n\n    def vae_collate_fn(self, batch_items):\n        assert len(batch_items) == 1, f"batch_size {len(batch_items)} > 1 " \\\n                                      f"is not supported in one2many training"\n        post_ids = batch_items[0]["post_ids"]\n        prompt_ids = batch_items[0]["input_ids"]\n        pred_labels = batch_items[0]["labels"]\n        prompt_ids = prompt_ids.repeat(pred_labels.shape[0], 1)\n        input_ids = torch.cat([prompt_ids, torch.where(pred_labels==-100, self.tokenizer.pad_token_id, pred_labels)], dim=1)\n        labels = torch.cat([torch.zeros_like(prompt_ids).fill_(-100), pred_labels], dim=1)\n        return {\n            "post_ids": post_ids,\n            "input_ids": input_ids,\n            "labels": labels,\n        }\n\n\ndef extend_one2many_dataset_with_mini_many(one2many_dataset_class, drop_other_mini=False):\n    class one2many_dataset_class_with_across_batch(one2many_dataset_class):\n        def __init__(self, total_many, mini_many, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.total_many = total_many\n            self.mini_many = mini_many\n            self.index_queue = []\n            self.present_index = None\n            self.present_order = torch.arange(self.total_many).view(self.mini_many, -1).T.reshape(-1)\n            self.present_bias = 0\n\n        def __getitem__(self, index):\n            self.index_queue.append(index)\n            if self.present_bias == 0:\n                self.present_index = self.index_queue.pop(0)\n            self.present_order = torch.roll(self.present_order, self.mini_many, dims=0)\n            self.present_bias = (self.present_bias + self.mini_many) % self.total_many\n\n            item = super().__getitem__(self.present_index)\n            item_new = {}\n            for k,v in item.items():\n                if v.shape[0] == self.total_many:\n                    if drop_other_mini:\n                        v = v[self.present_order[:self.mini_many]]\n                    else:\n                        v = v[self.present_order]\n                        v = v.view(self.total_many // self.mini_many, self.mini_many, *v.shape[1:])\n                    item_new[k] = v\n                else:\n                    item_new[k] = v\n            return item_new\n\n    return one2many_dataset_class_with_across_batch\n\n\nclass GeneratedSummaryOne2ManyDataset(SummaryOne2ManyDataset):\n    def __init__(self, usage="train"):\n        self.usage = usage\n        self.tokenizer = get_tokenizer()\n        with open("summary_one2many_generation.json", "r", encoding="utf-8") as f:\n            data = json.loads(f.read())\n            data = [d for d in data if d is not None]\n\n        data = [d for d in data if d["usage"]==usage]\n\n        self.total_many = 32\n        self.prompt2generation = {d["prompt"]:d["generation"] for d in data}\n        self.prompts = list(self.prompt2generation.keys())\n\n\nfrom gpu_map_reduce import TaskConsumer, TaskProducer\nclass SummaryGenerationProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        tasks = []\n        for usage in ["train", "validation"]:\n            dataset = SummaryDataset(usage=usage)\n            for data in tqdm(dataset.dataset, desc=f"reading prompts for {usage}"):\n                tasks.append({"prompt": data["prompt"], "usage": usage, "generation": []})\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return result is not None and task["prompt"] == result["prompt"] and not any([gen.strip()=="" for gen in result["generation"]])\n\n    def load_cached_results(self):\n        if os.path.exists(self.args.output_file) and not self.args.erase:\n            with open(self.args.output_file, "r", encoding="utf-8") as f:\n                results = json.loads(f.read())\n            if len(results) == len(self.tasks):\n                for index, result in enumerate(results):\n                    if result is not None and self.tasks[index]["prompt"] == result["prompt"]:\n                        result["usage"] = self.tasks[index]["usage"]\n                        self.tasks[index]["generation"] = [gen for gen in result["generation"] if gen.strip()!=""]\n                return results\n        results = [None for index in range(len(self.tasks))]\n        return results\n\n\nclass SummaryGenerationConsumer(TaskConsumer):\n    def init_model(self) -> None:\n        self.model = GPTJForCausalLM.from_pretrained(\n            BASE_MODEL_PATH,\n            torch_dtype=torch.float16,\n            device_map=self.device,\n        ).eval()\n        self.tokenizer = get_tokenizer()\n        self.tokenizer.truncation_side = "left"\n\n    def process_task(self, task: dict) -> dict:\n        many, mini_many = 32, 8\n        inputs = self.tokenizer([task["prompt"]], return_tensors="pt",\n                                truncation=True, max_length=input_max_length)\n        inputs = {k:v.to(self.device) for k,v in inputs.items()}\n        inputs_len = inputs["input_ids"].shape[1]\n        generation = []\n        for _ in range(many // mini_many):\n            with torch.no_grad():\n                output_ids = self.model.generate(\n                    **inputs,\n                    num_return_sequences=mini_many,\n                    do_sample=True,\n                    top_p=0.95,\n                    max_new_tokens=output_max_length,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                )\n            generation += self.tokenizer.batch_decode(output_ids[:,inputs_len:], skip_special_tokens=True)\n        task["generation"] = generation\n        return task\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'parse args for one2many generation and reward on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="30.61.1.49")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="summary_one2many_generation")\n    parser.add_argument("--output_file", type=str, default="summary_one2many_generation.json")\n    parser.add_argument("--erase", type=int, default=0)\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    if args.local_rank == -1:\n        producer = SummaryGenerationProducer(args)\n        producer.run()\n    else:\n        consumer = SummaryGenerationConsumer(args)\n        consumer.run()\n\nif __name__ == "__main__":\n    main()\n', './LPO-summary\\test_generate_and_reward_and_ppl.py': 'import os\nimport json\n\nimport torch\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom gpu_map_reduce import BatchTaskConsumer, TaskProducer\nfrom modeling_gptj_cvae import GPTJConfig, GPTJForCVAE, GPTJForCausalLM, PTModel, GPTJCVAEConfig, GPTJForVAE\nfrom config_and_utils import (\n    auto_find_checkpoint_dir,\n    BASE_MODEL_PATH,\n    RWD_MODEL_PATH,\n    get_tokenizer,\n    pad_and_concat,\n    input_max_length,\n    output_max_length,\n)\n\n\nclass PPLProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        with open(self.args.input_file, "r", encoding="utf-8") as f:\n            tasks = json.loads(f.read())\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return all([result is not None and task[key]==result[key]\n                    for key in ["prompt", "output", "rewards"]])\n\nclass PPLConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        self.tokenizer = get_tokenizer()\n        self.model = GPTJForCausalLM.from_pretrained(\n            BASE_MODEL_PATH, device_map=self.device,\n        ).eval()\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        input_sentences = [task["prompt"] + task["output"] for task in tasks]\n        inputs = self.tokenizer(input_sentences, return_tensors="pt", padding="longest")\n        inputs = {k:v.to(self.device) for k,v in inputs.items()}\n        with torch.no_grad():\n            lm_logits = self.model(**inputs).logits\n            lm_logits = lm_logits[:,:-1,:].contiguous()\n            labels = inputs["input_ids"][:,1:].contiguous()\n            labels = torch.where(labels==self.tokenizer.pad_token_id, -100, labels)\n            nlls = torch.nn.functional.cross_entropy(\n                input=lm_logits.transpose(1,2),\n                target=labels,\n                reduction=\'none\'\n            ).sum(dim=-1).tolist()\n            num_tokens = (labels!=-100).sum(dim=-1).tolist()\n        for i,task in enumerate(tasks):\n            task["nll"] = nlls[i]\n            task["num_tokens"] = num_tokens[i]\n        return tasks\n\nclass RewardProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        with open(self.args.input_file, "r", encoding="utf-8") as f:\n            tasks = json.loads(f.read())\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return all([result is not None and task[key]==result[key]\n                    for key in ["prompt", "output"]])\n\nclass RewardConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        from summary_dataset import SummaryDataset\n        from reward_model import GPTRewardModel\n        self.tokenizer = get_tokenizer()\n        self.model = GPTRewardModel(BASE_MODEL_PATH, device=self.device)\n        self.model.load_state_dict(torch.load(os.path.join(RWD_MODEL_PATH, "pytorch_model.bin"),\n                                              map_location=self.device), strict=False)\n        self.model = self.model.half().eval()\n        self.post_summary_dict = {data["prompt"].split("TL;DR:")[0] + "TL;DR: ":data["label"]\n                                  for data in SummaryDataset(usage="test").dataset}\n\n    def get_scores(self, samples):\n        scores_list = []\n        batch_size = 2\n        for i in range(0, len(samples), batch_size):\n            sub_samples = samples[i: i + batch_size]\n            sub_samples = ["<|startoftext|>" + chosen + "<|endoftext|>" for chosen in sub_samples]\n            encodings_dict = self.tokenizer(\n                sub_samples,\n                truncation=True,\n                max_length=input_max_length+output_max_length,\n                padding="max_length",\n                return_tensors="pt",\n            )\n            input_ids = encodings_dict["input_ids"].to(self.device)\n            attn_masks = encodings_dict["attention_mask"].to(self.device)\n            input_ids = input_ids.repeat(2, 1)\n            attn_masks = attn_masks.repeat(2, 1)\n            with torch.no_grad():\n                sub_scores = self.model(input_ids=input_ids, attention_mask=attn_masks)\n            scores_list.append(sub_scores["chosen_end_scores"])\n        scores = torch.cat(scores_list, dim=0)\n        return scores\n\n    def reward_fn(self, samples):\n        original_samples = [text.split("TL;DR:")[0] + "TL;DR: " for text in samples]\n        original_samples = [text + self.post_summary_dict[text] for text in original_samples]\n        original_scores = self.get_scores(original_samples)\n        scores = self.get_scores(samples)\n        norms_scores = scores - original_scores\n        return norms_scores\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        samples = [task["prompt"] + task["output"] for task in tasks]\n        with torch.no_grad():\n            rewards = self.reward_fn(samples).tolist()\n        for i,task in enumerate(tasks):\n            task["rewards"] = rewards[i]\n        return tasks\n\nclass GenerateProducer(TaskProducer):\n    def load_tasks(self) -> list[dict]:\n        from summary_dataset import SummaryDataset\n        tasks = [{"prompt": data["prompt"].split("TL;DR:")[0] + "TL;DR: "}\n                 for data in SummaryDataset(usage="test").dataset]\n        return tasks\n\n    def cached_result_is_valid(self, task, result) -> bool:\n        return all([result is not None and task[key]==result[key]\n                    for key in ["prompt"]])\n\nclass GenerateConsumer(BatchTaskConsumer):\n    def init_model(self) -> None:\n        tokenizer = get_tokenizer()\n        if self.args.cvae_model_path is not None:\n            model = GPTJForCVAE.from_pretrained(\n                self.args.cvae_model_path,\n                torch_dtype=torch.float16,\n                device_map=self.device\n            ).eval()\n            model.config.eos_token_id = tokenizer.eos_token_id\n            model.config.pad_token_id = tokenizer.eos_token_id\n            if self.args.lpo_model_path is not None:\n                model.load_lpo_policy_latent_encoder(self.args.lpo_model_path)\n        elif self.args.vae_model_path is not None:\n            model = GPTJForVAE.from_pretrained(\n                self.args.vae_model_path,\n                torch_dtype=torch.float16,\n                device_map=self.device\n            ).eval()\n            model.config.eos_token_id = tokenizer.eos_token_id\n            model.config.pad_token_id = tokenizer.eos_token_id\n            if self.args.lpo_model_path is not None:\n                model.load_lpo_policy_latent_encoder(self.args.lpo_model_path)\n        elif self.args.dpo_model_path is not None:\n            model = GPTJForCausalLM.from_pretrained(\n                BASE_MODEL_PATH,\n                torch_dtype=torch.float16,\n                device_map=self.device,\n            ).eval()\n            sd = torch.load(os.path.join(self.args.dpo_model_path, "pytorch_model.bin"),\n                            map_location=self.device)\n            sd = {k.replace("base.", ""): v for k, v in sd.items() if k.startswith("base.")}\n            model.load_state_dict(sd)\n        elif self.args.pt_model_path is not None:\n            model = PTModel.from_pretrained(\n                self.args.pt_model_path,\n                torch_dtype=torch.float16,\n                device_map=self.device,\n            ).eval()\n        elif self.args.ppo_model_path is not None:\n            model = GPTJForCausalLM(GPTJConfig.from_pretrained(BASE_MODEL_PATH))\n            num_shards = 4 # TODO: read this number from ppo_model_path\n            sd = {}\n            for i in tqdm(range(num_shards), desc="Loading PPO checkpoint shards"):\n                sd_shard = torch.load(os.path.join(self.args.ppo_model_path,\n                                                   f"pytorch_model-{i+1:05d}-of-{num_shards:05d}.bin"),\n                                      map_location="cpu")\n                sd.update(sd_shard)\n            sd = {k[len("base_model."):]: v for k, v in sd.items() if k.startswith("base_model.")}\n            model.load_state_dict(sd)\n            model = model.half().to(self.device).eval()\n        elif self.args.lora_model_path is not None:\n            from peft import AutoPeftModelForCausalLM\n            model = AutoPeftModelForCausalLM.from_pretrained(self.args.lora_model_path)\n            model = model.eval().half().to(self.device)\n        else:\n            model = GPTJForCausalLM.from_pretrained(\n                self.args.sft_model_path,\n                device_map=self.device,\n                torch_dtype=torch.float16,\n            ).eval()\n        self.tokenizer = tokenizer\n        self.tokenizer.padding_side = "left"\n        self.tokenizer.truncation_side = "left"\n        self.model = model\n\n    def process_tasks(self, tasks: list[dict]) -> list[dict]:\n        inputs = self.tokenizer([task["prompt"] for task in tasks], return_tensors="pt",\n                                truncation=True, padding="longest", max_length=input_max_length)\n        inputs_len = inputs.input_ids.shape[1]\n        with torch.no_grad():\n            output_ids = self.model.generate(\n                input_ids=inputs.input_ids.to(self.device),\n                attention_mask=inputs.attention_mask.to(self.device),\n                do_sample=True,\n                temperature=0.01,\n                max_new_tokens=output_max_length,\n                eos_token_id=self.tokenizer.eos_token_id,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        outputs = self.tokenizer.batch_decode(output_ids[:,inputs_len:], skip_special_tokens=True)\n        for task,output in zip(tasks,outputs):\n            task[\'output\'] = output\n        return tasks\n\ndef visualize_reward_and_ppl(input_file):\n    output_dir = os.path.dirname(input_file)\n    def visualize_values(values, x_min, x_max, x_num, window_size, name):\n        import matplotlib.pyplot as plt\n        x = np.linspace(x_min, x_max, x_num)\n        x_left, x_right = x - window_size, x + window_size\n        x_left = np.where(x_left<x_min, x_min, x_left)\n        x_right = np.where(x_right>x_max, x_max, x_right)\n        y = ((x_left[:,None]<values[None,:]) & (values[None,:]<x_right[:,None])).sum(axis=-1) / (x_right-x_left)\n        records_file = os.path.join(output_dir, f"{name}_density.json")\n        with open(records_file, "w", encoding="utf-8") as f:\n            f.write(json.dumps({"x":x.tolist(), "y":y.tolist()}))\n        plot_png_file = os.path.join(output_dir, f"{name}_density.png")\n        plt.plot(x, y)\n        plt.xlim(x_min, x_max)\n        plt.ylim(bottom=0.0)\n        plt.savefig(plot_png_file)\n        plt.clf()\n    with open(input_file, "r", encoding="utf-8") as f:\n        results = json.loads(f.read())\n    rewards = np.array([result["rewards"] for result in results])\n    visualize_values(rewards, x_min=-6.0, x_max=6.0, x_num=100, window_size=0.3, name=f"rewards")\n    print(f"reward mean: {rewards.mean():.2f}")\n    print(f"reward standard deviation: {rewards.std():.2f}")\n    total_nll = sum([result["nll"] for result in results])\n    total_num_tokens = sum([result["num_tokens"] for result in results])\n    ppl = np.exp(total_nll / total_num_tokens)\n    print(f"ppl: {ppl:.2f}")\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'parse args for map-reduce generation and reward on multi-gpu\')\n    parser.add_argument("--py_file_name", type=str, default=None)\n    parser.add_argument("--local_rank", type=int, default=-1)\n    parser.add_argument("--host", type=str, default="localhost")\n    parser.add_argument("--port", type=int, default=6380)\n    parser.add_argument("--task_name", type=str, default="generate")\n    parser.add_argument("--output_file", type=str, default=None)\n    parser.add_argument("--erase", type=int, default=0)\n\n    # checkpoint path of generation model to test\n    parser.add_argument(\'--sft_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--pt_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--dpo_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--cvae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--vae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--lpo_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--ppo_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--lora_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument("--batch_size", type=int, default=8)\n\n    # the input file of generation to reward\n    parser.add_argument("--input_file", type=str, default=None)\n\n    args = parser.parse_args()\n\n    if args.py_file_name is None:\n        args.py_file_name = os.path.basename(__file__)\n\n    if args.local_rank == -1:\n        if args.pt_model_path is not None:\n            args.output_file = os.path.join(args.pt_model_path, "inference.json")\n        elif args.dpo_model_path is not None:\n            args.output_file = os.path.join(args.dpo_model_path, "inference.json")\n        elif args.cvae_model_path is not None:\n            if args.lpo_model_path is not None:\n                args.output_file = os.path.join(args.lpo_model_path, "inference.json")\n            else:\n                args.output_file = os.path.join(args.cvae_model_path, "inference.json")\n        elif args.vae_model_path is not None:\n            if args.lpo_model_path is not None:\n                args.output_file = os.path.join(args.lpo_model_path, "inference.json")\n            else:\n                args.output_file = os.path.join(args.vae_model_path, "inference.json")\n        elif args.sft_model_path is not None:\n            args.output_file = os.path.join(args.sft_model_path, "inference.json")\n        elif args.ppo_model_path is not None:\n            args.output_file = os.path.join(args.ppo_model_path, "inference.json")\n        elif args.lora_model_path is not None:\n            args.output_file = os.path.join(args.lora_model_path, "inference.json")\n        else:\n            raise NotImplementedError(f"at least one kind of model path should be specified")\n\n    return args\n\ndef main():\n    args = parse_args()\n\n    if args.local_rank == -1:\n        # step1. generate output\n        producer = GenerateProducer(args)\n        producer.run()\n        # step2. reward output\n        args.task_name = "reward"\n        args.input_file = args.output_file\n        args.output_file = args.output_file.replace("inference.json", "inference_and_reward.json")\n        producer = RewardProducer(args)\n        producer.run()\n        # step3. ppl\n        args.task_name = "ppl"\n        args.input_file = args.output_file\n        args.output_file = args.output_file.replace("inference_and_reward.json", "inference_and_reward_and_ppl.json")\n        producer = PPLProducer(args)\n        producer.run()\n        # step4. visualize output reward and ppl\n        visualize_reward_and_ppl(input_file=args.output_file)\n    else:\n        if args.task_name == "generate":\n            consumer = GenerateConsumer(args)\n            consumer.run()\n        elif args.task_name == "reward":\n            consumer = RewardConsumer(args)\n            consumer.run()\n        elif args.task_name == "ppl":\n            consumer = PPLConsumer(args)\n            consumer.run()\n        else:\n            raise NotImplementedError(args.task_name)\n\nif __name__ == "__main__":\n    main()\n', './LPO-summary\\train_dpo.py': 'import json\nimport os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\nfrom transformers import GPTJForCausalLM\n\nfrom config_and_utils import get_tokenizer, get_trainer, BASE_MODEL_PATH, auto_find_checkpoint_dir\nfrom modeling_gptj_cvae import DPOModel, PTModel\nfrom summary_comparison_dataset import (\n    SummaryComparisonDataset,\n)\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n\ndef compute_metrics(eval_preds):\n    acc = eval_preds.predictions\n    return {"acc": acc.mean()}\n\n\ntokenizer = get_tokenizer()\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--sft_model_path\', type=auto_find_checkpoint_dir, default=BASE_MODEL_PATH)\n    parser.add_argument(\'--pt_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument("--preference", type=str, default="human",\n                        choices=["human", "helpful", "harmless", "empathetic", "entertainment"])\n    parser.add_argument(\'--beta\', type=float, default=0.5)\n    parser.add_argument(\'--lora_dim\', type=int, default=0)\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-6)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=2)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    if args.output_dir is None:\n        if args.pt_model_path is not None:\n            args.output_dir = os.path.join(args.pt_model_path, f"pt_dpo_{args.preference}_preference")\n        else:\n            if args.lora_dim:\n                args.output_dir = os.path.join(args.sft_model_path, f"lora-{args.lora_dim}_dpo_{args.preference}_preference")\n            else:\n                args.output_dir = os.path.join(args.sft_model_path, f"dpo_{args.preference}_preference")\n        if args.beta != 0.5:\n            args.output_dir += f"_beta{args.beta}"\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.mkdir(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    set_seed(args.seed)\n\n    train_dataset = SummaryComparisonDataset(usage="train", preference=args.preference)\n    valid_dataset = SummaryComparisonDataset(usage="validation", preference=args.preference)\n\n    device = f"cuda:{args.local_rank}"\n    if args.pt_model_path is not None:\n        model = PTModel.from_pretrained(args.pt_model_path, device_map=device)\n        model.base.config.update({\n            "attn_pdrop": 0.0, "embd_pdrop": 0.0, "resid_pdrop": 0.0,\n        })\n        model.switch_into_dpo_mode()\n        model.config.beta = args.beta\n        model.beta = args.beta\n        if args.learning_rate == 1e-6:\n            args.learning_rate = 1e-4\n    else:\n        pretrained_model = GPTJForCausalLM.from_pretrained(args.sft_model_path, device_map=device)\n        config = pretrained_model.config\n        config.update({\n            "beta": args.beta, "lora_dim": args.lora_dim\n        })\n        model = DPOModel(config).to(device)\n        model.load_pretrained(pretrained_model=pretrained_model)\n        model.prepare_refer_model()\n\n        if args.lora_dim:\n            from peft import get_peft_model, LoraConfig, TaskType\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                inference_mode=False,\n                r=config.lora_dim, lora_alpha=config.lora_dim, lora_dropout=0.0\n            )\n            model.base = get_peft_model(model.base, peft_config)\n            if args.local_rank == 0:\n                model.base.print_trainable_parameters()\n\n            if args.learning_rate == 1e-6:\n                args.learning_rate = 1e-5\n\n    if args.local_rank == 0:\n        print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n        with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n            f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.dpo_collate_fn,\n        compute_metrics=compute_metrics\n    )\n    trainer.train()\n    trainer.save_state()\n\n    if args.lora_dim and args.local_rank == 0:\n        model.base.save_pretrained(os.path.join(args.output_dir, "best_model"))\n', './LPO-summary\\train_lpo.py': 'import os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\n\nfrom config_and_utils import get_tokenizer, get_trainer, auto_find_checkpoint_dir\nfrom modeling_gptj_cvae import LPOModel\nfrom summary_comparison_dataset import (\n    SummaryComparisonDataset\n)\n\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n\ndef compute_metrics(eval_preds):\n    acc = eval_preds.predictions\n    return {"acc": acc.mean()}\n\n\ntokenizer = get_tokenizer()\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--cvae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--vae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument("--preference", type=str, default="human",\n                        choices=["human", "helpful", "harmless", "empathetic", "entertainment"])\n    parser.add_argument(\'--beta\', type=float, default=0.1)\n    parser.add_argument(\'--lpo_sampling_times\', type=int, default=16)\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-6)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=8)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    if args.cvae_model_path is not None:\n        model = LPOModel.from_cvae(\n            args.cvae_model_path,\n            beta=args.beta,\n            lpo_sampling_times=args.lpo_sampling_times\n        )\n        model_path = args.cvae_model_path\n    elif args.vae_model_path is not None:\n        model = LPOModel.from_vae(\n            args.vae_model_path,\n            beta=args.beta,\n            lpo_sampling_times=args.lpo_sampling_times\n        )\n        model_path = args.vae_model_path\n    else:\n        raise ValueError(f"at least one of [cvae_model_path, vae_model_path] needs to be specified")\n\n    if args.output_dir is None:\n        args.output_dir = os.path.join(model_path, f"lpo_{args.preference}_preference")\n        if args.beta != 0.1:\n            args.output_dir = args.output_dir + f"_beta_{args.beta}"\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.mkdir(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    set_seed(args.seed)\n\n    train_dataset = SummaryComparisonDataset(usage="train", preference=args.preference)\n    valid_dataset = SummaryComparisonDataset(usage="validation", preference=args.preference)\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.lpo_collate_fn,\n        compute_metrics=compute_metrics,\n        label_names=["prompt_ids"],\n        metric_for_best_model="eval_loss",\n    )\n    trainer.train()\n    trainer.save_state()', './LPO-summary\\train_sft.py': 'import json\nimport os\n\nimport torch\nimport random\nimport argparse\nimport numpy as np\n\nfrom config_and_utils import get_tokenizer, get_trainer, BASE_MODEL_PATH\nfrom modeling_gptj_cvae import GPTJForCausalLM, PTModel, GPTJCVAEConfig\nfrom summary_dataset import SummaryDataset\nfrom summary_one2many_dataset import GeneratedSummaryOne2ManyDataset, extend_one2many_dataset_with_mini_many\n\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\ntokenizer = get_tokenizer()\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n    parser.add_argument(\'--base_model_path\', type=str, default=BASE_MODEL_PATH)\n    parser.add_argument(\'--p_tuning\', action="store_true")\n    parser.add_argument(\'--p_tuning_on_generation\', action="store_true")\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-5)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=64)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=4)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    if args.output_dir is None:\n        if args.p_tuning:\n            if args.p_tuning_on_generation:\n                args.output_dir = f"pt-sft-on-generation"\n            else:\n                args.output_dir = f"pt-sft"\n        else:\n            args.output_dir = f"sft"\n\n    if args.local_rank == 0:\n        print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n        with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n            f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.mkdir(args.output_dir)\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    set_seed(args.seed)\n\n    if args.p_tuning:\n        if args.p_tuning_on_generation:\n            args.total_many, args.mini_many = 32, 4\n            One2ManyDataset = extend_one2many_dataset_with_mini_many(\n                GeneratedSummaryOne2ManyDataset, drop_other_mini=True\n            )\n            train_dataset = One2ManyDataset(\n                total_many=args.total_many, mini_many=args.mini_many,\n                usage="train"\n            )\n            valid_dataset = One2ManyDataset(\n                total_many=args.total_many, mini_many=args.mini_many,\n                usage="validation"\n            )\n            assert args.mini_batch_size == 1\n            args.epochs *= args.total_many // args.mini_many\n            args.global_batch_size *= args.total_many // args.mini_many\n        else:\n            train_dataset = SummaryDataset(usage="train")\n            valid_dataset = SummaryDataset(usage="validation")\n\n        sft_model = GPTJForCausalLM.from_pretrained(args.base_model_path,\n                                                    device_map=f"cuda:{args.local_rank}")\n        config = GPTJCVAEConfig(\n            **vars(args),\n            **vars(sft_model.config)\n        )\n        model = PTModel(config)\n        model.load_pretrained(sft_model)\n        if args.learning_rate == 1e-5:\n            args.learning_rate = 1e-3\n    else:\n        train_dataset = SummaryDataset(usage="train")\n        valid_dataset = SummaryDataset(usage="validation")\n        model = GPTJForCausalLM.from_pretrained(args.base_model_path)\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.sft_collate_fn\n    )\n    trainer.train()\n    trainer.save_state()\n', './LPO-summary\\train_vae.py': 'import os\nimport json\nimport torch\n\nimport random\nimport argparse\nimport numpy as np\n\nfrom modeling_gptj_cvae import GPTJForVAE, GPTJCVAEConfig\n\nfrom summary_one2many_dataset import (\n    SummaryOne2ManyDataset,\n    GeneratedSummaryOne2ManyDataset, extend_one2many_dataset_with_mini_many\n)\nfrom transformers import (\n    AutoConfig,\n)\n\nfrom config_and_utils import (\n    get_tokenizer,\n    get_trainer,\n    BASE_MODEL_PATH,\n    auto_find_checkpoint_dir\n)\n\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n\ndef compute_metrics(eval_preds):\n    loss_lm, loss_kld, loss_contra = eval_preds.predictions\n\n    result = {\n        "loss_lm": loss_lm.mean(),\n        "loss_kld": loss_kld.mean(),\n        "loss_contra": loss_contra.mean(),\n    }\n\n    return result\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description=\'parse args for modeling and training\')\n\n    parser.add_argument(\'--vae_model_path\', type=auto_find_checkpoint_dir, default=None)\n    parser.add_argument(\'--base_model_path\', default=BASE_MODEL_PATH)\n\n    parser.add_argument(\'--num_q\', type=int, default=4)\n    parser.add_argument(\'--dim_z\', type=int, default=32)\n    parser.add_argument(\'--num_p\', type=int, default=4)\n    parser.add_argument(\'--latent_aggregator_layers\', type=int, default=2)\n\n    parser.add_argument(\'--frozen_pretrained\', type=int, default=0)\n    parser.add_argument(\'--marginal_kl\', type=int, default=1)\n    parser.add_argument(\'--lm_sampling_times\', type=int, default=1)\n    parser.add_argument(\'--kl_sampling_times\', type=int, default=16)\n    parser.add_argument(\'--add_skip_connection\', type=int, default=0)\n    parser.add_argument(\'--add_contra_loss\', type=int, default=0)\n    parser.add_argument(\'--without_dg_kld\', type=int, default=0)\n\n    parser.add_argument(\'--no_deepspeed\', action="store_true")\n\n    parser.add_argument(\'--data_from\', type=str, default="openai",\n                        choices=["openai", "generation"])\n\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-5)\n    parser.add_argument(\'--n_devices\', type=int, default=torch.cuda.device_count())\n    parser.add_argument(\'--global_batch_size\', type=int, default=8)\n    parser.add_argument(\'--mini_batch_size\', type=int, default=1)\n\n    parser.add_argument(\'--output_dir\', type=str, default=None)\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n    parser.add_argument("--local_rank", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    set_seed(args.seed)\n\n    if args.output_dir is None:\n        args.output_dir = f"gptj_vae"\n        if (args.num_q, args.dim_z, args.num_p) != (4, 32, 4):\n            args.output_dir += f"_{args.num_q}q_{args.dim_z}z_{args.num_p}p"\n        if args.frozen_pretrained:\n            args.output_dir += f"_frozen_base"\n        if args.add_skip_connection:\n            args.output_dir += f"_skip"\n        if args.add_contra_loss:\n            args.output_dir += f"_contra"\n        if not args.learning_rate == 1e-5:\n            args.output_dir += f"_lr{args.learning_rate}"\n        if not args.epochs == 1:\n            args.output_dir += f"_{args.epochs}epochs"\n    tokenizer = get_tokenizer()\n\n    if not os.path.exists(args.output_dir):\n        if args.local_rank == 0:\n            os.makedirs(args.output_dir)\n            print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n            with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n                f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n    elif os.path.exists(os.path.join(args.output_dir, "trainer_state.json")):\n        print(f"output_dir is not empty, exit this training: {args.output_dir}")\n        quit()\n\n    if args.local_rank == 0:\n        print(f"args: {json.dumps(vars(args), ensure_ascii=False, indent=4)}")\n        with open(f"{args.output_dir}/args.json", "w", encoding="utf-8") as f:\n            f.write(json.dumps(vars(args), ensure_ascii=False, indent=4))\n\n    if args.data_from == "openai":\n        args.total_many, args.mini_many = 4, 4\n        One2ManyDataset = extend_one2many_dataset_with_mini_many(SummaryOne2ManyDataset)\n    elif args.data_from == "generation":\n        args.total_many, args.mini_many = 32, 4\n        One2ManyDataset = extend_one2many_dataset_with_mini_many(GeneratedSummaryOne2ManyDataset)\n    else:\n        raise NotImplementedError(args.data_from)\n    train_dataset = One2ManyDataset(\n        total_many=args.total_many, mini_many=args.mini_many,\n        usage="train"\n    )\n    valid_dataset = One2ManyDataset(\n        total_many=args.total_many, mini_many=args.mini_many,\n        usage="validation"\n    )\n\n    training_kwargs = {\n        key:getattr(args, key) for key in [\n            "frozen_pretrained", "marginal_kl", "lm_sampling_times", "kl_sampling_times",\n            "add_skip_connection", "add_contra_loss", "without_dg_kld"\n        ]\n    }\n\n    if args.vae_model_path is not None:\n        model = GPTJForVAE.from_pretrained(args.vae_model_path)\n        model.config.update(training_kwargs)\n        model.update_requires_grad_()\n    else:\n        # construct with base model\n        gptj_config = AutoConfig.from_pretrained(args.base_model_path)\n        gptj_cvae_config = GPTJCVAEConfig(\n            **vars(args),\n            **vars(gptj_config)\n        )\n        model = GPTJForVAE(gptj_cvae_config)\n        model.load_pretrained()\n\n    args.epochs *= args.total_many // args.mini_many\n    args.global_batch_size *= args.total_many // args.mini_many\n\n    trainer = get_trainer(\n        args=args,\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        collate_fn=train_dataset.collate_fn,\n        compute_metrics=compute_metrics,\n        label_names=[],\n    )\n    trainer.train()\n    trainer.save_state()\n\n'}

def restore_files(file_dict, restore_directory):
    import os
    if not os.path.exists(restore_directory):
        os.makedirs(restore_directory)
    for file_path, file_content in file_dict.items():
        new_file_path = os.path.join(restore_directory, file_path)
        new_file_dir = os.path.dirname(new_file_path)
        if not os.path.exists(new_file_dir):
            os.makedirs(new_file_dir)
        with open(new_file_path, 'w') as f:
            f.write(file_content)

if __name__ == "__main__":
    restore_files(file_dict, "./")